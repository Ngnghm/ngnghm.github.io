<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>Houyhnhnm Computing: Posts tagged 'Persistence'</title>
  <description>Houyhnhnm Computing: Posts tagged 'Persistence'</description>
  <link>http://ngnghm.github.io/tags/Persistence.html</link>
  <lastBuildDate>Sun, 12 Jun 2016 00:34:38 UT</lastBuildDate>
  <pubDate>Sun, 12 Jun 2016 00:34:38 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>Chapter 10: Houyhnhnms vs Martians</title>
   <link>http://ngnghm.github.io/blog/2016/06/11/chapter-10-houyhnhnms-vs-martians/?utm_source=Persistence&amp;utm_medium=RSS</link>
   <guid>urn:http-ngnghm-github-io:-blog-2016-06-11-chapter-10-houyhnhnms-vs-martians</guid>
   <pubDate>Sun, 12 Jun 2016 00:34:38 UT</pubDate>
   <description>&lt;html&gt;
&lt;p&gt;What did Ngnghm think of &lt;a href="http://urbit.org/"&gt;Urbit&lt;/a&gt;? Some elements in Ngnghm&amp;rsquo;s descriptions of Houyhnhnm computing were remindful of the famous Martian system software stack Urbit: both computing worlds were alien to Human Computing; both had Orthogonal Persistence; and both relied heavily on pure deterministic computations to minimize the amount of data to log in the persistence journal (as contrasted for instance with the amount of data to manipulate to compute and display answers to end-users). What else did Houyhnhnm computing have in common with Martian software? How did it crucially differ? How did they equally or differently resemble Human systems or differ from them? Ngnghm took a long look at Urbit; while he concluded that indeed the three approaches were quite distinct, he also helped me identify the principles underlying their mutual differences and commonalities.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="urbit-the-martian-model"&gt;Urbit: The Martian Model&lt;/h3&gt;

&lt;p&gt;&lt;a href="http://moronlab.blogspot.com/2010/01/urbit-functional-programming-from.html"&gt;Martians&lt;/a&gt; have developed a peculiar operating system, &lt;a href="http://media.urbit.org/whitepaper.pdf"&gt;Urbit&lt;/a&gt; (&lt;a href="http://urbit.org/docs/"&gt;docs&lt;/a&gt;), the Terran port of which seems to be semi-usable since &lt;a href="https://medium.com/@urbit/design-of-a-digital-republic-f2b6b3109902"&gt;2015&lt;/a&gt;. At the formal base of it is a pure functional applicative virtual machine, called &lt;em&gt;Nock&lt;/em&gt;. On top of it, a pure functional applicative programming language, called &lt;em&gt;Hoon&lt;/em&gt;, with an unusual terse syntax and a very barebones static type inferencer. On top of that, an Operating System, call &lt;em&gt;Arvo&lt;/em&gt;, that on each server of the network runs by applying the current state of the system to the next event received. The networking layer &lt;em&gt;Ames&lt;/em&gt; implements a secure P2P protocol, while the underlying C runtime system, &lt;em&gt;u3&lt;/em&gt;, makes it all run on top of a regular Linux machine.&lt;/p&gt;

&lt;p&gt;The data model of &lt;em&gt;Nock&lt;/em&gt; is that everything is a &lt;em&gt;noun&lt;/em&gt;, which can be either a non-negative integer or a pair of nouns. Since the language is pure and applicative (and otherwise without cycle-creating primitives), there can be no cycle in this binary tree of integers. Since the only equality test is extensional, identical subtrees can be merged and the notional tree can be implemented as a DAG (Directed Acyclic Graph).&lt;/p&gt;

&lt;p&gt;On top of those, the execution model of Nock is to interpret some of these trees as programs in a variant of combinatory logic, with additional primitives for literals, peano integers, structural equality, and a primitive for tree access indexed by integers. The inefficiency of a naive implementation would be hopeless. However, just like the tree can be optimized into a DAG, the evaluation can be optimized by recognizing that some programs implement known functions, then using a special fast implementation of an equivalent program (which Martians call a &lt;em&gt;jet&lt;/em&gt;, by contrast with &lt;em&gt;JIT&lt;/em&gt;) rather than interpreting the original programs by following the definitional rules. Recognizing such programs in general could be hard, but in practice Urbit only needs recognize specific instances of such programs â€” those generated by Hoon and/or present in the standard library.&lt;/p&gt;

&lt;p&gt;Therefore, it is the C runtime system &lt;em&gt;u3&lt;/em&gt; that specifies the operational semantics of programs, whereas Nock only specifies their denotational semantics as arbitrary recursive functions. By recognizing and efficiently implementing specific Nock programs and subprograms, u3, like any efficient implementation of the JVM or of any other standardized virtual machine, can decompile VM programs (in this case Nock programs) into an AST and recompile them into machine code using the usual compilation techniques. At that point, like every VM, Nock is just a standardized though extremely awkward representation of programming language semantics (usually all the more awkward since such VM standards are often decided early on, at the point when the least is known about what makes a good representation). Where Urbit distinguishes itself from other VM-based systems, however, is that the semantics of its virtual machine Nock is forever fixed, totally defined, deterministic, and therefore &lt;em&gt;future-proof&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Hoon is a pure functional applicative programming language. Its syntax is terse, where the core syntax is specified using non-alphanumeric characters and digraphs thereof (or equivalent for letter keywords); The syntax allows to write expressions as one liners using parentheses, but it is colloquial to break functions onto many lines where indentation is meaningful; as contrasted with other indentation-sensitive languages, however, the indentation rules are cleverly designed to prevent extraneous indentation to the right as you nest expressions, by deindenting the last, tail position in a function call. Whereas Nock is trivially typed (some would say untyped or dynamically typed), Hoon has a static type system, although quite a primitive one, with a type inferencer that requires more type hints than a language with e.g. Hindley-Milner type inference (such as ML), yet less than one without type inference (such as Java).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Arvo&lt;/em&gt; is the operating system of Urbit. The Urbit model is that the state of the system (a noun) encodes a function that will be applied to the next communication event received by the system. If the processing of the event terminates, then the event is transactionally appended to the event journal making it persistent. The value returned specifies the next state of the system and any messages to be sent to the world. Arvo is just the initial state of the system, a universal function that depending on the next event, may do anything, but in particular provides a standard library including anything from basic arithmetics to virtualization of the entire system. The core of Arvo is typically preserved when processing a message, even as the state of the system changes to reflect the computations controlled by the user; as long as this core keeps running (as it should), Arvo remains the operating system of Urbit; but users who insist may upgrade and replace Arvo with a new version, or with another system of their own creation, if they dare.&lt;/p&gt;

&lt;p&gt;The events fed into Urbit are generated by the C runtime system u3, to represent console input, incoming network messages, etc. Conversely the messages generated by Urbit are translated by the implementation into console output, outgoing network messages, etc. If processing an event results in an error, if it is interrupted by the impatient user, or if it times out after a minute (for network messages), then u3 just drops the event and doesn&amp;rsquo;t include it in the event journal. (Of course, if an adversarial network message can time out an Urbit machine for a minute or even a second, that&amp;rsquo;s probably already a denial of service vulnerability; on the other hand, if the owner, being remote, can&amp;rsquo;t get his long-running computations going, that&amp;rsquo;s probably another problem.) A stack trace is generated by u3 when an error occurs, and injected as an event into Arvo in place of the triggering event, that is not persisted. Users can at runtime toggle a flag in the interactive shell &lt;em&gt;Dojo&lt;/em&gt; so that it will or won&amp;rsquo;t display these stack traces.&lt;/p&gt;

&lt;p&gt;The networking layer &lt;em&gt;Ames&lt;/em&gt; is conceptually a global broadcast network, where network messages are conceptually visible by all other nodes. However, a message is typically addressed to a specific node, using a public key for which only this node has the private key; and other nodes will drop messages they cannot decrypt. Therefore, the C runtime will optimize the sending of a message to route it directly to its destined recipient, as registered on the network. A node in the network is identified by its address, or &lt;em&gt;plot&lt;/em&gt;, that can be 8-bit (&amp;ldquo;galaxy&amp;rdquo;), 16-bit (&amp;ldquo;star&amp;rdquo;), 32-bit (&amp;ldquo;planet&amp;rdquo;), 64-bit (&amp;ldquo;moon&amp;rdquo;) or 128-bit (&amp;ldquo;comet&amp;rdquo;). A comet has for 128-bit address the cryptographic digest of its public key, making it self-authenticating. A moon has its public key signed by the corresponding planet; a planet has its public key signed by the corresponding star, a star has its public key signed by the corresponding galaxy, a galaxy has its public key included in Arvo itself, in a hierarchical system rooted in whoever manages the base Operating System. All communications are thus authenticated by construction. Galaxies, stars, planets and moons are scarce entities, thus constituting &amp;ldquo;digital real estate&amp;rdquo; (hence the name &lt;em&gt;plot&lt;/em&gt;), that the Urbit curators intend to sell to fund technological development.&lt;/p&gt;

&lt;p&gt;One of Urbit&amp;rsquo;s innovations is to invent mappings from octet to pronounceable three-letter syllables, so that you can pronounce 8-, 16-, 32-, 64- or 128-bit addresses, making them memorable, though not meaningful. So that names with the same address prefix shall &lt;em&gt;not&lt;/em&gt; sound the same, a simple bijective mangling function is applied to an address before to extract its pronounciation. This deemphasizes the signing authority behind an identity: the reputation of a person shouldn&amp;rsquo;t too easily wash onto another just because they used the same registrar; and it&amp;rsquo;s easier to avoid a &amp;ldquo;hash collision&amp;rdquo; in people&amp;rsquo;s minds by having vaguely related but notably different identities have notably different names. This constitutes an interesting take on &lt;a href="https://en.wikipedia.org/wiki/Zooko%27s%5Ftriangle"&gt;Zooko&amp;rsquo;s Triangle&lt;/a&gt;. Actually, care was taken so that the syllables would &lt;em&gt;not&lt;/em&gt; be too meaningful (and especially not offensive) in any human language that the author knew of. Non-alphanumerical characters are also given three-letter syllable names, though this time the names were chosen so that there were simple mnemonic rules to remember them (for instance, &amp;ldquo;wut&amp;rdquo; for the question mark "?"); this makes it easier to read and learn digraphs (though you might also name them after the corresponding keywords).&lt;/p&gt;

&lt;h3 id="houyhnhnms-vs-martians"&gt;Houyhnhnms vs Martians&lt;/h3&gt;

&lt;p&gt;Most importantly, the Martian&amp;rsquo;s Urbit is actually available for humans to experiment with (as of May 2016, its authors describe its status as post-alpha and pre-beta). By contrast, no implementation of Houyhnhnm Computing system is available to humans (at the same date), though the ideas may be older. This alone make Urbit superior in a non-negligible way; yet it is all the other ways that we will examine it.&lt;/p&gt;

&lt;p&gt;Superficially, both Martian and Houyhnhnm Computing provide Orthogonal Persistence. But the way they do it is very different. Martians provide a single mechanism for persistence at a very low-level in their system, separately on each virtual machine in their network. But Houyhnhnms recognize that there is no one size fits all in matter of Persistence: for performance reasons, the highest level of abstraction is desired for the persistence journal; at the same time, transient or loosely-persisted caches are useful for extra indices; and for robustness, a number of replicas are required, with a continuum of potential synchronization policies. Therefore, Houyhnhnms provide a general framework for first-class computations, based on which users may select what to persist under what modalities.&lt;/p&gt;

&lt;p&gt;One could imagine ways that Urbit could be modified so its persistence policies would become configurable. For instance, the underlying C runtime u3 could be sensitive to special side-effects, such as messages sent to a magic comet, and modify its evaluation and persistence strategies based on specified configuration. That would mean, however, that most of the interesting work would actually happen inside u3, and not over Nock. What would Nock&amp;rsquo;s purpose then be? It could remain as an awkward but standardized and future-proof way to represent code and data. However, unless great care is taken, using formal proofs and/or extensive testing, so that the semantics of the Nock code generated indeed implements the actual computations, while indeed being implemented by the underlying system, then at the first bug introduced or &amp;ldquo;shortcut&amp;rdquo; taken, the entire Nock VM becomes a &lt;em&gt;sham&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now, assuming Nock isn&amp;rsquo;t a complete sham, it remains an obligatory intermediate representation between the computations desired by users and the machine implementations provided by the system. Because Nock is never &lt;em&gt;exactly&lt;/em&gt; what the user wants or what the machine provides, this intermediate representation always introduces an impedance mismatch, that is all the more costly as the desired computing interactions are remote from the Nock model.&lt;/p&gt;

&lt;p&gt;In an extreme case, one could imagine that u3 would be configured using a Houyhnhnm first-class computation framework. Users would develop their computations at the level of abstraction they desired; and they would dynamically configure u3 to use the desired tower of first-class implementations. At this point, any encoding in terms of Nock could be altogether short-circuited at runtime; and any impedance mismatch introduced by Nock is thus worked around. But then, Nock is purely a hurdle and not at all an asset: all the semantics that users care about is expressed in the Houyhnhnm Computing system; any Nock code generated is just for show, obfuscating the real high-level or low-level computations without bringing anything; and Nock is either a sham, or an expensive tax on the computation framework.&lt;/p&gt;

&lt;h3 id="future-proofing-the-wrong-thing"&gt;Future-proofing the wrong thing&lt;/h3&gt;

&lt;p&gt;Both Martians and Houyhnhnms rely heavily on pure deterministic computations to minimize the amount of data to log in the persistence journal to describe issues (as contrasted for instance with the amount of data to manipulate to compute and display answers to end-users). But Martians rely on Nock, and to a lesser extent, Hoon, Arvo, Ames, etc., having a constant deterministic semantics, cast in stone for all users at all time; Houyhnhnms frown at the notion: they consider that constraint as unnecessary as it is onerous. Martians justify the constraint as making it possible to have robust, future-proof persistence. Houyhnhnms contend that this constant semantics doesn&amp;rsquo;t actually make for robust persistence, and that on the contrary, it prevents future improvements and fixes while encouraging bad practice. Also, Houyhnhms claim that requiring the function to be the same for everyone introduces an extraordinary coordination problem where none existed, without helping any of the real coordination problems that users actually have.&lt;/p&gt;

&lt;p&gt;A global consensus on deterministic computation semantics only matters if you want to replay and verify other random people&amp;rsquo;s computations, i.e. for crypto-currencies with &amp;ldquo;smart contracts&amp;rdquo; like &lt;a href="https://www.ethereum.org/"&gt;Ethereum&lt;/a&gt;; but that&amp;rsquo;s not at all what Urbit is about, and such computation replay in a hostile environment indeed has issues of its own (such as misincentives or for resource abuse) that Urbit doesn&amp;rsquo;t even try to address. If you only want to replay your own computations (or those of friends), you don&amp;rsquo;t need a global consensus on a deterministic function; you only need to know what you&amp;rsquo;re talking about, and write it down.&lt;/p&gt;

&lt;p&gt;Houyhnhnms always consider first the interactions that are supposed to be supported by computing activities. In the case of Persistence, Houyhnhnms are each interested in persisting their own code and data. There is no global entity interested in simultaneously looking at the persistence logs of everyone; there is no &amp;ldquo;collective&amp;rdquo; will, no magically coordinated knowledge. Each individual Houyhnhnm wants to ensure the persistence of their own data and that data only, or of that entrusted to them personally; and even if they want more, that&amp;rsquo;s both the only thing they must do and the only thing they can do. Now, they each want the most adequate technology for their purpose, taking costs and benefits into account. If they somehow had to coordinate together to find a common solution, the coordination would be extraordinarily costly and would take a lot of time; they would have to settle on some old technology devised when people knew least, and could never agree on improvements. And if the technology were frozen in time at the beginning, as in Urbit, nothing short of retroactive agreement using a time machine could improve it. If on the contrary each individual is allowed to choose his own persistence solution, then those who can devise improved solutions can use them without having to convince anyone; they can also compete to have their improvements adopted, whereas users compete to not be left behind, until they all adopt the improvements that make sense. In the end, in matters of persistence &lt;a href="http://common-lisp.net/project/asdf/ilc2010draft.pdf"&gt;as of build systems&lt;/a&gt;, &lt;em&gt;allowing for divergence creates an incentive towards convergence&lt;/em&gt;, reaching better solutions, through competition.&lt;/p&gt;

&lt;p&gt;Urbit incorrectly formulates the problem as being a social problem requiring a central solution, when it is actually a technical problem for which a decentralized social arrangement is much better. Persistence doesn&amp;rsquo;t require anyone to agree with other people on a low-level protocol; it only requires each person to maintain compatibility with their own previous data. To decode the data they persisted, users don&amp;rsquo;t need a one deterministic function forever, much less one they agree on with everyone else: what they need is to remember the old code and data, and to be able to express the new code (generator) in terms of the old one (to upgrade the code) and able to interpret the old data schema in terms of the new data schema (to upgrade the data). Indeed, even the &lt;a href="http://media.urbit.org/whitepaper.pdf"&gt;Urbit whitepaper&lt;/a&gt; acknowledges that as far as data above the provided abstraction matters, such schema changes happen (see section 2.0.3 Arvo).&lt;/p&gt;

&lt;p&gt;Where Martians get it just as wrong as Humans is in believing that solving one issue (e.g. persistence) at the system level is enough. But onerous local &amp;ldquo;persistence&amp;rdquo; of low-level data can actually be counter-productive when what users require is distributed persistence of high-level data at some level of service involving enough replicas yet low-enough latency: it costs a lot and for no actual benefit may cause a large increase in latency. The entire point of computing is to support user programs, and solving an issue for some underlying system at a lower-level of abstraction without solving it at the higher-level that the user cares about is actually no solution at all. It can sometimes be &lt;em&gt;part&lt;/em&gt; of a solution, but only if (1) the desired property can also be expressed in a composable way so that higher layers of software may benefit from it, and (2) the lower layers don&amp;rsquo;t impose specific policy choices that will be detrimental to the higher layers of software. And this is what Houyhnhnm systems uniquely enable that Human and Martian systems can&amp;rsquo;t express because it goes against their paradigm.&lt;/p&gt;

&lt;h3 id="neglect-for-the-meta-level"&gt;Neglect for the Meta-level&lt;/h3&gt;

&lt;p&gt;The mistake shared by Martians and Humans is to share the approach of neglecting the importance of metaprogramming.&lt;/p&gt;

&lt;p&gt;For Humans, this is often out of ignorance and of fear of the unknown: Humans are not usually trained in metaprogramming they don&amp;rsquo;t understand the importance of it, or its proper usage; they don&amp;rsquo;t know how to define and use Domain Specific Languages (DSLs). Though their job consists in building machines, they &amp;ldquo;enjoy&amp;rdquo; the job security that comes from breaking machines that would replace &lt;em&gt;their&lt;/em&gt; current jobs: Mechanized modernity for me, protectionist luddyism for thee.&lt;/p&gt;

&lt;p&gt;For Martians, unhappily, there is a conscious decision to eschew metaprogramming. One recent Urbit presentation explicitly declares that DSLs are considered harmful; the rationale given is that the base programming language should have low cognitive overload on entry-level programmers. (Though there again, the very same Urbit authors who claim their programmers shouldn&amp;rsquo;t do metaprogramming themselves spend most of their time at the meta-level â€” base-level for thee, meta-level for me.) To Martians, making the system deliberately simpler and less sophisticated makes it easier for people to understand and adopt it. Martians with Hoon commit the same error as the Humans systematically committed with COBOL, or to a lesser degree with Java: they designed languages that superficially allow any random layman (for COBOL) or professional (for Java) or enthusiast (for Hoon) to understand each of the steps of the program, by making those steps very simple, minute and detailed.&lt;/p&gt;

&lt;p&gt;But the price for this clarity at the micro-level is to make programs harder to follow at the macro-level. The abstractions that are denied expression are precisely those that would allow to concisely and precisely express the ideas for the actual high-level problem at hand. Every issue therefore become mired with a mass of needless concerns, extraneous details, and administrative overhead, that simultaneously slow down programmers with make-work and blur his understanding of the difficult high-level issues that matter to the user. The concepts that underlie these issues cannot be expressed explicitly, yet programmers need to confront them and possess the knowledge of them implicitly to grasp, develop and debug the high-level program. The rejection of abstraction in general, and metaprogramming in particular, prevents unimpeded clear thinking where it is the most sorely needed; it makes the easy harder and the hard nearly impossible, all for the benefit of giving random neophytes a false sense of comfort.&lt;/p&gt;

&lt;p&gt;The same mistake goes for all languages that wholly reject syntactic abstraction, or provide a version thereof that is very awkward (like C++ templates or Java compile-time annotations) and/or very limited (such as C macros). It also applies to all programmers and coding styles that frown upon syntactic abstraction (maybe after being bitten by the bad implementations thereof such as above). If you don&amp;rsquo;t build DSLs, your general purpose language has all the downsides of Turing-equivalence with none of the upsides.&lt;/p&gt;

&lt;p&gt;Note however that even though Urbit officially rejects abstraction, Hoon is at its core a functional programming language. Therefore, unlike Humans stuck with COBOL or Java, Martian programmers using Hoon can, if they so choose, leverage this core to develop their own set of high-level composable abstractions; and for that they can reuse or get inspired by all the work done in more advanced functional languages such as Haskell or Lisp. But of course, if that&amp;rsquo;s the route chosen for further development, in the end, the programmers might better directly adopt Haskell or Lisp and make it persistent rather than use Urbit. If the Urbit persistence model is exactly what they need, they could implement a Hoon backend for their favorite language; if not, they can probably more easily reimplement persistence on their platform based on the Urbit experience than try to evolve Urbit to suit their needs.&lt;/p&gt;

&lt;p&gt;Finally, in their common reject of metaprogramming, both the Human and Martian computing approaches lack first-class notions of meta-levels at runtime. Therefore, all their software is built and distributed as a fixed semantic tower on top of a provided common virtual machine. It&amp;rsquo;s just that the virtual machine is very different between the Humans and Martians: the Martian VM is oriented towards persistence and determinism, the Human VM is just a low-level portability layer for families of cheap human hardware. As we explained in our &lt;a href="/blog/2015/08/24/chapter-4-turtling-down-the-tower-of-babel/"&gt;chapter 4&lt;/a&gt; and subsequent chapters, this makes for rigid, brittle and expensive development processes.&lt;/p&gt;

&lt;h3 id="impedance-mismatch"&gt;Impedance Mismatch&lt;/h3&gt;

&lt;p&gt;One way that Martian is worse than Human as well as Houyhnhnm systems though is that it introduce a virtual machine that makes sense neither at a high-level nor at a low-level, but only introduces an impedance mismatch.&lt;/p&gt;

&lt;p&gt;Houyhnhnms clearly understand that the ultimate purpose of computer systems is to support some kind of interaction with some sentient users (be it via a console, via a robot, via a wider institutional process involving other sentient beings, etc.). In other words, the computer system is an enabler, a means, and the computing system is the goal, i.e. the user interactions involving applications. If some computer system makes it harder (than others; than it can; than it used to) to write, use or maintain such applications, then it is (comparatively) failing at its goal.&lt;/p&gt;

&lt;p&gt;Humans clearly understand that the ultimate starting point for building the computer software is whatever cost efficient computer hardware is available. At the bottom of the software stack are thin portable abstractions over the hardware, that together constitute the operating system. Every layer you pile on top is costly and goes against the bottom line. If it&amp;rsquo;s a good intermediate abstraction in the cheapest path from the low-level hardware to the desired high-level application, then it&amp;rsquo;s part of the cost of doing business. Otherwise it&amp;rsquo;s just useless overhead.&lt;/p&gt;

&lt;p&gt;Unhappily Martians seem to miss both points of view. The Nock virtual machine is justified neither by sophisticated high-level concepts that allow to easily compose and decompose high-level applications, nor by efficient low-level concepts that allow to cost-effectively build software as layers on top of existing hardware. It sits in the middle; and not as a flexible and adaptable piece of scaffolding that helps connect the top to the bottom; but as a fixed detour you have to make along the way, as a bottleneck in your semantic tower, a floor the plan of which was designed by aliens yet compulsorily included in your architecture, that everything underneath has to support and everything above has to rest upon.&lt;/p&gt;

&lt;p&gt;Thus, if you want your high-level programs to deal with some low-level concept that isn&amp;rsquo;t expressible in Nock (hint: it probably won&amp;rsquo;t be), then you&amp;rsquo;re in big trouble. One class of issues that Nock itself makes unexpressible yet that any programmer developing non-trivial programs has to care for is resource management: the programmer has no control over how much time or memory operations &lt;em&gt;really&lt;/em&gt; take. Yet resources such as speed and memory matter, a lot: &amp;ldquo;Speed has always been important otherwise one wouldn&amp;rsquo;t need the computer.&amp;rdquo; â€” Seymour Cray There &lt;em&gt;is&lt;/em&gt; a resource model in Urbit, but it&amp;rsquo;s all defined and hidden in u3, out of sight and out of control of the Martian programmer (unless we lift the lid on u3, at which point Urbiters leave Martian computing to go back to all too Human computing â€” and certainly not Houyhnhnm computing). At best, you have to consider evaluation of Nock programs as happening in a big fat ugly &lt;a href="https://wiki.haskell.org/Monad"&gt;Monad&lt;/a&gt; whereby programs compute functions that chain state implicitly managed by u3.&lt;/p&gt;

&lt;p&gt;Of course, you could write a resource-aware language as a slow interpreter on top of Nock, then reimplement it efficiently under u3 as &amp;ldquo;jets&amp;rdquo;. Sure you could. That&amp;rsquo;s exactly what a Houyhnhnm would do if forced to use Urbit. But of course, every time you make a change to your design, you must implement things twice, where you used to do it only once on Human or Houyhnhnm systems: you must implement your logic once as a slow interpreter in Nock; and you must implement it a second time in the Human system in which u3 jets are written. And how do you ensure the equivalence between those two implementations? You can fail to, or lie, and then Urbit is all a sham; or you can spend a lot of time doing it, at which point you wasted a lot of effort, but didn&amp;rsquo;t win anything as compared to implementing the human code without going through Urbit. What did the detour through Nock buy you? Nothing. Maybe the persistence â€” but only if persistence with the exact modalities offered by u3 are what you want. If you aim at a different tradeoff between latency, coherency, replication, etc., you lose. And even if perchance you aimed at the exact very same tradeoff, you might be better off duplicating the general persistence design of u3 without keeping any of Nock and Urbit above it.&lt;/p&gt;

&lt;p&gt;Oh, if only you had an advanced metaprogramming infrastructure capable of manipulating arbitrary program semantics in a formally correct way! You might then automatically generate both the Nock code in Monadic style and the supporting u3 code for your software, and be confident they are equivalent. And if furthermore your metaprogramming infrastructure could also dynamically replace &lt;em&gt;at runtime&lt;/em&gt; an inefficient implementation by a more efficient one that was shown to be equivalent, and for arbitrary programs defined by the users rather than a fixed list of &amp;ldquo;jets&amp;rdquo; hardwired in the system, then you could short-circuit any inefficiency and directly call the low-level implementation you generated without ever going through any of the Urbit code. But then, you&amp;rsquo;d have been using a Houyhnhnm system all along, and Urbit would have been a terrible impediment that you had to deal with and eventually managed to do away with and make irrelevant, at the cost of a non-trivial effort.&lt;/p&gt;

&lt;h3 id="computing-ownership"&gt;Computing Ownership&lt;/h3&gt;

&lt;p&gt;Martian computing is presented as a technical solution to a social problem, that of allowing individuals to reclaim sovereignty on their computations. That&amp;rsquo;s a lofty goal, and it would certainly be incorrect to retort that technology can&amp;rsquo;t change the structure of society. Gunpowder did. The Internet did. But Urbit is not the solution, because it doesn&amp;rsquo;t address any of the actually difficult issues with ownership and soreignty; I have discussed some of these issues in a previous speech: &lt;a href="http://fare.tunes.org/computing/reclaim_your_computer.html"&gt;Who Controls Your Computer? (And How to make sure itâ€™s you)&lt;/a&gt; The only valuable contribution of Urbit in this space is its naming scheme and its take on Zooko&amp;rsquo;s triangle â€” which is extremely valuable, but a tiny part of Urbit (happily, that also makes it easy to duplicate in your own designs, if you wish). The rest, in the end, is mostly a waste of time as far as ownership goes (but resurrecting the idea of orthogonal persistence is still independently cool, though its Urbit implementation is ultimately backwards).&lt;/p&gt;

&lt;p&gt;It could be argued that the Nock VM makes it easier to verify computations, and thus to ascertain that nobody is tampering with your computations (though of course these verifications can&amp;rsquo;t protect against leakage of information at lower levels of the system). Certainly, Urbit makes this possible, where random Human systems can&amp;rsquo;t do it. But if Humans wanted to verify computations they could do it much more easily than by using Urbit, using much lighter weight tools. Also, the apparent simplicity of Nock only hides the ridiculous complexity of the layers below (u3) or above (Arvo, Ames). To really verify the computation log, you&amp;rsquo;d also have to check that packets injected by u3 are consistent with your model of what u3 should be doing, which is extremely complex; and to make sense of the packets, you have to handle all the complexity that was moved into the higher layers of the system. Once again, introducing an intermediate virtual machine that doesn&amp;rsquo;t naturally appear when factoring an application creates an impedance mismatch and a semantic overhead, for no overall gain.&lt;/p&gt;

&lt;h3 id="not-invented-here"&gt;Not Invented Here&lt;/h3&gt;

&lt;p&gt;Martian computing comes with its own meta-language for sentient beings to describe computing notions. Since Martians are not Humans, it is completely understandable that the (meta)language they speak is completely different from a Human language, and that there is not exact one-to-one correspondance between Martian and Human concepts. That&amp;rsquo;s a given.&lt;/p&gt;

&lt;p&gt;Still, those who bring Martian technology to Earth fail their public every time they use esoteric terms that make it harder for Humans to understand Martian computing. The excuse given for using esoteric terms is that using terms familiar to Human programmers would come with the &lt;em&gt;wrong&lt;/em&gt; connotations, and would lead Humans to an incorrect conceptual map that doesn&amp;rsquo;t fit the delineations relevant to Martians. But that&amp;rsquo;s a cop out. Beginners will start with an incorrect map anyway, and experts will have a correct map anyway, whichever terms are chosen. Using familiar terms would speed up learning and would crucially make it easier to pin point the similarities as well as dissimilarities in the two approaches, as you reuse a familiar term then explain how the usage differs.&lt;/p&gt;

&lt;p&gt;As someone who tries to translate alien ideas into Human language, I can relate to the difficulty of explaining ideas to people whose &lt;em&gt;paradigm&lt;/em&gt; makes it unexpressible. This difficulty was beautifully evidenced and argued by Richard P. Gabriel in his article &lt;a href="https://www.dreamsongs.com/Files/Incommensurability.pdf"&gt;The Structure of a Programming Language Revolution&lt;/a&gt;. But the Urbit authors are not trying to be understood, trying their best not to be, and that&amp;rsquo;s a shame, because whatever good and bad ideas exist in their paradigm deserve to be debated, which first requires that they should be understood. Instead they lock themselves into their own autistic planet.&lt;/p&gt;

&lt;p&gt;There is a natural tradeoff when designing computing systems, whereby a program can be easy to write, be easy to read, be fast to run, and can even be two of these, but not three. Or at least, there is a &amp;ldquo;triangle&amp;rdquo; of a tradeoff (as with Zooko&amp;rsquo;s triangle), and you can only improve a dimension so much before the other dimensions suffer. But Urbit seems to fail in all these dimensions. Its alien grammar, vocabulary, primitives, paradigm, etc., make it both hard to read and hard to write; and its forced abstraction makes programs slower to run.&lt;/p&gt;

&lt;p&gt;If that abstraction came &amp;ldquo;naturally&amp;rdquo; when factoring some programs, then it could make writing these programs easier; but the Urbit VM looks very little like what Humans use for anything, and offers no &amp;ldquo;killer app&amp;rdquo; that can&amp;rsquo;t be implemented more simply. Its applicative functional machine with no cycles exchanging messages is reminiscent of the Erlang VM; but then it&amp;rsquo;s not obvious what advantages Nock brings for the applications that currently use the Erlang VM, and all too obvious what it costs. It would be much easier to make an Erlang VM persistent or to teach Erlang Ames-style authentication than to teach u3 to do anything useful.&lt;/p&gt;

&lt;p&gt;Yet, by having deliberately cut themselves from the rest of the world in so many ways, Urbit programmers find themselves forced to reinvent the world from scratch without being able to reuse much of other people&amp;rsquo;s code, except at a very high cost both in terms of implementation effort (doing things both in Nock and in u3) and integrity (ensuring the two things are equivalent, or cheating). For instance, it looks like the Urbit authors wrote a markdown processor in Hoon, for instance, and have a &amp;ldquo;jet&amp;rdquo; recognizing it and replacing it by some common Markdown library in C. Except the two pieces of code are not bug compatible, so it&amp;rsquo;s all a lie.&lt;/p&gt;

&lt;h3 id="urbit-as-a-demo"&gt;Urbit as a demo&lt;/h3&gt;

&lt;p&gt;Urbit has none of the support for modular design necessary for programming &lt;a href="https://en.wikipedia.org/wiki/Programming_in_the_large_and_programming_in_the_small"&gt;&amp;ldquo;in the large&amp;rdquo;&lt;/a&gt;. But its superficial simplicity of Nock makes it suitable as a cool demo of orthogonally persistent system.&lt;/p&gt;

&lt;p&gt;Of course, the demo only &amp;ldquo;works&amp;rdquo; by sweeping under the rug the difficult issues, to be solved by u3, the metasystem of Urbit; and unlike Nock, u3, where most of the interesting things happen, remains informal in its all-important side-effects, and not actually bound to behave as a faithful implementation as for the parts specified by the Nock machine. In other words, the pretense of having fully formalized the state of the system and its state function, and of putting the end-user in control of it, is ultimately a &lt;em&gt;sham&lt;/em&gt;, a corruption. The power remains in the opaque and totally unspecified centralized implementation of the metaprogram that implements Nock and issues real-world side-effects.&lt;/p&gt;

&lt;p&gt;There is no one-size fits all way to handle all the issues with connection to real-world devices, and policies that resolve tradeoffs regarding persistence, privacy, latency, efficiency, safety, etc. A centralized implementation for the metaprogram that handles them is not a universal solution. Only a general purpose platform for people to build their own metaprograms can enable them to each solve the issues to their satisfaction. And once you have this platform, you don&amp;rsquo;t need any of the Urbit operating system, because you already have a Houyhnhnm computing system.&lt;/p&gt;

&lt;p&gt;Houyhnhnms have no ill feelings towards either Martians or Humans. They hope that Urbit will be a great success, and demonstrate a lot of cool things and inspire people to adopt orthogonal persistence. However, Houyhnhnms believe that Urbit won&amp;rsquo;t be able to outgrow being a cool demo unless it embraces a more general purpose metaprogramming architecture.&lt;/p&gt;&lt;/html&gt;</description></item>
  <item>
   <title>Chapter 4: Turtling down the Tower of Babel</title>
   <link>http://ngnghm.github.io/blog/2015/08/24/chapter-4-turtling-down-the-tower-of-babel/?utm_source=Persistence&amp;utm_medium=RSS</link>
   <guid>urn:http-ngnghm-github-io:-blog-2015-08-24-chapter-4-turtling-down-the-tower-of-babel</guid>
   <pubDate>Mon, 24 Aug 2015 23:51:01 UT</pubDate>
   <description>&lt;html&gt;
&lt;p&gt;Ngnghm examined how manual persistence was managed underneath Human computer systems, and contrasted with how Houyhnhnms automated its implementation. This led him to more general remarks about the compared architectures of Human computer systems and Houyhnhnm computing systems: Houyhnhnm computing systems can and do go meta, which to them is notionally &lt;em&gt;down&lt;/em&gt; (not &lt;em&gt;up&lt;/em&gt;, as some Humans would have it). Going meta allows Houyhnhm computing systems to enjoy qualities not found in Human computer systems, that can&amp;rsquo;t go meta.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="stacked-up-against-quality"&gt;Stacked up against Quality&lt;/h3&gt;

&lt;p&gt;Ngnghm wanted to know how Humans dealt with &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;manual persistence&lt;/a&gt;. He found that we were using an large quantity of mutually incompatible and often fragile &amp;ldquo;libraries&amp;rdquo; in each of many loose categories that each implement some aspect of persistence: &amp;ldquo;I/O&amp;rdquo;, &amp;ldquo;file formats&amp;rdquo;, &amp;ldquo;serialization&amp;rdquo;, &amp;ldquo;marshalling&amp;rdquo;, &amp;ldquo;markup languages&amp;rdquo;, &amp;ldquo;XML schemas&amp;rdquo;, &amp;ldquo;communication protocols&amp;rdquo;, &amp;ldquo;interchange formats&amp;rdquo;, &amp;ldquo;memory layout&amp;rdquo;, &amp;ldquo;database schema&amp;rdquo;, &amp;ldquo;database servers&amp;rdquo;, &amp;ldquo;query languages&amp;rdquo;, &amp;ldquo;object relational mapping&amp;rdquo;, &amp;ldquo;object request brokers&amp;rdquo;, &amp;ldquo;foreign function interface&amp;rdquo;, and many &amp;ldquo;wrappers&amp;rdquo;, &amp;ldquo;adapters&amp;rdquo; and &amp;ldquo;glue layers&amp;rdquo; to make them work together. Indeed, some old IBM study had estimated that 30% of all application code written was related to the basic functions of saving data and restoring it â€” and at least my experience suggests that this estimate might still be valid to this day. Houyhnhnms, like Dijkstra, regard this as a huge cost: &lt;a href="https://www.cs.utexas.edu/~EWD/transcriptions/EWD10xx/EWD1036.html"&gt;if we wish to count lines of code, we should not regard them as &amp;ldquo;lines produced&amp;rdquo; but as &amp;ldquo;lines spent&amp;rdquo;: the current conventional wisdom is so foolish as to book that count on the wrong side of the ledger.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unhappily, that huge cost also comes with limited benefits, because a program can only manipulate an object if it gets the entire large tower of libraries, the &amp;ldquo;software stack&amp;rdquo;, just right, and thus two objects built on top of incompatible &amp;ldquo;software stacks&amp;rdquo; cannot interoperate. Costly adapters can be written to bridge between the two towers, but this not only requires extra copying and management by programmers, this also loses any atomicity properties of transactions between the two object systems â€” and isn&amp;rsquo;t accessible to casual users, who thus pain to manage their data.&lt;/p&gt;

&lt;p&gt;Moreover, the above estimate did not include the error handling strategies when the above failed; meanwhile, the complexity of these baroque towers incur enormous security risks. Indeed, a lot of &amp;ldquo;layers&amp;rdquo; in these software &amp;ldquo;stacks&amp;rdquo; are written in unsafe low-level languages for reasons of alleged &amp;ldquo;performance&amp;rdquo; or &amp;ldquo;compatibility&amp;rdquo;, whereas another (overlapping) lot of such &amp;ldquo;layers&amp;rdquo; include some complex &lt;a href="https://www.usenix.org/system/files/login/articles/login_aug15_02_bratus.pdf"&gt;manual parsing&lt;/a&gt; of data going through the layer, that are as many points where attackers may inject unwanted behavior; these many layers further interact in ways that make it nearly impossible to assess the overall semantics of the system, much less its security properties. As for performance, a lot of it is wasted just crossing the layers at runtime, rather than e.g. folding them at compile-time.&lt;/p&gt;

&lt;p&gt;This architecture in software towers is thus detrimental not only to persistence, but also to robustness, to security, to performance, to upgradability, to maintainability, etc., â€” all the qualities that managers of Human computer development projects often demote as being &amp;ldquo;non-functional&amp;rdquo;, because their development processes are so deeply dysfunctional, at least from the Houyhnhnm point of view: by neglecting as an afterthought aspects of software development that are not directly visible through a quick test of a software artifact, these processes ensure that those aspects cannot be addressed properly. By contrast, Houyhnhnm computing systems consider as primary the processes of software development and use, not the artifacts; they thus consider the above aspects as primary properties of the overall system, that are important to address as part of the architecture of the softwaring process.&lt;/p&gt;

&lt;h3 id="meta-level-strategies"&gt;Meta-level Strategies&lt;/h3&gt;

&lt;p&gt;Houyhnhnms do not have any library to manage persistence; instead, Houyhnhnms have a number of libraries to manage transience. Indeed, persistence is a system-wide protocol, universally provided using generic strategies, and comes for free to users and programmers alike; they don&amp;rsquo;t have to manually flush main memory buffers to mass storage any more than they have to manually flush memory cache lines to main memory buffers, or to manually spill processor registers to memory cache lines. But if they care about extra performance, they can manage these things indeed, and escape or improve the system-provided strategies. In other words, correctness, safety, etc., come for free, and it takes extra effort for a variable &lt;em&gt;not&lt;/em&gt; to be saved, for infinite undo &lt;em&gt;not&lt;/em&gt; to be available, etc., â€” and for extra performance to be squeezed out of otherwise working programs. I already mentioned in &lt;a href="/blog/2015/08/09/chapter-3-the-houyhnhnm-version-of-salvation/"&gt;the previous chapter&lt;/a&gt; many things that you might want not to persist altogether, or for which to only keep episodic backups. More interesting are the cases where you may want to extend the system to more efficiently support some data type (say, domain-specific compression), some consensus protocol (say, a variant of the PAXOS algorithm), some reconciliation process (say, a new CRDT), or some resource ownership discipline (say, a variant of linear logic). Then you want to specify a new implementation strategy for common system protocols; and for this you usually specify a modular incremental variant of the openly-accessible existing strategies.&lt;/p&gt;

&lt;p&gt;Unlike what you&amp;rsquo;d use in Human computer systems, these strategies are not merely runtime libraries that you link to, the APIs of which programs must explicitly call â€” this would require every program to be modified any time you change a persistence strategy (alternatively, every program would have to use very rigid virtual machine, with either a very slow interpreter or a very expensive compiler). Instead, persistence strategies are meta-level software modifications that customize the implementation of the usual programming languages. Thus, these strategies can arbitrarily instrument the code generated for existing programs, to automatically add any required call to suitable libraries, but also to efficiently handle any associated bookkeeping, depending on what strategies are in the &lt;em&gt;domain&lt;/em&gt; in which the unmodified programs are run. Updated objects may be marked, either individually, in &amp;ldquo;cards&amp;rdquo; or in &amp;ldquo;pages&amp;rdquo; for the sake garbage collection or persistence; counts or sets of local or remote references may be maintained; drawing pictures may be achieved either by blitting directly to video memory or by issuing requests to some server; some type system may be enforced through some combination of static inference and dynamic checks; etc. Of course, these implementation strategies may reject requests to create or move a process into a domain where some incompatibility exists: the program might not pass some static type checks; it might fail to possess appropriate permissions, or sufficient resources, etc. Then the user or programmer may have to modify his program or try a different strategy.&lt;/p&gt;

&lt;p&gt;Importantly, this variety of strategies is made possible because Houyhnhnm computing systems are first-class entities abstracted from any specific implementation strategy. Therefore, a very same process (which includes not only source program, but also running state) may be run with different strategies â€” and indeed with strategies that vary during its execution. When you write a program, the source language you choose completely specifies allowed behavior, and all strategies are guaranteed to preserve this behavior, no more, no less.&lt;/p&gt;

&lt;p&gt;Of course, either at the time you start the program or later, you may decide to constrain the process to only use a given subset of strategies: this actually means that you really wanted a more specific program in a more specific language than initially declared. Not only is that fine, that&amp;rsquo;s a common and recommended way of writing programs: always specify the program&amp;rsquo;s behavior at as high-level as you can, to make it easier to reason about it; yet make sure the optimization strategies you require have been applied, so the performance profile isn&amp;rsquo;t surprisingly bad. As a trivial example, the Fibonacci function would be specified with its usual equational definition, but would typically be annotated with a compile-time assertion that the linear recursion recognizer has kicked in, at which point the system guarantees that the function will be computed in constant time for small values, and polylog time for big ones â€” rather than exponential time, with a naive implementation.&lt;/p&gt;

&lt;p&gt;Formally speaking, if you wrote a program in abstract language &lt;em&gt;A&lt;/em&gt;, and specify a given implementation &lt;em&gt;I&lt;/em&gt; of language &lt;em&gt;A&lt;/em&gt; generating code in concrete language &lt;em&gt;C&lt;/em&gt;, then you actually specified a program in language &lt;em&gt;C&lt;/em&gt;. And as long as you don&amp;rsquo;t proceed to make modifications at the lower level of language &lt;em&gt;C&lt;/em&gt; that invalidate the abstraction to language &lt;em&gt;A&lt;/em&gt;, then you can remove the constraint, go back to the level of program &lt;em&gt;A&lt;/em&gt;, and later choose a different implementation &lt;em&gt;I&amp;rsquo;&lt;/em&gt; targetting language &lt;em&gt;C&amp;rsquo;&lt;/em&gt;. That&amp;rsquo;s how you migrate a process from one domain to another. (This ability to do generalized migration also requires having formalized the notion of an implementation such that you can interrupt and decompile a process, including running state, and not just source code, from its concrete implementation back to the level of abstraction at which the user has chosen to interact with it â€” but that&amp;rsquo;s a topic for a future chapter.)&lt;/p&gt;

&lt;h3 id="anything-you-can-do-i-can-do-meta"&gt;Anything You Can Do I Can Do Meta&lt;/h3&gt;

&lt;p&gt;In Houyhnhnm computing systems, programs are thus persistent by default (as well as type-safe, and safe in many other ways); yet they can be made faster and smaller by locally dropping to lower levels of abstraction in structured ways that preserve higher level of semantics. This generalizes the remark made by Paul Graham that, on Lisp, as compared to other languages, &amp;ldquo;You can get fast programs, but you have to work for them. In this respect, using Lisp is like living in a rich country instead of a poor one: it may seem unfortunate that one has to work so as to stay thin, but surely this is better than working to stay alive, and being thin as a matter of course.&amp;rdquo; This doesn&amp;rsquo;t mean that the default mode of operation is especially slow or wasteful of memory: given a fixed amount of development resources, accumulating reusable automated strategies as in Houyhnhnm computing systems can achieve more performance than manually implementing strategies in every program like in Human computer systems.&lt;/p&gt;

&lt;p&gt;Indeed, manual implementation of software strategies, known in the Human computer industry as &amp;ldquo;design patterns&amp;rdquo;, is the primary source of bad quality in software: humans are just so much worse than machines (not to mention slower and more expensive) at applying algorithmic strategies â€” which notably goes against the &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;Sacred Motto of the Guild of Houyhnhnm Programmers&lt;/a&gt;. (Of course, quality is &lt;em&gt;even worse&lt;/em&gt; when the underlying design patterns have not even been recognized and their properties haven&amp;rsquo;t even been semi-formalized between sentients.) Now, errors can be made when writing the meta-program that automates the strategy â€” but it&amp;rsquo;s much easier to debug one simple general meta-program once than thousands of context-specific manual instances of the pattern that each had to precisely match the pattern in excruciating details. What more, without automation, it&amp;rsquo;s much harder to keep these myriads of instances right as the pattern or its parameters change, and maintenance requires all of them to be modified accordingly. As Rich Hickey quipped, &lt;em&gt;(Design) patterns mean &amp;ldquo;I have run out of language.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Because software strategies ultimately preserve the safe semantics of high-level languages, they involve less code written in unsafe low-level languages, and what low-level code is generated can be automatically and verifiably made to preserve high-level invariants that matter for safety. Entire classes of bugs that commonly plague Human computer systems thus never appear in Houyhnhnm computing systems. Of course, Houyhnhnms make many mistakes while developing their computing systems, and the inconsistent strategies they write can cause inconsistent behavior, with catastrophic consequences. But virtualization ensures that these catastrophes do not escape the narrow scope of the sandbox in which the developer is trying them; and catastrophic effects are actually easier to detect, so that most such bugs are faster to fix. Subtle meta-level bugs causing delayed catastrophes, though they exist, are quite rare. To eliminate them, the usual combination of testing and formal methods can help. There again, generic code is usually harder to test or formalize than a single specific instance of the code, but much easier to test or formalize than thousands or millions of instances, as necessarily happens when strategies are applied manually rather than automatically.&lt;/p&gt;

&lt;p&gt;Finally, because Houyhnhnm computing systems work at the level of abstract data types, most messaging happens with robust system-provided pairs of printers and parsers, rather than an ever renewed collection of &lt;em&gt;ad hoc&lt;/em&gt; manual printers and parsers for manually designed interchange languages, each introducing a renewed layer of bugs. Indeed, in Human computer systems, the humans who &amp;ldquo;design&amp;rdquo; these interchange languages are often unaware that they are designing languages indeed, or in deep denial when confronted to that fact; they thus prefer to remain ignorant of the very basics of language design, and ever repeat all the beginners&amp;rsquo; mistakes. In Houyhnhnm computing systems, it is understood that whatever interactions happen between sentient beings and/or automated processes by definition constitute a language; and while you want the overall design interaction between sentient being and machine to happen at the highest possible level using as expressive a language as possible, the interactions between automated processes should happen using the highest level but least expressive language possible, so they remain easier to analyze.&lt;/p&gt;

&lt;p&gt;Therefore, when contrasted to Human computer systems, it appears that Houyhnhnm computing system thus achieve &lt;em&gt;better&lt;/em&gt; quality through &lt;em&gt;meta&lt;/em&gt; programming.&lt;/p&gt;

&lt;h3 id="building-up-vs-building-down"&gt;Building up vs building down&lt;/h3&gt;

&lt;p&gt;Humans can only build software but &lt;em&gt;up&lt;/em&gt;. Houyhnhnms can build both up &lt;em&gt;and&lt;/em&gt; down.&lt;/p&gt;

&lt;p&gt;All computer software has to start from a given &lt;em&gt;base&lt;/em&gt;: whatever abstractions the operating system provides, or, in absence of operating system, the &amp;ldquo;bare metal&amp;rdquo; â€” which for Human computer systems is often not quite so bare these days, with plenty of firmware, coprocessors and virtualization layers involved. Now, Human computer systems are built by piling layers upon layers on top of this base; and a Human operating system itself can be already considered such a tower of layers, on top of which to build higher towers. One limitation of Human computer systems, though, is that to cooperate on the same data structures, programs typically have to reuse the very exact same tower of layers. Because each layer adds a lot of informal underspecified details, and it is impossible to reproduce computations or assume that programs have similar enough semantics unless they are identical from the ground up. With this tower architecture, as with the legendary Tower of Babel, people are divided by a confusing diversity of languages that prevent them from communicating.&lt;/p&gt;

&lt;p&gt;Now, it is actually important to share data between different programs. Human software developers thus onerously build &lt;em&gt;abstractions&lt;/em&gt;, without system support, so that they may save files in one format, which will hopefully be implemented in a compatible enough way by the other program or next version of the program. The operating system itself is such an abstraction, trying to present a uniform view of the computer to programs that run on top of it, despite a wild variety of underlying computers; so are to a point various virtual machines, or programming language specifications. So is, more trivially, the informal promise in successive versions of the &amp;ldquo;same&amp;rdquo; program to keep working with data saved by previous versions. Yet, any given abstraction usually has at most one sensible implementation on any given Human computer system.&lt;/p&gt;

&lt;p&gt;Slightly more advanced Human computer systems, using macros, can at compile time lift the system up and add a number of layers below. For an extreme case, some &lt;a href="http://www.cliki.net/screamer"&gt;Common Lisp&lt;/a&gt; &lt;a href="http://quickdocs.org/hu.dwim.delico/api"&gt;libraries&lt;/a&gt; reimplement Common Lisp in Common Lisp to add first-class multiple-entry or even serializable continuations, so as to enable logic programming or direct-style web programming. Some interactive development systems also instrument the virtual machine so as to lift execution into something that allows for debugging, with &lt;a href="http://www.drdobbs.com/tools/omniscient-debugging/184406101"&gt;Omniscient Debugging&lt;/a&gt; as an extreme example. But even then, once the program is built, once the runtime has been chosen, once the program has started running, the system remains forever grounded on top of the chosen basis.&lt;/p&gt;

&lt;p&gt;Houyhnhnm computer systems, by contrast, can dynamically add new layers below a running program: not only can you add a layer on top of any existing tower before you start using it, you can add or replace layers below the tower, or anywhere in the middle of it, while you are using it. This ability to build &lt;em&gt;down&lt;/em&gt; as well as &lt;em&gt;up&lt;/em&gt; crucially relies on processes being specified in formally well-defined high-level languages, so that it is always clear what are the semantics to be preserved when modifying the underlying implementation. Therefore, Houyhnhnms don&amp;rsquo;t even have a fixed notion of ground or base. Rather than rigid towers of stone being built up, they have living worlds that stand on an indefinite number of other living worlds, just like the turtles of the common joke, whereby there are &lt;a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down"&gt;&lt;em&gt;turtles all the way down&lt;/em&gt;&lt;/a&gt;. Then Houyhnhnms can lift the stack of turtles at any desired point and add or replace some of the turtles beneath, all while the system keeps running. Every turtle is unique, but no turtle is special.&lt;/p&gt;

&lt;p&gt;The superficial differences between Houyhnhnm computing systems and Human computer systems are thus the reflection of radical differences between their underlying software architectures â€” that once again, derive from the initial divergence in &lt;em&gt;point of view&lt;/em&gt;: considering the entire sentient-machine processes, rather than focusing only on the finished machine artifacts.&lt;/p&gt;&lt;/html&gt;</description></item>
  <item>
   <title>Chapter 3: The Houyhnhnm Version of Salvation</title>
   <link>http://ngnghm.github.io/blog/2015/08/09/chapter-3-the-houyhnhnm-version-of-salvation/?utm_source=Persistence&amp;utm_medium=RSS</link>
   <guid>urn:http-ngnghm-github-io:-blog-2015-08-09-chapter-3-the-houyhnhnm-version-of-salvation</guid>
   <pubDate>Sun, 09 Aug 2015 05:10:00 UT</pubDate>
   <description>&lt;html&gt;
&lt;p&gt;Following our &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;discussion on persistence&lt;/a&gt;, Ngnghm had plenty of questions about how Human computer systems held together when they can&amp;rsquo;t seem to get basic persistence right. But in return, I had even more questions about what Houyhnhnm computing systems could even be like, when all data persisted by default: What did the user interface look like? Was there no more save button? What happened when you copied or deleted files? Were there files at all? How did people deal with all the garbage? Were your mistakes forever? If you somehow hosed your machine, would it remain forever hosed? How did you test potentially dangerous changes?&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="when-data-persists"&gt;When Data Persists&lt;/h3&gt;

&lt;p&gt;Thus, on a first approach, the interface to a Houyhnhnm computing system may look very similar to that of a Human computer system â€” and that&amp;rsquo;s how Ngnghm had been fooled at first into thinking they were designed along the same principles. If you have a Human laptop that you usually put to sleep and don&amp;rsquo;t turn off or restart, your setup might be similar to what it would be on a Houyhnhnm muzzletop. But even if your operating system is very stable, you must always be prepared for a catastrophic failure; and whenever you upgrade your software (which you &lt;em&gt;must&lt;/em&gt; do eventually if only to apply security patches), then you &lt;em&gt;must&lt;/em&gt; restart just to make sure the restart procedure will be working if the failure happens while you&amp;rsquo;re disconnected from online help â€” even though your system is stable and doesn&amp;rsquo;t really need it. And at that point you lose all your current working state. The eventuality of losing all your current working state is thus a Sword of Damocles hanging above any and all working state you weave, even on the most stable of systems. Note that a Houyhnhnm computing system may have to be restarted semi-periodically for the very same reason; the difference being that you don&amp;rsquo;t lose any working state as you do â€” or else, your system administrator and/or your or his insurance will cover any damages caused. Impermanence is thus a pervasive assumption in all Human computer systems, around which all habits are built; the loss of working state can be mitigated by using &amp;ldquo;session managers&amp;rdquo; that automatically save your session, or &amp;ldquo;startup scripts&amp;rdquo; where you manually re-create your usual work session, but both approaches only save very partial session information, in addition to being quite fragile and unreliable in practice â€” and intensive in programmer effort to implement. By contrast, in a Houyhnhnm computing system, you never lose your session; moreover, you can extract a startup script to re-create a similar session on another system, by introspecting the state of the system (the support for which is necessarily present for the sake of persistence) or by selectively replaying the relevant parts of the system persistence log.&lt;/p&gt;

&lt;h3 id="system-wide-versioning"&gt;System-wide Versioning&lt;/h3&gt;

&lt;p&gt;In Human computer systems, editors and other applications have a &amp;ldquo;save&amp;rdquo; button. In Houyhnhnm computing systems, there are no applications and no &amp;ldquo;save&amp;rdquo; buttons. Instead, there are components that each deal with the specific aspects of some kinds of documents or data, and otherwise share common system features for general aspects of data â€” including notions of versioning and releasing, i.e. publishing a stable version so it&amp;rsquo;s visible to other people, whereas intermediate changes and their inglorious details remain unpublished. Thus, instead of &amp;ldquo;text editors&amp;rdquo; and &amp;ldquo;picture editors&amp;rdquo;, there are &amp;ldquo;text editing components&amp;rdquo; and &amp;ldquo;picture-editing components&amp;rdquo;, that are available anywhere that there are modifiable texts or pictures in the system â€” and pretty much any text or picture you see can be copied into an editable variant, or traced back to its source, which can be forked and edited, if it&amp;rsquo;s not directly editable already. In Human computer systems, programmers have to bundle a finite number of such components into the package-deal that is an &amp;ldquo;application&amp;rdquo;, where you can&amp;rsquo;t use the component you want without being stuck with those you don&amp;rsquo;t want. In Houyhnhnm computing systems, users can individually configure the components or combinations of components they want to use for each type of data they are interested in. They all delegate their versioning aspect to the user&amp;rsquo;s favorite versioning component, that will handle forking new branches of data, and branches off branches of data, merging data from multiple branches, atomically committing changes, releasing data from a subbranch to make its changes available to a wider branch, etc.&lt;/p&gt;

&lt;p&gt;Another advantage of system-wide versioning is that in Houyhnhnm computing systems, infinite undo comes for free on any kind of data, without any special effort from the developer, and without any limitation for the user; what more it is available atomically for all data in the system or any joint subset thereof. By contrast, in Human computer systems, the ability to undo a few steps for a few kinds of documents is a very costly, unreliable and/or error-prone operation requiring a lot of programming and a lot of maintenance, and working on one document at a time; some applications maintain history, but it is optimized for data mining by spies, and useless for users to recover past sessions. One more feature made possible by system-wide versioning is the ability to easily reproduce and isolate bugs â€” an activity that consumes a lot of expensive programmer time in Human computer systems, and that is made much easier in Houyhnhnm computing systems, since the log of interaction events that led to the erroneous behavior was recorded and can be replayed until the behavior was narrowed down; then the error case can automatically be reduced to its essence, by shaking the tree of actually used dependencies as detected by re-running an instrumented version of the same code to achieve e.g. &lt;a href="http://www.drdobbs.com/tools/omniscient-debugging/184406101"&gt;Omniscient Debugging&lt;/a&gt;; the test case is then ready for inclusion in a regression test suite.&lt;/p&gt;

&lt;h3 id="data-at-the-proper-level-of-abstraction"&gt;Data at the Proper Level of Abstraction&lt;/h3&gt;

&lt;p&gt;Because persistence in Human Computer Systems consists in communicating sequences of bytes to external processes and systems (whether disks or clouds of servers), all data they hold is ultimately defined in terms of sequences of bytes, or files; when persisting these files, they are identified by file paths that themselves are short sequences of bytes interpreted as a series of names separated by slashes &lt;code&gt;/&lt;/code&gt; (or on some systems, backslashes &lt;code&gt;\&lt;/code&gt;, or something else). Because persistence in Houyhnhnm Computing Systems applies to any data in whichever high-level language it was defined, all Houyhnhnm computing data is defined in terms of &lt;a href="https://en.wikipedia.org/wiki/Algebraic_data_type"&gt;Algebraic Data Types&lt;/a&gt;, independently from underlying encoding (which might automatically and atomically change later system-wide). For the sake of importing data in and out of independently evolving systems, as well as for the sake of keeping the data compressed to make the most of limited resources, some low-level encoding in terms of bytes may be defined for some data types. But on the one hand, this is the exception; on the other other, the data is still part of the regular Algebraic Data Type system, can still be used with type constructors (e.g. as part of sum, product or function types), etc. Whereas Human computer systems would require explicit serialization and deserialization of data, and would require ad hoc containers or protocol generators to allow larger objects to contain smaller ones, Houyhnhnm computing systems abstract those details away and generate any required code from type definitions. Low-level encodings can even be replaced by newer and improved ones, and all objects will be transparently upgraded in due time â€” while preserving all user-visible identities and relationships across such changes in representation.&lt;/p&gt;

&lt;p&gt;Since objects are not defined in terms of sequences of bytes, the very notion of file doesn&amp;rsquo;t apply to Houyhnhnm computing systems. At the same time, accessing an object inside a data structure is often (though not always) conveniently represented as following an access path from the root of the data structure to the desired element. An &amp;ldquo;access path&amp;rdquo; is thus a natural notion in all computing systems &lt;!--
see Clojure: assoc-in, update-in--&gt; even though in general a path is not a sequence of strings separated by slashes, but a list of accessors, that may be symbols or strings (when accessing a dictionary), integers (when accessing a sequence by index), or arbitrary accessor functions. But few are the cases where the natural way to locate data is via a list of sequences of bytes containing neither ASCII slash nor ASCII NUL; or worse, sequences of Unicode code glyphs up to some subtle case-conversion, represented as UTF&amp;ndash;8 code points in some normal form; or even worse, the greatest common denominator between an underspecified set of several variants of the above, with unspecified separators.&lt;/p&gt;

&lt;p&gt;Thus, files are not the general case for persisting data; text files even less so. Still, a good text editor and good text-based diff tools can provide a handy way to view and modify data and view and act on modifications to data. Indeed, unless and until you have better tools to represent change between arbitrary data structures, it makes sense to translate otherwise unsupported data structures to and from a well supported generic data structure such as text. &lt;a href="http://www.cs.yale.edu/homes/perlis-alan/quotes.html"&gt;&lt;em&gt;It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures.&lt;/em&gt;&lt;/a&gt; Now, it is important to understand the distinction between a representation and the real thing; the text being presented is not &amp;ldquo;canonical&amp;rdquo;, it is not usually &amp;ldquo;source&amp;rdquo;. In Houyhnhnm computing systems, the source is the semantic state of the system, on which change happens, and from which the text is extracted if and when needed; this is in sharp contrast with typical Human computer systems, where the source (that is, the locus of modification by sentients) is text files that are compiled, disconnected from the state of the system.&lt;/p&gt;

&lt;h3 id="dealing-with-bad-memories"&gt;Dealing with Bad Memories&lt;/h3&gt;

&lt;p&gt;But, I inquired, if they log everything and almost never forget anything, don&amp;rsquo;t Houyhnhnm computing system quickly get filled with garbage? No, replied Ngnghm. The amount of information that users enter through a keyboard and mouse (or their Houyhnhnm counterparts) is minute compared to the memory of modern computers, yet, starting from a well-determined state of the system, it fully determines the subsequent state of the system. Hence, the persistence log doesn&amp;rsquo;t need to record anything else but these events with their proper timestamp. This however, requires that all sources of non-determinism are either eliminated or recorded â€” which Houyhnhnm computing systems do by construction. Of course, to save resources, you can also configure some computations so they are not recorded, or so their records aren&amp;rsquo;t kept beyond some number of days. For instance, you might adopt a &lt;a href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller"&gt;&lt;em&gt;model-view-controller&lt;/em&gt;&lt;/a&gt; approach, and consider the view as transient while only logging changes to the model and the controller, or even only to the model; or you might eschew long-term storage of your game sessions; or you might forget the awkward silences and the street noise from your always-on microphone; or you might drop data acquired by your surveillance camera when it didn&amp;rsquo;t catch any robber; or you might delete uninteresting videos; or you might expunge old software installation backups from long gone computers; or you might preserve a complete log only for a day, then an hourly snapshot for a few days, and a daily snapshot for a few weeks, a weekly snapshot for a few months, etc.; or you might obliterate logs and snapshots as fast as you can while still ensuring that the system will be able to withstand partial or total hardware failure of your personal device; or then again, given enough storage, you might decide to keep &lt;em&gt;everything&lt;/em&gt;. It&amp;rsquo;s your choice â€” as long as you pay for the storage. The decision doesn&amp;rsquo;t have to be made by the programmer, though he may provide hints: the end-user has the last say. Indeed, the interests of the programmer may not be aligned with those of the user; if he needs to delegate these decisions, the user will thus entrust administrators he pays, whose interests are better aligned because they will be liable in case of malpractice.&lt;/p&gt;

&lt;p&gt;Now, beyond clutter that uselessly fills up memory, what about actively bad things that surely you don&amp;rsquo;t want to memorize? Some mistake might cause your entire system to become unresponsive by resource exhaustion (a fork bomb on Unix, an out-of-memory situation on any system); something might trigger a system bug and cause a hardware crash, a Blue Screen Of Death or a kernel panic; even worse, some subtle combination of factors could generate a memory corruption that jeopardizes the integrity of the persistent data. Houyhnhnms computing systems may be more robust than Human computer systems in this regard, yet even Houyhnhnms are not perfect in avoiding catastrophic mistakes. If you detect such a situation, what do you do? Old Houyhnhnm engineers tell classic stories of catastrophic system modifications that were reverted by shutting down the computer before the modification was written to disk; but of course, as the latency of persistence goes down, the window of opportunity for such a feat goes away. The general answer is that to fix a system that has entered a bad state, you need an &lt;em&gt;external&lt;/em&gt; system that can stop the broken system, inspect it, fix it, and restart it.&lt;/p&gt;

&lt;p&gt;On a Human computer system, when things get that bad, you can often reboot in a special &amp;ldquo;failsafe&amp;rdquo; mode (that can usually handle but the simplest of situations), or you can use a USB key with a known stable version of the system (with which experts can handle complex situations), or at worst if the hardware was damaged, you can disconnect the computer&amp;rsquo;s mass memory unit and connect it into another computer. In a Houyhnhnm computing system, you can do as much, but you can also use a reserved input sequence (the equivalent of 
 &lt;kbd&gt;Ctrl-Alt-Del&lt;/kbd&gt; on Windows) to enter a &lt;em&gt;monitor&lt;/em&gt;. The monitor is a &lt;em&gt;simple&lt;/em&gt; but complete computing system, as per the &lt;a href="/blog/2015/08/02/chapter-1-the-way-houyhnhnms-compute/"&gt;Houyhnhnm criteria of simplicity&lt;/a&gt;; it is universal and can do everything a computing system can do, and is often a bare-bones variant of a regular computing system, as used for secure bootstrapping purposes; it also specifically understands enough of the semantics of the regular system to inspect it, fix it, and restart it, using the full power of a complete computing system (though a simple one). A small amount of memory is reserved for the operation of the monitor; actually, if mass memory units are working (as they should be) and have some reserved space for the monitor (as is the case on a default installation), then the monitor will actually spawn a virtualized monitor; this allows monitor operations to have more memory available, so they can for instance merge in a lot of the system state (up to some point deemed safe by the user); but this also makes it possible to still have a monitor (and possibly more virtualized monitors) in case you make mistakes in the virtualized monitor. As a result, it is safe to use dichotomy (binary search) to determine which change broke the system.&lt;/p&gt;

&lt;h3 id="virtualization-as-branching"&gt;Virtualization as Branching&lt;/h3&gt;

&lt;p&gt;More generally, a Houyhnhnm can use virtualization and system rollback while conducting any kinds of experiments, so he never has to hesitate about doing anything risky, half-baked, downright stupid, or otherwise dangerous. But virtualization doesn&amp;rsquo;t mean the same thing in a Houyhnhnm computing system as in a Human computer system. In a Human computer system, virtualization is an &lt;em&gt;ad hoc&lt;/em&gt; tool for system administrators, that allows the deployment of specially prepared servers; it is implemented using heroic techniques, by faking an entire physical computer at the level of abstraction of CPU instructions and memory accesses; the awkward result requires extra care and makes the users&amp;rsquo; life overall more miserable rather than less, though it is sometimes an economic necessity. In a Houyhnhnm computing system, virtualization is merely the ability to branch the history of changes in the system, and derives naturally from the fact that the entire system is under version control; it is available at whichever level of abstraction the users and programmers specify their computations, which is most usually at a much higher level of abstraction than that of CPU instructions (though a naive, fallback strategy is indeed always available that consists in going down to that low level).&lt;/p&gt;

&lt;p&gt;Thus, all destructive or catastrophic experiments happen in branches that are never merged into the official reality â€” the errors remain imaginary. Alternatively, when bad things happen in said official reality, they can be &lt;a href="http://www.jargon.net/jargonfile/r/retcon.html"&gt;retcon&lt;/a&gt;&amp;rsquo;ed into having been but a bad dream, as the bad reality is demoted into being a mere unmerged error branch while a nicer reality becomes the master copy. Of course, it may be too late to undo bad communications with external systems, if they were let happen. Some of them may be cancelled or compensated with new transactions. Some of them may have to be accepted as losses or errors: you can&amp;rsquo;t unprint a pile of garbled characters, and you won&amp;rsquo;t get your bitcoins back. Even Houyhnhnm computing systems can&amp;rsquo;t protect you from yourself; but they can make it easy to try things in a virtualized environment, and to only merge into reality those transactions that pass all checks.&lt;/p&gt;

&lt;p&gt;It is also possible to branch only part of the system while the rest of the system remains shared; and of course you can merge two branches back together, somehow fusing changes. Thus, there is no need to specially prepare an image for virtualization; any system and any subsystem, any program and any subprogram, can be forked at any time; any ongoing I/O with other systems can be redirected to one fork or the other, or multiplexed to/from both, or filtered, etc., and the user can dynamically re-wire all these connections from a monitor outside the virtualized system, that can itself be virtualized, etc.&lt;/p&gt;

&lt;p&gt;Version control should be familiar to developers of Human computer systems; but these days, they apply it only to source code; and so live, interactive data is not covered by the version control, or at best only in a very indirect way, if the programmers make large, contrived and expensive efforts to take regular snapshots or, which is harder, to check in every change. Houyhnhnms think of code and data as coming together, part of the same interaction with the Sentient user, with data and code being useless without the other, or out of synch with the other; and thus Houyhnhnm computing systems casually apply version control to the entire state of the system.&lt;/p&gt;

&lt;p&gt;In the end, thinking like Houyhnhnms in terms of &lt;em&gt;computing&lt;/em&gt; systems, rather than like Humans in terms of &lt;em&gt;computer&lt;/em&gt; systems has far-ranging consequences in terms of software and hardware architecture. Persistence is but one aspect of this architecture, though ultimately, it cannot be separated from the rest. And on this aspect like on others, from the necessity of dealing with the same basic needs and failure scenarios, the change in &lt;em&gt;point of view&lt;/em&gt; leads to very different approaches to making and keeping the systems working.&lt;/p&gt;&lt;/html&gt;</description></item>
  <item>
   <title>Chapter 2: Save Our Souls</title>
   <link>http://ngnghm.github.io/blog/2015/08/03/chapter-2-save-our-souls/?utm_source=Persistence&amp;utm_medium=RSS</link>
   <guid>urn:http-ngnghm-github-io:-blog-2015-08-03-chapter-2-save-our-souls</guid>
   <pubDate>Mon, 03 Aug 2015 05:10:00 UT</pubDate>
   <description>&lt;html&gt;
&lt;p&gt;&lt;a href="/blog/2015/08/02/chapter-1-the-way-houyhnhnms-compute/"&gt;Ngnghm&lt;/a&gt; was fascinated by our keyboards: because of physiological differences between our races, similar devices had never been imagined by &lt;a href="http://en.wikipedia.org/wiki/Houyhnhnm"&gt;Houyhnhnm&lt;/a&gt; computing engineers. Now, as he was watching me closely, Ngnghm noticed that I was punctuating most of my typing with recurring combinations of key chords. I told him I had no idea what he meant; and so he had me record and review how, after every sentence or so, or before changing activities, I was composing the sequence 
 &lt;kbd&gt;Ctrl-X Ctrl-S&lt;/kbd&gt;, or 
 &lt;kbd&gt;Command-S&lt;/kbd&gt;, or some other mantra that varied slightly with the application I was using. Interestingly, I wasn&amp;rsquo;t even aware that I was doing that before he told me! What was this mantra doing, he inquired? How could I possibly repeat it without even noticing â€” and why would I? I told him that depending on the application, each of these mantra &lt;em&gt;saved&lt;/em&gt; the current file, and that typing it had become ingrained in me as a subconscious habit, because I used it so often, out of necessity. What does "&lt;em&gt;saved&lt;/em&gt;" mean wondered Ngnghm, and what made it a necessity?&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="persistence-automated"&gt;Persistence, Automated&lt;/h3&gt;

&lt;p&gt;I explained that Human computer software and hardware are prone to crashing, or to losing battery power, and other unexpected failures â€” there he sighed with sympathy, for Houyhnhnms were just as frustrated as Humans with how unreliable their computers were. I continued that the solution universally adopted for Human computer systems was therefore that Humans had to explicitly &lt;em&gt;save&lt;/em&gt; each file for its contents to be later recoverable in the event of such a crash. Having been burned too many times by the loss of many hours of hard work, I had grown the habit of saving often, and doing it unconsciously at every pause in my thought process; thus I didn&amp;rsquo;t have to think hard to predict when the computer was at risk and explicitly decide when I ought to save. Ngnghm was properly appalled. Didn&amp;rsquo;t the system just automatically save everything I typed? Why was human thought and habit involved at all in a task that could have been fully automated long ago â€” and indeed had been automated in all but the earliest and most primitive Houyhnhnm computing systems?&lt;/p&gt;

&lt;p&gt;Although, he remarked, considering the overall computing system containing both Sapient and Computer, the task had been automated indeed. Indeed, if you came to think of it, this task couldn&amp;rsquo;t possibly &lt;em&gt;not&lt;/em&gt; be automated, unless the computing system were only used but to produce worthless data never worth keeping â€” at which point it would thus be itself worthless. However, the task had been imperfectly automated at great cost by creating a habit in my brain and hands, rather than automated both perfectly and cheaply by having it done by the computer. Certainly, building a physical habit that lightened the burden on the higher parts of my mind was better than no automation at all, but what a waste of precious wetware! At least in this instance and for this concern, the very purpose of computers had been defeated. As went the &lt;em&gt;Sacred Motto&lt;/em&gt; of the Guild of Houyhnhnm Programmers: &lt;a href="http://www.wanderings.net/notebook/Main/BitterAcknowledgmentsOfOlinShivers"&gt;&lt;em&gt;I object to doing things that computers can do&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And then, suddenly, Ngnghm became worried about his journal. He had been consigning his observations on a computer ever since he had learned to use a mouse to draw Houyhnhnm symbols in a paint application. (Ngnghm once remarked that a one-button mouse is an exquisite input device for a Houyhnhnm&amp;rsquo;s hoof, but that it takes a Yahoo to believe it is suited to a Yahoo&amp;rsquo;s hand.) Now, he admitted that he could never retrieve his old notes; but he just thought that it was due to his not understanding the Houyhnhnm-Computer Interface properly, and to his not knowing how to search back in time what he had previously drawn. He assumed that Human computers were probably not geared to properly index his observations for easy retrieval, but that they would otherwise all be logged in the computer&amp;rsquo;s memory. Was I implying that all his notes were lost, including some of the finest poetry he had ever written, as inspired both by the suffering from his predicament and the marvel at his discoveries? I won&amp;rsquo;t claim any proficiency at judging Houyhnhnm poetry â€” it all sounds like nickering and whickering to me â€” but to this day, I fear that one of the greatest pieces of Houyhnhnm literature has been lost to the world, due to the failings of Human computer systems.&lt;/p&gt;

&lt;h3 id="orthogonal-persistence"&gt;Orthogonal Persistence&lt;/h3&gt;

&lt;p&gt;Ngnghm explained to me that Houyhnhnm computing systems make data persistence the default, at every level of abstraction. Whether you open the canvas of a graphical interface and start drawing freely, or you open an interactive evaluation prompt and bind a value to a variable, or you make any kind of modification to any document or program, the change you made will remain in the system forever â€” that is, until Civilization itself crumbles, or you decide to delete it (a tricky operation, more below). Everything you type remains in your automatic planet-wide backups, providing several layers of availability and of latency â€” kept private using several layers of cryptography.&lt;/p&gt;

&lt;p&gt;Of course, you can control what does or doesn&amp;rsquo;t get backed up where, by defining domains each with its own privacy policy that may differ from the reasonable defaults. The user interface is aware of these domains, and makes it clear at all times which domain you&amp;rsquo;re currently working with. It also prevents you from inadvertently copying data from a more private domain then pasting it into a more public one; in particular, you only type your primary passwords but in a very recognizable special secure domain that never stores them; and your secondary access keys are stored in a special private domain using stronger cryptography than usual, and also subject to various safety rules to avoid leakage.&lt;/p&gt;

&lt;p&gt;Deletion (as opposed to mere de-indexing), while possible, gets more expensive as the data you want to delete gets older: logs, backups and indexes dating back to the deleted change have to be scrubbed and rewritten; the system must triple-check that everything is still in working order after this sweeping change; it must also make sure that the user is ultimately happy with the results, including with whatever might break for other users he knows who might have depended on details of the old history (assuming he shared any of it). Now, when deleting anything but most recent changes, this expensive operation will leave traces that something was deleted, though the details of what was deleted will indeed have been deleted. Of course, deletion doesn&amp;rsquo;t affect copies other people may have of the data, if you ever shared it; therefore, thou shalt not lightly share thy data, and thou shalt never share any access keys â€” but that&amp;rsquo;s true anyway. At least Houyhnhnm systems let you manage your sharing and backup policies in a systematic way, and ensure that everyone can depend on sensible, safe, defaults.&lt;/p&gt;

&lt;p&gt;In other words, Houyhnhnm computing systems have &lt;a href="http://tunes.org/wiki/orthogonal_20persistence.html"&gt;&lt;em&gt;orthogonal persistence&lt;/em&gt;&lt;/a&gt; â€” and have had it for &lt;a href="http://tunes.org/wiki/eumel.html"&gt;&lt;em&gt;decades&lt;/em&gt;&lt;/a&gt;. The adjective &amp;ldquo;orthogonal&amp;rdquo; means that the persistence of data is a property of the domain you&amp;rsquo;re working in, as managed by the system; it is &lt;em&gt;not&lt;/em&gt; an aspect of data that programmers have to deal with in most ordinary programs; unless of course they are programmers specifically working on a new abstraction for persistence, which is after all an ordinary program, just in a narrow niche. Regular programmers just manipulate the data with full confidence that the inputs they consume, the code that manipulates them, and the outputs they produce will each remain available as long as the user wants them, with the consistency guarantees specified by the user, as long as the user affords the associated costs.&lt;/p&gt;

&lt;p&gt;Actually, ordinary programs don&amp;rsquo;t know and can&amp;rsquo;t even possibly know which domain they will be running in, and letting them query those details would be a breach of abstraction, with serious security implications and performance impediments, even assuming for a moment that it wouldn&amp;rsquo;t otherwise affect program correctness. Therefore, only programs with adequate capabilities can manipulate the persistence and privacy levels of computing domains, except of course to deliberately spawn a subdomain with yet strictly fewer capabilities. The system of course can recognize privacy and performance annotations about authorized programs and automatically distribute the many components of these programs each in a suitable domain.&lt;/p&gt;

&lt;p&gt;It is important to maintain full abstraction when keeping the semantics of ordinary programs orthogonal to various concrete aspects of the computing domains: the persistence, privacy, robustness and performance (but also machine word size, endianness, memory layout, physical location of the machine, etc.). This abstraction allows the user to independently specify what domain he wants, and to later change his specification, while the program keeps running. The same abstraction allows the underlying system to independently pick the best suited or cheapest concrete implementation, and to migrate the program to a different underlying machine when the conditions change. And whether migration is prompted by user request, system adaptation, or a change of phase in the execution of the program, the concrete code to run the program can automatically be re-generated to fit the new conditions, so the program may continue running in a new domain implementation, without any interruption in its semantics (though possibly with an observable pause). Thus, the system may optimize away logging and copying in transient computations for which speed matters more than robustness; or it may introduce extra logging and extra copying when debugging existing programs (e.g. enabling &lt;a href="http://www.lambdacs.com/debugger/"&gt;Omniscient Debugging&lt;/a&gt; for a failed computation); it may automatically introduce synchronization steps in computations performed in lock-step by several redundant machines based on different architectures to ensure detection and elimination of low-level failures (or tampering); or then again it may add layers of encryption between CPU and memory where the user feels paranoid; or it may compile the code to FPGA where performance &lt;em&gt;really&lt;/em&gt; matters.&lt;/p&gt;

&lt;p&gt;The possibilities are endless, as long as the system maintains full abstraction of program semantics from the underlying implementation, as Houyhnhnm computing systems do. When on the contrary, as in Human computer systems, the code is pegged to a particular implementation, then not only is it practically impossible to migrate a program from one domain to another at runtime, but programs may have to be completely rewritten from scratch before they may even be executed in a domain with slightly different constraints regarding persistence, privacy, performance, etc.&lt;/p&gt;

&lt;h3 id="fractal-transience"&gt;Fractal Transience&lt;/h3&gt;

&lt;p&gt;Interestingly, on the visible side of the system, successful Human &amp;ldquo;apps&amp;rdquo; these days have evolved into offering to users some semblance of persistence: configuration settings, lists of open tabs, documents you manipulate â€” most user-visible application state, most of the time, seems to be preserved from one session to the next, without the user having to issue any explicit command to &amp;ldquo;save&amp;rdquo; anything. Desktop apps still tend to display a counter-productive &amp;ldquo;recovery&amp;rdquo; menu at startup, though. And more annoyingly, this apparent persistence still doesn&amp;rsquo;t cover the most frequent case these days of people typing things: input forms and message boxes in web pages. Also, the &amp;ldquo;catastrophic&amp;rdquo; events are covered include so predictable the event as is the eventual death of each and every piece of hardware â€” and of each and every software project and service-providing business. Yet, content with expectations from this &lt;em&gt;apparent&lt;/em&gt; persistence, users can easily be fooled, like Ngnghm was initially, into believing that Human computer systems are just as good as Houyhnhnm computing systems in this regard; and just like Ngnghm, they can be led to believe that failures are due to incompetence on their part, rather than on the part of the computing system developers.&lt;/p&gt;

&lt;p&gt;Well, at least, that&amp;rsquo;s how the Houyhnhnm see things: whether or not you can assign blame to any person in particular for the situation of Human computer systems, this situation is deeply dysfunctional. Actually, the Houyhnhnm also have something to say if you cannot assign personal blame for it â€” and it doesn&amp;rsquo;t look like you can: this means that the meta-system for assigning responsibilities itself is also dysfunctional. Why do &amp;ldquo;vendors&amp;rdquo; of Human computer systems by and large hoard all the freedom but none of the responsibility when it comes to modifying and maintaining their software so it doesn&amp;rsquo;t fail catastrophically and betray the customers? This is a clearly dysfunctional process according to Houyhnhnm criteria. Even when these vendors tout themselves as selling &amp;ldquo;software as a service&amp;rdquo;, they often hide behind their &amp;ldquo;Intellectual Property&amp;rdquo; monopolies to actually make it &amp;ldquo;rotware as a racket&amp;rdquo; â€” they offer &lt;a href="http://www.jargon.net/jargonfile/b/bitrot.html"&gt;bitrotting&lt;/a&gt; bad expensive service, oriented towards the vendor&amp;rsquo;s interests to the detriment of the users&amp;rsquo;, with no enforceable service level agreement, no way to extract your data in a state usable by any competing service, with the promise that the service &lt;em&gt;will&lt;/em&gt; grow even more inadequate and eventually die (being cancelled, bankrupted, or bungled), yet that you &lt;em&gt;will&lt;/em&gt; have to keep paying, and then pay again when you have to leave or be left behind; but you don&amp;rsquo;t have much choice because patents and other monopolies attract capital and provide disincentive to investment in any competition (if legally allowed at all) or in other services that don&amp;rsquo;t similarly exclude competition through legal tactics. By contrast, Houyhnhnms individually have full ultimate control over their own machines, and it is based on this control that they enjoy division of labour in delegating software maintenance of most (if they are programmers) or all (if they aren&amp;rsquo;t) of their systems to competing providers who are held individually liable in case of failure, and aren&amp;rsquo;t granted monopolies by a centralized privilege-doling entity.&lt;/p&gt;

&lt;p&gt;Now, after Ngnghm made this painful first hoof experience of the persistence failure of Human computer systems, he started investigating how Human computer systems implemented persistence, or failed to. And he discovered to his dismay that beneath the &lt;a href="http://www.www.loper-os.org/?p=448"&gt;veneer of persistence&lt;/a&gt;, there was transience at every level he was looking at â€” not just transience, but &lt;a href="http://rationalwiki.org/wiki/Fractal_wrongness"&gt;fractal transience&lt;/a&gt;: this fundamental design difference between Human and Houyhnhnm computing systems is observable at every level of these systems. The user, the programmer, the library developer, the compiler writer, the operating system implementer, everyone, all the time, has to assume the software and hardware layers below him are fragile, supposed to work only a single computing domain; everyone will in turn provide a similarly fragile and non-transportable device to the users above him. All the manual handling of persistence costs a significant fraction of software development (about 30% of all code written, an IBM study once counted); &lt;!--
https://web.archive.org/web/20060813202835/http://www.st-andrews.ac.uk/services/admissions/postgrad/schleaf5.html
...Ron Morrison...
"Well, perhaps not quite so easy. Research in persistent programming systems started in the late seventies, when it was noticed that storing 'long-term' data in a different logical framework from 'short-term' data leads to all sorts of problems in large and complex applications. An analysis by IBM showed that around 30% of the code of long-lived, large scale applications was devoted to the movement of data in and out of the programming language domain. The fact that this code is notoriously susceptible to system evolution errors, coupled with the statistic that 2% of the USA's GNP is spent on software 'maintenance', leads us to believe that storing long-term data in a file or database system is expensive."

King, F. IBM report on the contents of a sample of programs surveyed. San Jose, CA: IBM, 1978.
Notably cited by Atkinson &amp; Morrison https://dl.acm.org/citation.cfm?id=615226--&gt; and if you ever want to make a significant improvement to any component at any level, you pretty much have to rewrite the entire software &amp;ldquo;stack&amp;rdquo; above whichever level you are hoping to improve â€” in other words this requires a significant world-changing event.&lt;/p&gt;

&lt;p&gt;And yet, it runs! Ngnghm was in awe that Human computer systems could run at all; they clearly demonstrated some emerging order so powerful that it could survive despite ubiquitous design flaws â€” or could it possibly be surviving &lt;em&gt;thanks&lt;/em&gt; to what to this Houyhnhnm appeared as flaws? Ngnghm decided to pursue his investigationsâ€¦&lt;/p&gt;
&lt;!-- http://j.mp/NgnghmPersist--&gt;&lt;/html&gt;</description></item></channel></rss>