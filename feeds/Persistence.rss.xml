<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>Houyhnhnm Computing: Posts tagged 'Persistence'</title>
  <description>Houyhnhnm Computing: Posts tagged 'Persistence'</description>
  <link>http://ngnghm.github.io/tags/Persistence.html</link>
  <lastBuildDate>Mon, 24 Aug 2015 23:51:01 UT</lastBuildDate>
  <pubDate>Mon, 24 Aug 2015 23:51:01 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>Chapter 4: Turtling down the Tower of Babel</title>
   <link>http://ngnghm.github.io/blog/2015/08/24/chapter-4-turtling-down-the-tower-of-babel/?utm_source=Persistence&amp;utm_medium=RSS</link>
   <guid>urn:http-ngnghm-github-io:-blog-2015-08-24-chapter-4-turtling-down-the-tower-of-babel</guid>
   <pubDate>Mon, 24 Aug 2015 23:51:01 UT</pubDate>
   <description>&lt;html&gt;
&lt;p&gt;Ngnghm examined how manual persistence was managed underneath Human computer systems, and contrasted with how Houyhnhnms automated its implementation. This led him to more general remarks about the compared architectures of Human computer systems and Houyhnhnm computing systems: Houyhnhnm computing systems can and do go meta, which is notionally &lt;em&gt;down&lt;/em&gt;, which allows them to enjoy qualities not found in Human computer systems that can&amp;rsquo;t go meta.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="stacked-up-against-quality"&gt;Stacked up against Quality&lt;/h3&gt;

&lt;p&gt;Ngnghm wanted to know how Humans dealt with &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;manual persistence&lt;/a&gt;. He found that we were using an large quantity of mutually incompatible and often fragile &amp;ldquo;libraries&amp;rdquo; in each of many loose categories that each implement some aspect of persistence: &amp;ldquo;I/O&amp;rdquo;, &amp;ldquo;file formats&amp;rdquo;, &amp;ldquo;serialization&amp;rdquo;, &amp;ldquo;marshalling&amp;rdquo;, &amp;ldquo;markup languages&amp;rdquo;, &amp;ldquo;XML schemas&amp;rdquo;, &amp;ldquo;protocols&amp;rdquo;, &amp;ldquo;interchange formats&amp;rdquo;, &amp;ldquo;memory layout&amp;rdquo;, &amp;ldquo;database schema&amp;rdquo;, &amp;ldquo;database servers&amp;rdquo;, &amp;ldquo;query languages&amp;rdquo;, &amp;ldquo;object relational mapping&amp;rdquo;, &amp;ldquo;object request brokers&amp;rdquo;, &amp;ldquo;foreign function interface&amp;rdquo;, and many &amp;ldquo;wrappers&amp;rdquo;, &amp;ldquo;adapters&amp;rdquo; and &amp;ldquo;glue layers&amp;rdquo; to make them work together. Indeed, some old IBM study had estimated that 30% of all application code written was related to the basic functions of saving data and restoring it — and at least my experience suggests that this estimate might still be valid to this day. Houyhnhnms, like Dijkstra, regard this as a huge cost: &lt;a href="https://www.cs.utexas.edu/~EWD/transcriptions/EWD10xx/EWD1036.html"&gt;if we wish to count lines of code, we should not regard them as &amp;ldquo;lines produced&amp;rdquo; but as &amp;ldquo;lines spent&amp;rdquo;: the current conventional wisdom is so foolish as to book that count on the wrong side of the ledger.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unhappily, that huge cost also comes with limited benefits, because a program can only manipulate an object if it gets the entire large tower of libraries, the &amp;ldquo;software stack&amp;rdquo;, just right, and thus two objects built on top of incompatible &amp;ldquo;software stacks&amp;rdquo; cannot interoperate. Costly adapters can be written to bridge between the two towers, but this not only requires extra copying and management by programmers, this also loses any atomicity properties of transactions between the two object systems — and isn&amp;rsquo;t accessible to casual users, who thus pain to manage their data.&lt;/p&gt;

&lt;p&gt;Moreover, the above estimate did not include the error handling strategies when the above failed; meanwhile, the complexity of these baroque towers incur enormous security risks. Indeed, a lot of &amp;ldquo;layers&amp;rdquo; in these software &amp;ldquo;stacks&amp;rdquo; are written in unsafe low-level languages for reasons of alleged &amp;ldquo;performance&amp;rdquo; or &amp;ldquo;compatibility&amp;rdquo;, whereas another (overlapping) lot of such &amp;ldquo;layers&amp;rdquo; include some complex &lt;a href="https://www.usenix.org/system/files/login/articles/login_aug15_02_bratus.pdf"&gt;manual parsing&lt;/a&gt; of data going through the layer, that are as many points where attackers may inject unwanted behavior; these many layers further interact in ways that make it nearly impossible to assess the overall semantics of the system, much less its security properties. As for performance, a lot of it is wasted just crossing the layers at runtime, rather than e.g. folding them at compile-time.&lt;/p&gt;

&lt;p&gt;This architecture in software towers is thus detrimental not only to persistence, but also to robustness, to security, to performance, to upgradability, to maintainability, etc., — all the qualities that managers of Human computer development projects often demote as being &amp;ldquo;non-functional&amp;rdquo;, because their development processes are so deeply dysfunctional, at least from the Houyhnhnm point of view: by neglecting as an afterthought aspects of software development that are not directly visible through a quick test of a software artefact, these processes ensure that those aspects cannot be addressed properly. By contrast, Houyhnhnm computing systems consider as primary the processes of software development and use, not the artefacts, and thus consider these aspects as primary properties of the overall system, that are important to address as part of the architecture of the softwaring process.&lt;/p&gt;

&lt;h3 id="meta-level-strategies"&gt;Meta-level Strategies&lt;/h3&gt;

&lt;p&gt;Houyhnhnms do not have any library to manage persistence; instead, Houyhnhnms have a number of libraries to manage transience. Indeed, persistence is a system-wide protocol, universally provided using generic strategies, and comes for free to users and programmers alike; they don&amp;rsquo;t have to manually flush main memory buffers to mass storage any more than they have to manually flush memory cache lines to main memory buffers, or to manually spill processor registers to memory cache lines. But if they care about extra performance, they can manage these things indeed, and escape or improve the system-provided strategies. In other words, correctness, safety, etc., come for free, and it takes extra effort for a variable &lt;em&gt;not&lt;/em&gt; to be saved, for infinite undo &lt;em&gt;not&lt;/em&gt; to be available, etc., — and for extra performance to be squeezed out of otherwise working programs. I already mentioned in &lt;a href="/blog/2015/08/09/chapter-3-the-houyhnhnm-version-of-salvation/"&gt;the previous chapter&lt;/a&gt; many things that you might want not to persist altogether, or for which to only keep episodic backups. More interesting are the cases where you may want to extend the system to more efficiently support some data type (say, domain-specific compression), some consensus protocol (say, a variant of the PAXOS algorithm), some reconciliation process (say, a new CRDT), or some resource ownership discipline (say, a variant of linear logic). Then you want specify a new implementation strategy for common system protocols; and for this you usually specify a modular incremental variant of the openly-accessible existing strategies.&lt;/p&gt;

&lt;p&gt;Unlike what you&amp;rsquo;d use in Human computer systems, these strategies are not merely runtime libraries that you link to, the APIs of which programs must explicitly call — this would require every program to be modified any time you change its persistence strategy (or to use very rigid virtual machine, with either a very slow interpreter or a very expensive compiler). Instead, they are meta-level software modifications that customize the implementation of the usual programming languages. Thus, these strategies can arbitrarily instrument the code generated for existing programs, to automatically add any required call to suitable libraries, but also to efficiently handle any associated bookkeeping, depending on what strategies are in the &lt;em&gt;domain&lt;/em&gt; in which the unmodified programs are run. Updated objects may be marked, either individually, in &amp;ldquo;cards&amp;rdquo; or in &amp;ldquo;pages&amp;rdquo; for the sake garbage collection or persistence; counts or sets of local or remote references may be maintained; drawing pictures may be achieved either by blitting directly to video memory or by issuing requests to some server; some type system may be enforced through some combination of static inference or dynamic checks; etc. Of course, these implementation strategies may reject requests to create or move a process into a domain where some incompatibility exists: the program might not pass some static type checks, or possess appropriate permissions, or sufficient resources, etc. Then the user or programmer may have to modify his program or try a different strategy.&lt;/p&gt;

&lt;p&gt;Importantly, this variety of strategies is made possible because Houyhnhnm computing systems are first-class entities abstracted from any specific implementation strategy. Therefore, a very same process (which includes not only source program, but also running state) may be run with different strategies — and indeed with strategies that vary during its execution. When you write a program, the source language you choose completely specifies allowed behavior, and all strategies are guaranteed to preserve this behavior, no more, no less.&lt;/p&gt;

&lt;p&gt;Of course, either at the time you start the program or later, you may decide to constrain the process to only use a given subset of strategies: this actually means that you really wanted a more specific program in a more specific language than initially declared. Not only is that fine, that&amp;rsquo;s a common and recommended way of writing programs: always specify the program&amp;rsquo;s behavior at as high-level as you can, to make it easier to reason about it; yet make sure the optimization strategies you require have been applied, so the performance profile isn&amp;rsquo;t surprisingly bad. As a trivial example, the Fibonacci function would be specified with its usual equational definition, but would typically be annotated with a compile-time assertion that the linear recursion recognizer has kicked in, at which point the system guarantees that the function will be computed in constant time for small values, and polylog time for big ones — rather than exponential time, with a naive implementation.&lt;/p&gt;

&lt;p&gt;Formally speaking, if you wrote a program in abstract language &lt;em&gt;A&lt;/em&gt;, and specify a given implementation &lt;em&gt;I&lt;/em&gt; of language &lt;em&gt;A&lt;/em&gt; generating code in concrete language &lt;em&gt;C&lt;/em&gt;, then you actually specified a program in language &lt;em&gt;C&lt;/em&gt;. And as long as you don&amp;rsquo;t proceed to make modifications at the lower level of language &lt;em&gt;C&lt;/em&gt; that invalidate the abstraction to language &lt;em&gt;A&lt;/em&gt;, then you can remove the constraint, go back to the level of program &lt;em&gt;A&lt;/em&gt;, and later choose a different implementation &lt;em&gt;I&amp;rsquo;&lt;/em&gt; targetting language &lt;em&gt;C&amp;rsquo;&lt;/em&gt;. That&amp;rsquo;s how you migrate a process from one domain to another. (This also requires having formalized the notion of an implementation such that you can interrupt and decompile a process, including running state, and not just source code, from its concrete implementation back to the level of abstraction at which the user has chosen to interact with it — but that&amp;rsquo;s a topic for a future chapter.)&lt;/p&gt;

&lt;h3 id="anything-you-can-do-i-can-do-meta"&gt;Anything You Can Do I Can Do Meta&lt;/h3&gt;

&lt;p&gt;In Houyhnhnm computing systems, programs are thus persistent by default (as well as type-safe, and safe in many other ways); yet they can be made faster and smaller by locally dropping to lower levels of abstraction in structured ways that preserve higher level of semantics. This generalizes the remark made by Paul Graham that, on Lisp, as compared to other languages, &amp;ldquo;You can get fast programs, but you have to work for them. In this respect, using Lisp is like living in a rich country instead of a poor one: it may seem unfortunate that one has to work so as to stay thin, but surely this is better than working to stay alive, and being thin as a matter of course.&amp;rdquo; This doesn&amp;rsquo;t mean that the default mode of operation is especially slow or wasteful of memory: given a fixed amount of development resources, accumulating reusable automated strategies as in Houyhnhnm computing systems can achieve more performance than manually implementing strategies in every program like in Human computer systems.&lt;/p&gt;

&lt;p&gt;Indeed, manual implementation of software strategies, known in the Human computer industry as &amp;ldquo;design patterns&amp;rdquo;, is the primary source of bad quality in software: humans are just so much worse than machines (not to mention slower and more expensive) at applying algorithmic strategies — which notably goes against the &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;Sacred Motto of the Guild of Houyhnhnm Programmers&lt;/a&gt;. (Of course, quality is &lt;em&gt;even worse&lt;/em&gt; when the underlying design patterns have not even been recognized and their properties haven&amp;rsquo;t even been semi-formalized between sentients.) Now, errors can be made when writing the meta-program that automates the strategy — but it&amp;rsquo;s much easier to debug one simple general meta-program once than tens, thousands or more context-specific manual instances of the pattern that each had to precisely match the pattern in excruciating details; what more, without automation, it&amp;rsquo;s much harder to keep these myriads of instances right as the pattern or its parameters change, and maintenance requires all of them to be modified accordingly. As Rich Hickey quipped, &lt;em&gt;(Design) patterns mean &amp;ldquo;I have run out of language.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Because software strategies ultimately preserve the safe semantics of high-level languages, they involve less code written in unsafe low-level languages, and what low-level code is generated can be automatically and verifiably made to preserve high-level invariants that matter for safety. Entire classes of bugs that commonly plague Human computer systems thus never appear in Houyhnhnm computing systems. Of course, Houyhnhnms make many mistakes while developing their computing systems, and the inconsistent strategies they write can cause inconsistent behavior, with catastrophic consequences. But virtualization ensures that these catastrophes do not escape the narrow scope of the sandbox in which the developer is trying them; and catastrophic effects are actually easier to detect, so that most such bugs are faster to fix. Subtle meta-level bugs causing delayed catastrophes, though they exist, are quite rare. To eliminate them, the usual combination of testing and formal methods can help; there again, generic code is usually harder to test or formalize than a single specific instance of the code, but much easier to test or formalize than thousands or millions of instances, as necessarily happens when strategies are applied manually rather than automatically.&lt;/p&gt;

&lt;p&gt;Finally, because Houyhnhnm computing systems work at the level of abstract data types, most messaging happens with robust system-provided pairs of printers and parsers, rather than an ever renewed collection of &lt;em&gt;ad hoc&lt;/em&gt; manual printers and parsers for manually designed interchange languages, each introducing a renewed layer of bugs. Indeed, in Human computer systems, the humans who &amp;ldquo;design&amp;rdquo; these interchange languages are often unaware that they are designing languages indeed, or in deep denial when confronted to that fact; they thus prefer to remain to remain ignorant of the very basics of language design, and ever repeat all the beginners&amp;rsquo; mistakes. In Houyhnhnm computing systems, it is understood that whatever manual or automated processes use to interact with each other by definition constitutes a language; and that while you want the overall design interaction between sentient being and machine to happen at the highest possible level using as expressive a language as possible, the interactions between automated processes should happen using the least expressive language possible, so they remain easier to analyze.&lt;/p&gt;

&lt;p&gt;Therefore, when contrasted to Human computer systems, it appears that Houyhnhnm computing system thus achieve &lt;em&gt;better&lt;/em&gt; quality through &lt;em&gt;meta&lt;/em&gt; programming.&lt;/p&gt;

&lt;h3 id="building-up-vs-building-down"&gt;Building up vs building down&lt;/h3&gt;

&lt;p&gt;Humans can only build software but &lt;em&gt;up&lt;/em&gt;. Houyhnhnms can build both up &lt;em&gt;and&lt;/em&gt; down.&lt;/p&gt;

&lt;p&gt;All computer software has to start from a given &lt;em&gt;base&lt;/em&gt;: whatever abstractions the operating system provides, or, in absence of operating system, the &amp;ldquo;bare metal&amp;rdquo; — which for Human computer systems is often not quite so bare these days, with plenty of firmware and coprocessors involved. Now, Human computer systems are built by piling layers upon layers on top of this base; and the operating system itself can be already considered such a tower of layers, on top to build higher towers. One limitation of Human computer systems, though, is that to cooperate on the same data structures, programs typically have to reuse the very exact same tower of layers. Because each layer adds a lot of informal underspecified details, and it is impossible to reproduce computations or assume that programs have similar enough semantics unless they are identical from the ground up. With this tower architecture, as with the legendary Tower of Babel, people are divided by a confusing diversity of languages that prevent them from communicating.&lt;/p&gt;

&lt;p&gt;Now, it is actually important to share data between different programs. Human software developers thus onerously build &lt;em&gt;abstractions&lt;/em&gt;, without system support, so that they may save files in one format, which will hopefully be implemented in a compatible enough way by the other program or next version of the program. The operating system itself is such an abstraction, trying to present a uniform view of the computer to programs that run on top of it, despite a wild variety of underlying computers; so are to a point various virtual machines, or programming language specifications. So is, more trivially, the informal promise in successive versions of the &amp;ldquo;same&amp;rdquo; program to keep working with data saved by previous versions. Yet, any given abstraction usually has at most one sensible implementation on any given Human computer system.&lt;/p&gt;

&lt;p&gt;Slightly more advanced Human computer systems, using macros, can at compile time lift the system up and add a number of layers below. For an extreme case, some &lt;a href="http://www.cliki.net/screamer"&gt;Common Lisp&lt;/a&gt; &lt;a href="http://quickdocs.org/hu.dwim.delico/api"&gt;libraries&lt;/a&gt; reimplement Common Lisp in Common Lisp to add first-class multiple-entry or even serializable continuations, so as to enable logic programming or direct-style web programming. Some interactive development systems also instrument the virtual machine so as to lift execution into something that allows for debugging, with Omniscient Debugging as an extreme example. But even then, once the program is built, once the runtime has been chosen, once the program has started running, the system remains forever grounded on top of the chosen basis.&lt;/p&gt;

&lt;p&gt;Houyhnhnm computer systems, by contrast, can dynamically add new layers below a running program: not only can you add a layer on top of any existing tower before you start using it, you can add or replace layers below the tower, or anywhere in the middle of it, while you are using it. This ability to build &lt;em&gt;down&lt;/em&gt; as well as &lt;em&gt;up&lt;/em&gt; crucially relies on processes being specified in formally well-defined high-level languages, so that it is always clear what is the semantics to be preserved when modifying the underlying implementation. Therefore, Houyhnhnms don&amp;rsquo;t even have a fixed notion of ground or base. Rather than rigid towers of stone being built up, they have living worlds that stand on an indefinite number or other living worlds, just like the turtles of the common joke, whereby there are &lt;a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down"&gt;&lt;em&gt;turtles all the way down&lt;/em&gt;&lt;/a&gt;; then Houyhnhnms can lift the stack of turtles at any desired point and add or replace some of the turtles beneath, while the system is running. Every turtle is unique, but no turtle is special.&lt;/p&gt;

&lt;p&gt;The superficial differences between Houyhnhnm computing systems and Human computer systems are thus the reflection of radical differences between their underlying software architectures — that once again, derive from the initial divergence in &lt;em&gt;point of view&lt;/em&gt;: considering the entire sentient-machine processes, rather than focusing only on the finished machine artefacts.&lt;/p&gt;&lt;/html&gt;</description></item>
  <item>
   <title>Chapter 3: The Houyhnhnm Version of Salvation</title>
   <link>http://ngnghm.github.io/blog/2015/08/09/chapter-3-the-houyhnhnm-version-of-salvation/?utm_source=Persistence&amp;utm_medium=RSS</link>
   <guid>urn:http-ngnghm-github-io:-blog-2015-08-09-chapter-3-the-houyhnhnm-version-of-salvation</guid>
   <pubDate>Sun, 09 Aug 2015 05:10:00 UT</pubDate>
   <description>&lt;html&gt;
&lt;p&gt;Following our &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;discussion on persistence&lt;/a&gt;, Ngnghm had plenty of questions about how Human computer systems held together when they can&amp;rsquo;t seem to get basic persistence right. But in return, I had even more questions about what Houyhnhnm computing systems could even be like, when all data persisted by default: What did the user interface look like? Was there no more save button? What happened when you copied or deleted files? Were there files at all? How did people deal with all the garbage? Were your mistakes forever? If you somehow hosed your machine, would it remain forever hosed? How did you test potentially dangerous changes?&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="when-data-persists"&gt;When Data Persists&lt;/h3&gt;

&lt;p&gt;Thus, on a first approach, the interface to a Houyhnhnm computing system may look very similar to that of a Human computer system — and that&amp;rsquo;s how Ngnghm had been fooled at first into thinking they were designed along the same principles. If you have a Human laptop that you usually put to sleep and don&amp;rsquo;t turn off or restart, your setup might be similar to what it would be on a Houyhnhnm muzzletop. But even if your operating system is very stable, you must always be prepared for a catastrophic failure; and whenever you upgrade your software (which you &lt;em&gt;must&lt;/em&gt; do eventually if only to apply security patches), then you &lt;em&gt;must&lt;/em&gt; restart just to make sure the restart procedure will be working if the failure happens while you&amp;rsquo;re disconnected from online help — even though your system is stable and doesn&amp;rsquo;t really need it. And at that point you lose all your current working state. The eventuality of losing all your current working state is thus a Sword of Damocles hanging above any and all working state you weave, even on the most stable of systems. Note that a Houyhnhnm computing system may have to be restarted semi-periodically for the very same reason; the difference being that you don&amp;rsquo;t lose any working state as you do — or else, your system administrator and/or your or his insurance will cover any damages caused. Impermanence is thus a pervasive assumption in all Human computer systems, around which all habits are built; the loss of working state can be mitigated by using &amp;ldquo;session managers&amp;rdquo; that automatically save your session, or &amp;ldquo;startup scripts&amp;rdquo; where you manually re-create your usual work session, but both approaches only save very partial session information, in addition to being quite fragile and unreliable in practice — and intensive in programmer effort to implement. By contrast, in a Houyhnhnm computing system, you never lose your session; moreover, you can extract a startup script to re-create a similar session on another system, by introspecting the state of the system (the support for which is necessarily present for the sake of persistence) or by selectively replaying the relevant parts of the system persistence log.&lt;/p&gt;

&lt;h3 id="system-wide-versioning"&gt;System-wide Versioning&lt;/h3&gt;

&lt;p&gt;In Human computer systems, editors and other applications have a &amp;ldquo;save&amp;rdquo; button. In Houyhnhnm computing systems, there are no applications and no &amp;ldquo;save&amp;rdquo; buttons. Instead, there are components that each deal with the specific aspects of some kinds of documents or data, and otherwise share common system features for general aspects of data — including notions of versioning and releasing, i.e. publishing a stable version so it&amp;rsquo;s visible to other people, whereas intermediate changes and their inglorious details remain unpublished. Thus, instead of &amp;ldquo;text editors&amp;rdquo; and &amp;ldquo;picture editors&amp;rdquo;, there are &amp;ldquo;text editing components&amp;rdquo; and &amp;ldquo;picture-editing components&amp;rdquo;, that are available anywhere that there are modifiable texts or pictures in the system — and pretty much any text or picture you see can be copied into an editable variant, or traced back to its source, which can be forked and edited, if it&amp;rsquo;s not directly editable already. In Human computer systems, programmers have to bundle a finite number of such components into the package-deal that is an &amp;ldquo;application&amp;rdquo;, where you can&amp;rsquo;t use the component you want without being stuck with those you don&amp;rsquo;t want. In Houyhnhnm computing systems, users can individually configure the components or combinations of components they want to use for each type of data they are interested in. They all delegate their versioning aspect to the user&amp;rsquo;s favorite versioning component, that will handle forking new branches of data, and branches off branches of data, merging data from multiple branches, atomically committing changes, releasing data from a subbranch to make its changes available to a wider branch, etc.&lt;/p&gt;

&lt;p&gt;Another advantage of system-wide versioning is that in Houyhnhnm computing systems, infinite undo comes for free on any kind of data, without any special effort from the developer, and without any limitation for the user; what more it is available atomically for all data in the system or any joint subset thereof. By contrast, in Human computer systems, the ability undo of a few steps for a few kinds of documents is a very costly, unreliable and/or error-prone operation requiring a lot of programming and a lot of maintenance, and working on one document at a time; some applications maintain history, but it is optimized for data mining by spies, and useless for users to recover past sessions. One more feature made possible by system-wide versioning is the ability to easily reproduce and isolate bugs — an activity that consumes a lot of expensive programmer time in Human computer systems, and that is made much easier in Houyhnhnm computing systems, since the log of interaction events that led to the erroneous behavior was recorded and can be replayed until the behavior was narrowed down; then the error case can automatically be reduced to its essence, by shaking the tree of actually used dependencies as detected by re-running an instrumented version of the same code to achieve e.g. Omniscient Debugging; the test case is then ready for inclusion in a regression test suite.&lt;/p&gt;

&lt;h3 id="data-at-the-proper-level-of-abstraction"&gt;Data at the Proper Level of Abstraction&lt;/h3&gt;

&lt;p&gt;Because persistence in Human Computer Systems consists in communicating sequences of bytes to external processes and systems (whether disks or clouds of servers), all data they hold is ultimately defined in terms of sequences of bytes, or files; when persisting these files, they are identified by file paths that themselves are short sequences of bytes interpreted as a series of names separated by slashes &lt;code&gt;/&lt;/code&gt; (or on some systems, backslashes &lt;code&gt;\&lt;/code&gt;, or something else). Because persistence in Houyhnhnm Computing Systems applies to any data in whichever high-level language it was defined in, all Houyhnhnm computing data is defined in terms of &lt;a href="https://en.wikipedia.org/wiki/Algebraic_data_type"&gt;Algebraic Data Types&lt;/a&gt;, independently from underlying encoding (which might automatically and atomically change later system-wide). For the sake of importing data in and out of independently evolving systems, as well as for the sake of keeping the data compressed to make the most of limited resources, some low-level encoding in terms of bytes may be defined for some data types. But on the one hand, this is the exception; on the other other, the data is still part of the regular Algebraic Data Type system, can still be used with type constructors (e.g. as part of sum, product or function types), etc. Whereas Human computer systems would require explicit serialization and deserialization of data, and would require ad hoc containers or protocol generators to allow larger objects to contain smaller ones, Houyhnhnm computing systems abstract those details away and generate any required code from type definitions. Low-level encodings can even be replaced by newer and improved ones, and all objects will be transparently upgraded in due time — while preserving all user-visible identities and relationships across such changes in representation.&lt;/p&gt;

&lt;p&gt;Since objects are not defined in terms of sequences of bytes, the very notion of file doesn&amp;rsquo;t apply to Houyhnhnm computing systems. At the same time, accessing an object inside a data structure is often (though not always) conveniently represented as following an access path from the root of the data structure to the desired element. An &amp;ldquo;access path&amp;rdquo; is thus a natural notion in all computing systems &lt;!--
see Clojure: assoc-in, update-in--&gt; even though in general a path is not a sequence of strings separated by slashes, but a list of accessors, that may be symbols or strings (when accessing a dictionary), integers (when accessing a sequence by index), or arbitrary accessor functions. But few are the cases where the natural way to locate data is via a list of sequences of bytes containing neither ASCII slash nor ASCII NUL; or worse, sequences of Unicode code glyphs up to some subtle case-conversion, represented as UTF&amp;ndash;8 code points in some normal form; or even worse, the greatest common denominator between an underspecified set of several variants of the above, with unspecified separators.&lt;/p&gt;

&lt;p&gt;Thus, files are not the general case for persisting data; text files even less so. Still, a good text editor and good text-based diff tools can provide a handy way to view and modify data and view and act on modifications to data. Indeed, unless and until you have better tools to represent change between arbitrary data structures, it makes sense to translate otherwise unsupported data structures to and from a well supported generic data structure such as text. &lt;a href="http://www.cs.yale.edu/homes/perlis-alan/quotes.html"&gt;&lt;em&gt;It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures.&lt;/em&gt;&lt;/a&gt; Now, it is important to understand the distinction between a representation and the real thing; the text being presented is not &amp;ldquo;canonical&amp;rdquo;, it is not usually &amp;ldquo;source&amp;rdquo;. In Houyhnhnm computing systems, the source is the semantic state of the system, on which change happens, and from which the text is extracted if and when needed; this is in sharp contrast with typical Human computer systems, where the source (that is, the locus of modification by sentients) is text files that are compiled, disconnected from the state of the system.&lt;/p&gt;

&lt;h3 id="dealing-with-bad-memories"&gt;Dealing with Bad Memories&lt;/h3&gt;

&lt;p&gt;But, I inquired, if they log everything and almost never forget anything, don&amp;rsquo;t Houyhnhnm computing system quickly get filled with garbage? No, replied Ngnghm. The amount of information that users enter through a keyboard and mouse (or their Houyhnhnm counterparts) is minute compared to the memory of modern computers, yet, starting from a well-determined state of the system, it fully determines the subsequent state of the system. Hence, the persistence log doesn&amp;rsquo;t need to record anything else but these events with their proper timestamp. This however, requires that all sources of non-determinism are either eliminated or recorded — which Houyhnhnm computing systems do by construction. Of course, to save resources, you can also configure some computations so they are not recorded, or so their records aren&amp;rsquo;t kept beyond some number of days. For instance, you might adopt a &lt;a href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller"&gt;&lt;em&gt;model-view-controller&lt;/em&gt;&lt;/a&gt; approach, and consider the view as transient while only logging changes to the controller, or even only to the model; or you might eschew long-term storage of your game sessions; or you might forget the awkward silences and the street noise from your always-on microphone; or you might drop data acquired by your surveillance camera when it didn&amp;rsquo;t catch any robber; or you might delete uninteresting videos; or you might expunge old software installation backups from long gone computers; or you might preserve a complete log only for a day, then an hourly snapshot for a few days, and a daily snapshot for a few weeks, a weekly snapshot for a few months, etc.; or you might obliterate logs and snapshots as fast as you can while still ensuring that the system will be able to withstand partial or total hardware failure of your personal device; or then again, given enough storage, you might decide to keep &lt;em&gt;everything&lt;/em&gt;. It&amp;rsquo;s your choice — as long as you pay for the storage. The decision doesn&amp;rsquo;t have to be made by the programmer, though he may provide hints: the end-user has the last say. Indeed, the interests of the programmer may not be aligned with those of the user; if he needs to delegate these decisions, the user will thus entrust administrators he pays, whose interests are better aligned because they will be liable in case of malpractice.&lt;/p&gt;

&lt;p&gt;Now, beyond clutter that uselessly fills up memory, what about actively bad things that surely you don&amp;rsquo;t want to memorize? Some mistake might cause your entire system to become unresponsive by resource exhaustion (a fork bomb on Unix, an out-of-memory situation on any system); something might trigger a system bug and cause a hardware crash, a Blue Screen Of Death or a kernel panic; even worse, some subtle combination of factors could generate a memory corruption that jeopardizes the integrity of the persistent data. Houyhnhnms computing systems may be more robust than Human computer systems in this regard, yet even Houyhnhnms are not perfect in avoiding catastrophic mistakes. If you detect such a situation, what do you do? Old Houyhnhnm engineers tell classic stories of catastrophic system modifications that were reverted by shutting down the computer before the modification was written to disk; but of course, as the latency of persistence goes down, the window of opportunity for such a feat goes away. The general answer is that to fix a system that has entered a bad state, you need an &lt;em&gt;external&lt;/em&gt; system that can stop the broken system, inspect it, fix it, and restart it.&lt;/p&gt;

&lt;p&gt;On a Human computer system, when things get that bad, you can often reboot in a special &amp;ldquo;failsafe&amp;rdquo; mode (that can usually handle but the simplest of situations), or you can use a USB key with a known stable version of the system (with which experts can handle complex situations), or at worst if the hardware was damaged, you can disconnect the computer&amp;rsquo;s mass memory unit and connect it into another computer. In a Houyhnhnm computing system, you can do as much, but you can also use a reserved input sequence (the equivalent of 
 &lt;kbd&gt;Ctrl-Alt-Del&lt;/kbd&gt; on Windows) to enter a &lt;em&gt;monitor&lt;/em&gt;. The monitor is a &lt;em&gt;simple&lt;/em&gt; but complete computing system, as per the &lt;a href="/blog/2015/08/02/chapter-1-the-way-houyhnhnms-compute/"&gt;Houyhnhnm criteria of simplicity&lt;/a&gt;; it is universal and can do everything a computing system can do, and is often a bare-bones variant of a regular computing system, as used for secure bootstrapping purposes; it also specifically understands enough of the semantics of the regular system to inspect it, fix it, and restart it, using the full power of a complete computing system (though a simple one). A small amount of memory is reserved for the operation of the monitor; actually, if mass memory units are working (as they should be) and have some reserved space for the monitor (as is the case on a default installation), then the monitor will actually spawn a virtualized monitor; this allows monitor operations to have more memory available, so they can for instance merge in a lot of the system state (up to some point deemed safe by the user); but this also makes it possible to still have a monitor (and possibly more virtualized monitors) in case you make mistakes in the virtualized monitor; as a result, it is safe to use dichotomy to determine which change broke the system.&lt;/p&gt;

&lt;h3 id="virtualization-as-branching"&gt;Virtualization as Branching&lt;/h3&gt;

&lt;p&gt;More generally, a Houyhnhnm can use virtualization and system rollback while conducting any kinds of experiments, so he never has to hesitate about doing anything risky, half-baked, downright stupid, or otherwise dangerous. But virtualization doesn&amp;rsquo;t mean the same thing in a Houyhnhnm computing system as in a Human computer system. In a Human computer system, virtualization is an &lt;em&gt;ad hoc&lt;/em&gt; tool for system administrators, that allows the deployment of specially prepared servers; it is implemented using heroic techniques, by faking an entire physical computer at the level of abstraction of CPU instructions and memory accesses; the awkward result requires extra care and makes the users&amp;rsquo; life overall more miserable rather than less, though it is sometimes an economic necessity. In a Houyhnhnm computing system, virtualization is merely the ability to branch the history of changes in the system, and derives naturally from the fact that the entire system is under version control; it is available at whichever level of abstraction the users and programmers specify their computations, which is most usually at a much higher level of abstraction than that of CPU instructions (though a naive, fallback strategy is indeed always available that consists in going down to that low level).&lt;/p&gt;

&lt;p&gt;Thus, all destructive or catastrophic experiments happen in branches that are never merged into the official reality — the errors remain imaginary. Alternatively, when bad things happen in said official reality, they can be &lt;a href="http://www.jargon.net/jargonfile/r/retcon.html"&gt;retcon&lt;/a&gt;&amp;rsquo;ed into having been but a bad dream, as the bad reality is demoted into being a mere unmerged error branch while a nicer reality becomes the master copy. Of course, it may be too late to undo bad communications with external systems, if they were let happen; some of them may be cancelled or compensated with new transactions; some of them may have to be accepted as losses or errors: you can&amp;rsquo;t unprint a pile of garbled characters, and you won&amp;rsquo;t get your bitcoins back. Even Houyhnhnm computing systems can&amp;rsquo;t protect you from yourself; but they can make it easy to try things in a virtualized environment, and to only merge into reality those transactions that pass all checks.&lt;/p&gt;

&lt;p&gt;It is also possible to branch only part of the system while the rest of the system remains shared; and of course you can merge two branches back together, somehow fusing changes. Thus, there is no need to specially prepare an image for virtualization; any system and any subsystem, any program and any subprogram, can be forked at any time; any ongoing I/O with other systems can be redirected to one fork or the other, or multiplexed to/from both, or filtered, etc., and the user can dynamically re-wire all these connections from a monitor outside the virtualized system, that can itself be virtualized, etc.&lt;/p&gt;

&lt;p&gt;Version control should be familiar to developers of Human computer systems; but these days, they apply it only to source code; and so live, interactive data is not covered by the version control, or at best only in a very indirect way, if the programmers make large, contrived and expensive efforts to check in every change. Houyhnhnms think of code and data as coming together, part of the same interaction with the Sentient user, with data and code being useless without the other, or out of synch with the other; and thus Houyhnhnm computing systems casually apply version control to the entire state of the system.&lt;/p&gt;

&lt;p&gt;In the end, thinking like Houyhnhnms in terms of comput_ing_ systems, rather than like Humans in terms of comput_er_ systems has far-ranging consequences in terms of software and hardware architecture. Persistence is but one aspect of this architecture, though ultimately, it cannot be separated from the rest. And on this aspect like on others, from the necessity of dealing with the same basic needs and failure scenarios, the change in &lt;em&gt;point of view&lt;/em&gt; leads to very different approaches to making and keeping the systems working.&lt;/p&gt;&lt;/html&gt;</description></item>
  <item>
   <title>Chapter 2: Save Our Souls</title>
   <link>http://ngnghm.github.io/blog/2015/08/03/chapter-2-save-our-souls/?utm_source=Persistence&amp;utm_medium=RSS</link>
   <guid>urn:http-ngnghm-github-io:-blog-2015-08-03-chapter-2-save-our-souls</guid>
   <pubDate>Mon, 03 Aug 2015 05:10:00 UT</pubDate>
   <description>&lt;html&gt;
&lt;p&gt;&lt;a href="/blog/2015/08/02/chapter-1-the-way-houyhnhnms-compute/"&gt;Ngnghm&lt;/a&gt; was fascinated by our keyboards: because of physiological differences between our races, similar devices had never been imagined by &lt;a href="http://en.wikipedia.org/wiki/Houyhnhnm"&gt;Houyhnhnm&lt;/a&gt; computing engineers. Now, as he was watching me closely, Ngnghm noticed that I was punctuating most of my typing with recurring combinations of key chords. I told him I had no idea what he meant; and so he had me record and review how, after every sentence or so, or before changing activities, I was composing the sequence 
 &lt;kbd&gt;Ctrl-X Ctrl-S&lt;/kbd&gt;, or 
 &lt;kbd&gt;Command-S&lt;/kbd&gt;, or some other mantra that varied slightly with the application I was using. Interestingly, I wasn&amp;rsquo;t even aware that I was doing that before he told me! What was this mantra doing, he inquired? How could I possibly repeat it without even noticing — and why would I? I told him that depending on the application, each of these mantra &lt;em&gt;saved&lt;/em&gt; the current file, and that typing it had become ingrained in me as a subconscious habit, because I used it so often, out of necessity. What does "&lt;em&gt;saved&lt;/em&gt;" mean wondered Ngnghm, and what made it a necessity?&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="persistence-automated"&gt;Persistence, Automated&lt;/h3&gt;

&lt;p&gt;I explained that Human computer software and hardware are prone to crashing, or to losing battery power, and other unexpected failures — there he sighed with sympathy, for Houyhnhnms were just as frustrated as Humans with how unreliable their computers were. I continued that the solution universally adopted for Human computer systems was therefore that Humans had to explicitly &lt;em&gt;save&lt;/em&gt; each file for its contents to be later recoverable in the event of such a crash. Having been burned too many times by the loss of many hours of hard work, I had grown the habit of saving often, and doing it unconsciously at every pause in my thought process; thus I didn&amp;rsquo;t have to think hard to predict when the computer was at risk and explicitly decide when I ought to save. Ngnghm was properly appalled. Didn&amp;rsquo;t the system just automatically save everything I typed? Why was human thought and habit involved at all in a task that could have been fully automated long ago — and indeed had been automated in all but the earliest and most primitive Houyhnhnm computing systems?&lt;/p&gt;

&lt;p&gt;Although, he remarked, considering the overall computing system containing both Sapient and Computer, the task had been automated indeed. Indeed, if you came to think of it, this task couldn&amp;rsquo;t possibly &lt;em&gt;not&lt;/em&gt; be automated, unless the computing system were only used but to produce worthless data never worth keeping — at which point it would thus be itself worthless. However, the task had been imperfectly automated at great cost by creating a habit in my brain and hands, rather than automated both perfectly and cheaply by having it done by the computer. Certainly, building a physical habit that lightened the burden on the higher parts of my mind was better than no automation at all, but what a waste of precious wetware! At least in this instance and for this concern, the very purpose of computers had been defeated. As went the &lt;em&gt;Sacred Motto&lt;/em&gt; of the Guild of Houyhnhnm Programmers: &lt;a href="http://www.wanderings.net/notebook/Main/BitterAcknowledgmentsOfOlinShivers"&gt;&lt;em&gt;I object to doing things that computers can do&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And then, suddenly, Ngnghm became worried about his journal. He had been consigning his observations on a computer ever since he had learned to use a mouse to draw Houyhnhnm symbols in a paint application. (Ngnghm once remarked that a one-button mouse is an exquisite input device for a Houyhnhnm&amp;rsquo;s hoof, but that it takes a Yahoo to believe it is suited to a Yahoo´s hand.) Now, he admitted that he could never retrieve his old notes; but he just thought that it was due to his not understanding the Houyhnhnm-Computer Interface properly, and to his not knowing how to search back in time what he had previously drawn. He assumed that Human computers were probably not geared to properly index his observations for easy retrieval, but that they would otherwise all be logged in the computer&amp;rsquo;s memory. Was I implying that all his notes were lost, including some of the finest poetry he had ever written, as inspired both by the suffering from his predicament and the marvel at his discoveries? I won&amp;rsquo;t claim any proficiency at judging Houyhnhnm poetry — it all sounds like nickering and whickering to me — but to this day, I fear that one of the greatest pieces of Houyhnhnm literature has been lost to the world, due to the failings of Human computer systems.&lt;/p&gt;

&lt;h3 id="orthogonal-persistence"&gt;Orthogonal Persistence&lt;/h3&gt;

&lt;p&gt;Ngnghm explained to me that Houyhnhnm computing systems make data persistence the default, at every level of abstraction. Whether you open the canvas of a graphical interface and start drawing freely, or you open an interactive evaluation prompt and bind a value to a variable, or you make any kind of modification to any document or program, the change you made will remain in the system forever — that is, until Civilization itself crumbles, or you decide to delete it (a tricky operation, more below). Everything you type remains in your automatic planet-wide backups, providing several layers of availability and of latency — kept private using several layers of cryptography.&lt;/p&gt;

&lt;p&gt;Of course, you can control what does or doesn&amp;rsquo;t get backed up where, by defining domains each with its own privacy policy that may differ from the reasonable defaults. The user interface is aware of these domains, and makes it clear at all times which domain you&amp;rsquo;re currently working with. It also prevents you from inadvertently copying data from a more private domain then pasting it into a more public one; in particular, you only type your primary passwords but in a very recognizable special secure domain that never stores them; and your secondary access keys are stored in a special private domain using stronger cryptography than usual, and also subject to various safety rules to avoid leakage.&lt;/p&gt;

&lt;p&gt;Deletion (as opposed to mere de-indexing), while possible, gets more expensive as the data you want to delete gets older: logs, backups and indexes dating back to the deleted change have to be scrubbed and rewritten; the system must triple-check that everything is still in working order after this sweeping change; it must also make sure that the user is ultimately happy with the results, including with whatever might break for other users he knows who might have depended on details of the old history (assuming he shared any of it). Now, when deleting anything but most recent changes, this expensive operation will leave traces that something was deleted, though the details of what was deleted will indeed have been deleted. Of course, deletion doesn&amp;rsquo;t affect copies other people may have of the data, if you ever shared it; therefore, thou shalt not lightly share thy data, and thou shalt never share any access keys — but that&amp;rsquo;s true anyway. At least Houyhnhnm systems let you manage your sharing and backup policies in a systematic way, and ensure that everyone can depend on sensible, safe, defaults.&lt;/p&gt;

&lt;p&gt;In other words, Houyhnhnm computing systems have &lt;a href="http://tunes.org/wiki/orthogonal_20persistence.html"&gt;&lt;em&gt;orthogonal persistence&lt;/em&gt;&lt;/a&gt; — and have had it for &lt;a href="http://tunes.org/wiki/eumel.html"&gt;&lt;em&gt;decades&lt;/em&gt;&lt;/a&gt;. The adjective &amp;ldquo;orthogonal&amp;rdquo; means that the persistence of data is a property of the domain you&amp;rsquo;re working in, as managed by the system; it is &lt;em&gt;not&lt;/em&gt; an aspect of data that programmers have to deal with in most ordinary programs; unless of course they are programmers specifically working on a new abstraction for persistence, which is after all an ordinary program, just in a narrow niche. Regular programmers just manipulate the data with full confidence that the inputs they consume, the code that manipulates them, and the outputs they produce will each remain available as long as the user wants them, with the consistency guarantees specified by the user, as long as the user affords the associated costs.&lt;/p&gt;

&lt;p&gt;Actually, ordinary programs don&amp;rsquo;t know and can&amp;rsquo;t even possibly know which domain they will be running in, and letting them query those details would be a breach of abstraction, with serious security implications and performance impediments, even assuming for a moment that it wouldn&amp;rsquo;t otherwise affect program correctness. Therefore, only programs with adequate capabilities can manipulate the persistence and privacy levels of computing domains, except of course to deliberately spawn a subdomain with yet strictly fewer capabilities. The system of course can recognize privacy and performance annotations about authorized programs and automatically distribute the many components of these programs each in a suitable domain.&lt;/p&gt;

&lt;p&gt;It is important to maintain full abstraction when keeping the semantics of ordinary programs orthogonal to various concrete aspects of the computing domains: the persistence, privacy, robustness and performance (but also machine word size, endianness, memory layout, physical location of the machine, etc.). This abstraction allows the user to independently specify what domain he wants, and to later change his specification, while the program keeps running. The same abstraction allows the underlying system to independently pick the best suited or cheapest concrete implementation, and to migrate the program to a different underlying machine when the conditions change. And whether migration is prompted by user request, system adaptation, or a change of phase in the execution of the program, the concrete code to run the program can automatically be re-generated to fit the new conditions, so the program may continue running in a new domain implementation, without any interruption in its semantics (though possibly with an observable pause). Thus, the system may optimize away logging and copying in transient computations for which speed matters more than robustness; or it may introduce extra logging and extra copying when debugging existing programs (e.g. enabling &lt;a href="http://www.lambdacs.com/debugger/"&gt;Omniscient Debugging&lt;/a&gt; for a failed computation); it may automatically introduce synchronization steps in computations performed in lock-step by several redundant machines based on different architectures to ensure detection and elimination of low-level failures (or tampering); or then again it may add layers of encryption between CPU and memory where the user feels paranoid; or it may compile the code to FPGA where performance &lt;em&gt;really&lt;/em&gt; matters.&lt;/p&gt;

&lt;p&gt;The possibilities are endless, as long as the system maintains full abstraction of program semantics from the underlying implementation, as Houyhnhnm computing systems do. When on the contrary, as in Human computer systems, the code is pegged to a particular implementation, then not only is it practically impossible to migrate a program from one domain to another at runtime, but programs may have to be completely rewritten from scratch before they may even be executed in a domain with slightly different constraints regarding persistence, privacy, performance, etc.&lt;/p&gt;

&lt;h3 id="fractal-transience"&gt;Fractal Transience&lt;/h3&gt;

&lt;p&gt;Interestingly, on the visible side of the system, successful Human &amp;ldquo;apps&amp;rdquo; these days have evolved into offering to users some semblance of persistence: configuration settings, lists of open tabs, documents you manipulate — most user-visible application state, most of the time, seems to be preserved from one session to the next, without the user having to issue any explicit command to &amp;ldquo;save&amp;rdquo; anything. Desktop apps still tend to display a counter-productive &amp;ldquo;recovery&amp;rdquo; menu at startup, though. And more annoyingly, this apparent persistence still doesn&amp;rsquo;t cover the most frequent case these days of people typing things: input forms and message boxes in web pages. Also, the &amp;ldquo;catastrophic&amp;rdquo; events are covered include so predictable the event as is the eventual death of each and every piece of hardware — and of each and every software project and service-providing business. Yet, content with expectations from this &lt;em&gt;apparent&lt;/em&gt; persistence, users can easily be fooled, like Ngnghm was initially, into believing that Human computer systems are just as good as Houyhnhnm computing systems in this regard; and just like Ngnghm, they can be led to believe that failures are due to incompetence on their part, rather than on the part of the computing system developers.&lt;/p&gt;

&lt;p&gt;Well, at least, that&amp;rsquo;s how the Houyhnhnm see things: whether or not you can assign blame to any person in particular for the situation of Human computer systems, this situation is deeply dysfunctional. Actually, the Houyhnhnm also have something to say if you cannot assign personal blame for it — and it doesn&amp;rsquo;t look like you can: this means that the meta-system for assigning responsibilities itself is also dysfunctional. Why do &amp;ldquo;vendors&amp;rdquo; of Human computer systems by and large hoard all the freedom but none of the responsibility when it comes to modifying and maintaining their software so it doesn&amp;rsquo;t fail catastrophically and betray the customers? This is a clearly dysfunctional process according to Houyhnhnm criteria. Even when these vendors tout themselves as selling &amp;ldquo;software as a service&amp;rdquo;, they often hide behind their &amp;ldquo;Intellectual Property&amp;rdquo; monopolies to actually make it &amp;ldquo;rotware as a racket&amp;rdquo; — they offer &lt;a href="http://www.jargon.net/jargonfile/b/bitrot.html"&gt;bitrotting&lt;/a&gt; bad expensive service, oriented towards the vendor&amp;rsquo;s interests to the detriment of the users&amp;rsquo;, with no enforceable service level agreement, no way to extract your data in a state usable by any competing service, with the promise that the service &lt;em&gt;will&lt;/em&gt; grow even more inadequate and eventually die (being cancelled, bankrupted, or bungled), yet that you &lt;em&gt;will&lt;/em&gt; have to keep paying, and then pay again when you have to leave or be left behind; but you don&amp;rsquo;t have much choice because patents and other monopolies attract capital and provide disincentive to investment in any competition (if legally allowed at all) or in other services that don&amp;rsquo;t similarly exclude competition through legal tactics. By contrast, Houyhnhnms individually have full ultimate control over their own machines, and it is based on this control that they enjoy division of labour in delegating software maintenance of most (if they are programmers) or all (if they aren&amp;rsquo;t) of their systems to competing providers who are held individually liable in case of failure, and aren&amp;rsquo;t granted monopolies by a centralized privilege-doling entity.&lt;/p&gt;

&lt;p&gt;Now, after Ngnghm made this painful first hoof experience of the persistence failure of Human computer systems, he started investigating how Human computer systems implemented persistence, or failed to. And he discovered to his dismay that beneath the &lt;a href="http://www.www.loper-os.org/?p=448"&gt;veneer of persistence&lt;/a&gt;, there was transience at every level he was looking at — not just transience, but &lt;a href="http://rationalwiki.org/wiki/Fractal_wrongness"&gt;fractal transience&lt;/a&gt;: this fundamental design difference between Human and Houyhnhnm computing systems is observable at every level of these systems. The user, the programmer, the library developer, the compiler writer, the operating system implementer, everyone, all the time, has to assume the software and hardware layers below him are fragile, supposed to work only a single computing domain; everyone will in turn provide a similarly fragile and non-transportable device to the users above him. All the manual handling of persistence costs a significant fraction of software development (about 30% of all code written, an IBM study once counted); &lt;!--
https://web.archive.org/web/20060813202835/http://www.st-andrews.ac.uk/services/admissions/postgrad/schleaf5.html
...Ron Morrison...
"Well, perhaps not quite so easy. Research in persistent programming systems started in the late seventies, when it was noticed that storing 'long-term' data in a different logical framework from 'short-term' data leads to all sorts of problems in large and complex applications. An analysis by IBM showed that around 30% of the code of long-lived, large scale applications was devoted to the movement of data in and out of the programming language domain. The fact that this code is notoriously susceptible to system evolution errors, coupled with the statistic that 2% of the USA's GNP is spent on software 'maintenance', leads us to believe that storing long-term data in a file or database system is expensive."

King, F. IBM report on the contents of a sample of programs surveyed. San Jose, CA: IBM, 1978.
Notably cited by Atkinson &amp; Morrison https://dl.acm.org/citation.cfm?id=615226--&gt; and if you ever want to make a significant improvement to any component at any level, you pretty much have to rewrite the entire software &amp;ldquo;stack&amp;rdquo; above whichever level you are hoping to improve — in other words this requires a significant world-changing event.&lt;/p&gt;

&lt;p&gt;And yet, it runs! Ngnghm was in awe that Human computer systems could run at all; they clearly demonstrated some emerging order so powerful that it could survive despite ubiquitous design flaws — or could it possibly be surviving &lt;em&gt;thanks&lt;/em&gt; to what to this Houyhnhnm appeared as flaws? Ngnghm decided to pursue his investigations…&lt;/p&gt;
&lt;!-- http://j.mp/NgnghmPersist--&gt;&lt;/html&gt;</description></item></channel></rss>