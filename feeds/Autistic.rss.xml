<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>Houyhnhnm Computing: Posts tagged 'Autistic'</title>
  <description>Houyhnhnm Computing: Posts tagged 'Autistic'</description>
  <link>http://ngnghm.github.io/tags/Autistic.html</link>
  <lastBuildDate>Sun, 12 Jun 2016 00:34:38 UT</lastBuildDate>
  <pubDate>Sun, 12 Jun 2016 00:34:38 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>Chapter 10: Houyhnhnms vs Martians</title>
   <link>http://ngnghm.github.io/blog/2016/06/11/chapter-10-houyhnhnms-vs-martians/?utm_source=Autistic&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-ngnghm-github-io:-blog-2016-06-11-chapter-10-houyhnhnms-vs-martians</guid>
   <pubDate>Sun, 12 Jun 2016 00:34:38 UT</pubDate>
   <author>Ngnghm</author>
   <description>
&lt;p&gt;What did Ngnghm (which I pronounce &amp;ldquo;Ann&amp;rdquo;) think of &lt;a href="http://urbit.org/"&gt;Urbit&lt;/a&gt;? Some elements in Ann&amp;rsquo;s descriptions of Houyhnhnm computing (which I pronounce &amp;ldquo;Hunam computing&amp;rdquo;) were remindful of the famous Martian system software stack Urbit: both computing worlds were alien to Human Computing; both had Orthogonal Persistence; and both relied heavily on pure deterministic computations to minimize the amount of data to log in the persistence journal (as contrasted for instance with the amount of data to manipulate to compute and display answers to end-users). What else did Houyhnhnm computing have in common with Martian software? How did it crucially differ? How did they equally or differently resemble Human systems or differ from them? Ann took a long look at Urbit; while she concluded that indeed the three approaches were quite distinct, she also helped me identify the principles underlying their mutual differences and commonalities.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="urbit-the-martian-model"&gt;Urbit: The Martian Model&lt;/h3&gt;

&lt;p&gt;&lt;a href="http://moronlab.blogspot.com/2010/01/urbit-functional-programming-from.html"&gt;Martians&lt;/a&gt; have developed a peculiar operating system, &lt;a href="http://media.urbit.org/whitepaper.pdf"&gt;Urbit&lt;/a&gt; (&lt;a href="http://urbit.org/docs/"&gt;docs&lt;/a&gt;), the Terran port of which seems to be semi-usable since &lt;a href="https://medium.com/@urbit/design-of-a-digital-republic-f2b6b3109902"&gt;2015&lt;/a&gt;. At the formal base of it is a pure functional applicative virtual machine, called &lt;em&gt;Nock&lt;/em&gt;. On top of it, a pure functional applicative programming language, called &lt;em&gt;Hoon&lt;/em&gt;, with an unusual terse syntax and a very barebones static type inferencer. On top of that, an Operating System, call &lt;em&gt;Arvo&lt;/em&gt;, that on each server of the network runs by applying the current state of the system to the next event received. The networking layer &lt;em&gt;Ames&lt;/em&gt; implements a secure P2P protocol, while the underlying C runtime system, &lt;em&gt;u3&lt;/em&gt;, makes it all run on top of a regular Linux machine.&lt;/p&gt;

&lt;p&gt;The data model of &lt;em&gt;Nock&lt;/em&gt; is that everything is a &lt;em&gt;noun&lt;/em&gt;, which can be either a non-negative integer or a pair of nouns. Since the language is pure and applicative (and otherwise without cycle-creating primitives), there can be no cycle in this binary tree of integers. Since the only equality test is extensional, identical subtrees can be merged and the notional tree can be implemented as a Directed Acyclic Graph (DAG).&lt;/p&gt;

&lt;p&gt;On top of those, the execution model of Nock is to interpret some of these trees as programs in a variant of combinatory logic, with additional primitives for literals, peano integers, structural equality, and a primitive for tree access indexed by integers. The inefficiency of a naive implementation would be hopeless. However, just like the tree can be optimized into a DAG, the evaluation can be optimized by recognizing that some programs implement known functions, then using a special fast implementation of an equivalent program (which Martians call a &lt;em&gt;jet&lt;/em&gt;, by contrast with &lt;em&gt;JIT&lt;/em&gt;) rather than interpreting the original programs by following the definitional rules. Recognizing such programs in general could be hard, but in practice Urbit only needs recognize specific instances of such programs — those generated by Hoon and/or present in the standard library.&lt;/p&gt;

&lt;p&gt;Therefore, it is the C runtime system &lt;em&gt;u3&lt;/em&gt; that specifies the operational semantics of programs, whereas Nock only specifies their denotational semantics as arbitrary recursive functions. By recognizing and efficiently implementing specific Nock programs and subprograms, u3, like any efficient implementation of the JVM or of any other standardized virtual machine, can decompile VM programs (in this case Nock programs) into an AST and recompile them into machine code using the usual compilation techniques. At that point, like every VM, Nock is just a standardized though extremely awkward representation of programming language semantics (usually all the more awkward since such VM standards are often decided early on, at the point when the least is known about what makes a good representation). Where Urbit distinguishes itself from other VM-based systems, however, is that the semantics of its virtual machine Nock is forever fixed, totally defined, deterministic, and therefore &lt;em&gt;future-proof&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hoon&lt;/em&gt; is a pure functional applicative programming language. Its syntax is terse, where the core syntax is specified using non-alphanumeric characters and digraphs thereof (or equivalent for letter keywords); The syntax allows to write expressions as one liners using parentheses, but it is colloquial to break functions onto many lines where indentation is meaningful; as contrasted with other indentation-sensitive languages, however, the indentation rules are cleverly designed to prevent extraneous indentation to the right as you nest expressions, by deindenting the last, tail position in a function call. Whereas Nock is trivially typed (some would say untyped or dynamically typed), Hoon has a static type system, although quite a primitive one, with a type inferencer that requires more type hints than a language with e.g. Hindley-Milner type inference (such as ML), yet less than one without type inference (such as Java).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Arvo&lt;/em&gt; is the operating system of Urbit. The Urbit model is that the state of the system (a noun) encodes a function that will be applied to the next communication event received by the system. If the processing of the event terminates, then the event is transactionally appended to the event journal making it persistent. The value returned specifies the next state of the system and any messages to be sent to the world. Arvo is just the initial state of the system, a universal function that depending on the next event, may do anything, but in particular provides a standard library including anything from basic arithmetics to virtualization of the entire system. The core of Arvo is typically preserved when processing a message, even as the state of the system changes to reflect the computations controlled by the user; as long as this core keeps running (as it should), Arvo remains the operating system of Urbit; but users who insist may upgrade and replace Arvo with a new version, or with another system of their own creation, if they dare.&lt;/p&gt;

&lt;p&gt;The events fed into Urbit are generated by the C runtime system &lt;em&gt;u3&lt;/em&gt;, to represent console input, incoming network messages, etc. Conversely the messages generated by Urbit are translated by the implementation into console output, outgoing network messages, etc. If processing an event results in an error, if it is interrupted by the impatient user, or if it times out after a minute (for network messages), then u3 just drops the event and doesn&amp;rsquo;t include it in the event journal. (Of course, if an adversarial network message can time out an Urbit machine for a minute or even a second, that&amp;rsquo;s probably already a denial of service vulnerability; on the other hand, if the owner, being remote, can&amp;rsquo;t get his long-running computations going, that&amp;rsquo;s probably another problem.) A stack trace is generated by u3 when an error occurs, and injected as an event into Arvo in place of the triggering event, that is not persisted. Users can at runtime toggle a flag in the interactive shell &lt;em&gt;Dojo&lt;/em&gt; so that it will or won&amp;rsquo;t display these stack traces.&lt;/p&gt;

&lt;p&gt;The networking layer &lt;em&gt;Ames&lt;/em&gt; is conceptually a global broadcast network, where network messages are conceptually visible by all other nodes. However, a message is typically addressed to a specific node, using a public key for which only this node has the private key; and other nodes will drop messages they cannot decrypt. Therefore, the C runtime will optimize the sending of a message to route it directly to its destined recipient, as registered on the network. A node in the network is identified by its address, or &lt;em&gt;plot&lt;/em&gt;, that can be 8-bit (&amp;ldquo;galaxy&amp;rdquo;), 16-bit (&amp;ldquo;star&amp;rdquo;), 32-bit (&amp;ldquo;planet&amp;rdquo;), 64-bit (&amp;ldquo;moon&amp;rdquo;) or 128-bit (&amp;ldquo;comet&amp;rdquo;). A comet has for 128-bit address the cryptographic digest of its public key, making it self-authenticating. A moon has its public key signed by the corresponding planet; a planet has its public key signed by the corresponding star, a star has its public key signed by the corresponding galaxy, a galaxy has its public key included in Arvo itself, in a hierarchical system rooted in whoever manages the base Operating System. All communications are thus authenticated by construction. Galaxies, stars, planets and moons are scarce entities, thus constituting &amp;ldquo;digital real estate&amp;rdquo; (hence the name &lt;em&gt;plot&lt;/em&gt;), that the Urbit curators intend to sell to fund technological development.&lt;/p&gt;

&lt;p&gt;One of Urbit&amp;rsquo;s innovations is to invent mappings from octet to pronounceable three-letter syllables, so that you can pronounce 8-, 16-, 32-, 64- or 128-bit addresses, making them memorable, though not meaningful. So that names with the same address prefix shall &lt;em&gt;not&lt;/em&gt; sound the same, a simple bijective mangling function is applied to an address before to extract its pronunciation. This deemphasizes the signing authority behind an identity: the reputation of a person shouldn&amp;rsquo;t too easily wash onto another just because they used the same registrar; and it&amp;rsquo;s easier to avoid a &amp;ldquo;hash collision&amp;rdquo; in people&amp;rsquo;s minds by having vaguely related but notably different identities have notably different names. This constitutes an interesting take on &lt;a href="https://en.wikipedia.org/wiki/Zooko%27s%5Ftriangle"&gt;Zooko&amp;rsquo;s Triangle&lt;/a&gt;. Actually, care was taken so that the syllables would &lt;em&gt;not&lt;/em&gt; be too meaningful (and especially not offensive) in any human language that the author knew of. Non-alphanumerical characters are also given three-letter syllable names, though this time the names were chosen so that there were simple mnemonic rules to remember them (for instance, “wut” for the question mark “?”); this makes it easier to read and learn digraphs (though you might also name them after the corresponding keywords).&lt;/p&gt;

&lt;h3 id="houyhnhnms-vs-martians"&gt;Houyhnhnms vs Martians&lt;/h3&gt;

&lt;p&gt;Most importantly, the Martian&amp;rsquo;s Urbit is actually available for humans to experiment with (as of May 2016, its authors describe its status as post-alpha and pre-beta). By contrast, no implementation of Houyhnhnm Computing system is available to humans (at the same date), though the ideas may be older. This alone make Urbit superior in a non-negligible way; yet it is in all the other ways that we will examine it.&lt;/p&gt;

&lt;p&gt;Superficially, both Martian and Houyhnhnm Computing provide Orthogonal Persistence. But the way they do it is very different. Martians provide a single mechanism for persistence at a very low-level in their system, separately on each virtual machine in their network. But Houyhnhnms recognize that there is no one size fits all in matter of Persistence: for performance reasons, the highest level of abstraction is desired for the persistence journal; at the same time, transient or loosely-persisted caches are useful for extra indices; and for robustness, a number of replicas are required, with a continuum of potential synchronization policies. Therefore, Houyhnhnms provide a general framework for first-class computations, based on which users may select what to persist under what modalities.&lt;/p&gt;

&lt;p&gt;One could imagine ways that Urbit could be modified so its persistence policies would become configurable. For instance, the underlying C runtime u3 could be sensitive to special side-effects, such as messages sent to a magic comet, and modify its evaluation and persistence strategies based on specified configuration. That would mean, however, that most of the interesting work would actually happen inside u3, and not over Nock. What would Nock&amp;rsquo;s purpose then be? It could remain as an awkward but standardized and future-proof way to represent code and data. However, unless great care is taken, using formal proofs and/or extensive testing, so that the semantics of the Nock code generated indeed implements the actual computations, while indeed being implemented by the underlying system, then at the first bug introduced or &amp;ldquo;shortcut&amp;rdquo; taken, the entire Nock VM becomes a &lt;em&gt;sham&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Now, assuming Nock isn&amp;rsquo;t a complete sham, it remains an obligatory intermediate representation between the computations desired by users and the machine implementations provided by the system. Because Nock is never &lt;em&gt;exactly&lt;/em&gt; what the user wants or what the machine provides, this intermediate representation always introduces an impedance mismatch, that is all the more costly as the desired computing interactions are remote from the Nock model.&lt;/p&gt;

&lt;p&gt;In an extreme case, one could imagine that u3 would be configured using a Houyhnhnm first-class computation framework. Users would develop their computations at the level of abstraction they desired; and they would dynamically configure u3 to use the desired tower of first-class implementations. At this point, any encoding in terms of Nock could be altogether short-circuited at runtime; and any impedance mismatch introduced by Nock is thus worked around. But then, Nock is purely a hurdle and not at all an asset: all the semantics that users care about is expressed in the Houyhnhnm Computing system; any Nock code generated is just for show, obfuscating the real high-level or low-level computations without bringing anything; and Nock is either a sham, or an expensive tax on the computation framework.&lt;/p&gt;

&lt;h3 id="future-proofing-the-wrong-thing"&gt;Future-proofing the wrong thing&lt;/h3&gt;

&lt;p&gt;Both Martians and Houyhnhnms rely heavily on pure deterministic computations to minimize the amount of data to log in the persistence journal to describe issues (as contrasted for instance with the amount of data to manipulate to compute and display answers to end-users). But Martians rely on Nock, and to a lesser extent, Hoon, Arvo, Ames, etc., having a constant deterministic semantics, cast in stone for all users at all time; Houyhnhnms frown at the notion: they consider that constraint as unnecessary as it is onerous. Martians justify the constraint as making it possible to have robust, future-proof persistence. Houyhnhnms contend that this constant semantics doesn&amp;rsquo;t actually make for robust persistence, and that on the contrary, it prevents future improvements and fixes while encouraging bad practice. Also, Houyhnhms claim that requiring the function to be the same for everyone introduces an extraordinary coordination problem where none existed, without helping any of the real coordination problems that users actually have.&lt;/p&gt;

&lt;p&gt;A global consensus on deterministic computation semantics only matters if you want to replay and verify other random people&amp;rsquo;s computations, i.e. for crypto-currencies with &amp;ldquo;smart contracts&amp;rdquo; like &lt;a href="https://www.ethereum.org/"&gt;Ethereum&lt;/a&gt;; but that&amp;rsquo;s not at all what Urbit is about, and such computation replay in a hostile environment indeed has issues of its own (such as misincentives or for resource abuse) that Urbit doesn&amp;rsquo;t even try to address. If you only want to replay your own computations (or those of friends), you don&amp;rsquo;t need a global consensus on a deterministic function; you only need to know what you&amp;rsquo;re talking about, and write it down.&lt;/p&gt;

&lt;p&gt;Houyhnhnms always consider first the interactions that are supposed to be supported by computing activities. In the case of Persistence, Houyhnhnms are each interested in persisting their own code and data. There is no global entity interested in simultaneously looking at the persistence logs of everyone; there is no &amp;ldquo;collective&amp;rdquo; will, no magically coordinated knowledge. Each individual Houyhnhnm wants to ensure the persistence of their own data and that data only, or of that entrusted to them personally; and even if they want more, that&amp;rsquo;s both the only thing they must do and the only thing they can do. Now, they each want the most adequate technology for their purpose, taking costs and benefits into account. If they somehow had to coordinate together to find a common solution, the coordination would be extraordinarily costly and would take a lot of time; they would have to settle on some old technology devised when people knew least, and could never agree on improvements. And if the technology were frozen in time at the beginning, as in Urbit, nothing short of retroactive agreement using a time machine could improve it. If on the contrary each individual is allowed to choose his own persistence solution, then those who can devise improved solutions can use them without having to convince anyone; they can also compete to have their improvements adopted, whereas users compete to not be left behind, until they all adopt the improvements that make sense. In the end, in matters of persistence &lt;a href="http://common-lisp.net/project/asdf/ilc2010draft.pdf"&gt;as of build systems&lt;/a&gt;, &lt;em&gt;allowing for divergence creates an incentive towards convergence&lt;/em&gt;, reaching better solutions, through competition.&lt;/p&gt;

&lt;p&gt;Urbit incorrectly formulates the problem as being a social problem requiring a central solution, when it is actually a technical problem for which a decentralized social arrangement is much better. Persistence doesn&amp;rsquo;t require anyone to agree with other people on a low-level protocol; it only requires each person to maintain compatibility with their own previous data. To decode the data they persisted, users don&amp;rsquo;t need a one deterministic function forever, much less one they agree on with everyone else: what they need is to remember the old code and data, and to be able to express the new code (generator) in terms of the old one (to upgrade the code) and able to interpret the old data schema in terms of the new data schema (to upgrade the data). Indeed, even the &lt;a href="http://media.urbit.org/whitepaper.pdf"&gt;Urbit whitepaper&lt;/a&gt; acknowledges that as far as data above the provided abstraction matters, such schema changes happen (see section 2.0.3 Arvo).&lt;/p&gt;

&lt;p&gt;Where Martians get it just as wrong as Humans is in believing that solving one issue (e.g. persistence) at the system level is enough. But onerous local &amp;ldquo;persistence&amp;rdquo; of low-level data can actually be counter-productive when what users require is distributed persistence of high-level data at some level of service involving enough replicas yet low-enough latency: it costs a lot and for no actual benefit may cause a large increase in latency. The entire point of computing is to support user programs, and solving an issue for some underlying system at a lower-level of abstraction without solving it at the higher-level that the user cares about is actually no solution at all. It can sometimes be &lt;em&gt;part&lt;/em&gt; of a solution, but only if (1) the desired property can also be expressed in a composable way so that higher layers of software may benefit from it, and (2) the lower layers don&amp;rsquo;t impose specific policy choices that will be detrimental to the higher layers of software. And this is what Houyhnhnm systems uniquely enable that Human and Martian systems can&amp;rsquo;t express because it goes against their paradigm.&lt;/p&gt;

&lt;h3 id="neglect-for-the-meta-level"&gt;Neglect for the Meta-level&lt;/h3&gt;

&lt;p&gt;The mistake shared by Martians and Humans is to share the approach of neglecting the importance of metaprogramming.&lt;/p&gt;

&lt;p&gt;For Humans, this is often out of ignorance and of fear of the unknown: Humans are not usually trained in metaprogramming they don&amp;rsquo;t understand the importance of it, or its proper usage; they don&amp;rsquo;t know how to define and use Domain Specific Languages (DSLs). Though their job consists in building machines, they &amp;ldquo;enjoy&amp;rdquo; the job security that comes from breaking machines that would replace &lt;em&gt;their&lt;/em&gt; current jobs: Mechanized modernity for me, protectionist luddyism for thee.&lt;/p&gt;

&lt;p&gt;For Martians, unhappily, there is a conscious decision to eschew metaprogramming. One recent Urbit presentation explicitly declares that DSLs are considered harmful; the rationale given is that the base programming language should have low cognitive overload on entry-level programmers. (Though there again, the very same Urbit authors who claim their programmers shouldn&amp;rsquo;t do metaprogramming themselves spend most of their time at the meta-level — base-level for thee, meta-level for me.) To Martians, making the system deliberately simpler and less sophisticated makes it easier for people to understand and adopt it. Martians with Hoon commit the same error as the Humans systematically committed with COBOL, or to a lesser degree with Java: they designed languages that superficially allow any random layman (for COBOL) or professional (for Java) or enthusiast (for Hoon) to understand each of the steps of the program, by making those steps very simple, minute and detailed.&lt;/p&gt;

&lt;p&gt;But the price for this clarity at the micro-level is to make programs harder to follow at the macro-level. The abstractions that are denied expression are precisely those that would allow to concisely and precisely express the ideas for the actual high-level problem at hand. Every issue therefore become mired with a mass of needless concerns, extraneous details, and administrative overhead, that simultaneously slow down programmers with make-work and blur his understanding of the difficult high-level issues that matter to the user. The concepts that underlie these issues cannot be expressed explicitly, yet programmers need to confront them and possess the knowledge of them implicitly to grasp, develop and debug the high-level program. Instead of having a DSL that automatically handles the high-level concepts, programmers have to manually compile and decompile them as &amp;ldquo;design patterns&amp;rdquo;; they must manually track and enforce consistency in the manual compilation, and restore it after every change; there are more, not fewer, things to know: both the DSL and its current manual compilation strategy; and there are more things to keep in mind: both the abstract program and the details of its concrete representation. Therefore, the rejection of abstraction in general, and metaprogramming in particular, prevents unimpeded clear thinking where it is the most sorely needed; it makes the easy harder and the hard nearly impossible, all for the benefit of giving random neophytes a false sense of comfort.&lt;/p&gt;

&lt;p&gt;The same mistake goes for all languages that wholly reject syntactic abstraction, or provide a version thereof that is very awkward (like C++ templates or Java compile-time annotations) and/or very limited (such as C macros). It also applies to all programmers and coding styles that frown upon syntactic abstraction (maybe after being bitten by the bad implementations thereof such as above). If you don&amp;rsquo;t build DSLs, your general purpose language has all the downsides of Turing-equivalence with none of the upsides.&lt;/p&gt;

&lt;p&gt;Note however that even though Urbit officially rejects abstraction, Hoon is at its core a functional programming language. Therefore, unlike Humans stuck with COBOL or Java, Martian programmers using Hoon can, if they so choose, leverage this core to develop their own set of high-level composable abstractions; and for that they can reuse or get inspired by all the work done in more advanced functional languages such as Haskell or Lisp. But of course, if that&amp;rsquo;s the route chosen for further development, in the end, the programmers might better directly adopt Haskell or Lisp and make it persistent rather than use Urbit. If the Urbit persistence model is exactly what they need, they could implement a Hoon backend for their favorite language; if not, they can probably more easily reimplement persistence on their platform based on the Urbit experience than try to evolve Urbit to suit their needs.&lt;/p&gt;

&lt;p&gt;Finally, in their common rejection of metaprogramming, both the Human and Martian computing approaches lack first-class notions of meta-levels at runtime. Therefore, all their software is built and distributed as a fixed semantic tower on top of a provided common virtual machine. It&amp;rsquo;s just that the virtual machine is very different between the Humans and Martians: the Martian VM is oriented towards persistence and determinism, the Human VM is just a low-level portability layer for families of cheap human hardware. As we explained in our &lt;a href="/blog/2015/08/24/chapter-4-turtling-down-the-tower-of-babel/"&gt;chapter 4&lt;/a&gt; and subsequent chapters, this makes for rigid, brittle and expensive development processes.&lt;/p&gt;

&lt;h3 id="impedance-mismatch"&gt;Impedance Mismatch&lt;/h3&gt;

&lt;p&gt;One way that Martian is worse than Human as well as Houyhnhnm systems though is that it introduce a virtual machine that makes sense neither at a high-level nor at a low-level, but only introduces an impedance mismatch.&lt;/p&gt;

&lt;p&gt;Houyhnhnms clearly understand that the ultimate purpose of computer systems is to support some kind of interaction with some sentient users (be it via a console, via a robot, via a wider institutional process involving other sentient beings, etc.). In other words, the computer system is an enabler, a means, and the computing system is the goal, i.e. the user interactions involving applications. If some computer system makes it harder (than others; than it can; than it used to) to write, use or maintain such applications, then it is (comparatively) failing at its goal.&lt;/p&gt;

&lt;p&gt;Humans clearly understand that the ultimate starting point for building the computer software is whatever cost efficient computer hardware is available. At the bottom of the software stack are thin portable abstractions over the hardware, that together constitute the operating system. Every layer you pile on top is costly and goes against the bottom line. If it&amp;rsquo;s a good intermediate abstraction in the cheapest path from the low-level hardware to the desired high-level application, then it&amp;rsquo;s part of the cost of doing business. Otherwise it&amp;rsquo;s just useless overhead.&lt;/p&gt;

&lt;p&gt;Unhappily Martians seem to miss both points of view. The Nock virtual machine is justified neither by sophisticated high-level concepts that allow to easily compose and decompose high-level applications, nor by efficient low-level concepts that allow to cost-effectively build software as layers on top of existing hardware. It sits in the middle; and not as a flexible and adaptable piece of scaffolding that helps connect the top to the bottom; but as a fixed detour you have to make along the way, as a bottleneck in your semantic tower, a floor the plan of which was designed by aliens yet compulsorily included in your architecture, that everything underneath has to support and everything above has to rest upon.&lt;/p&gt;

&lt;p&gt;Thus, if you want your high-level programs to deal with some low-level concept that isn&amp;rsquo;t expressible in Nock (hint: it probably won&amp;rsquo;t be), then you&amp;rsquo;re in big trouble. One class of issues that Nock itself makes unexpressible yet that any programmer developing non-trivial programs has to care for is resource management: the programmer has no control over how much time or memory operations &lt;em&gt;really&lt;/em&gt; take. Yet resources such as speed and memory matter, a lot: &amp;ldquo;Speed has always been important otherwise one wouldn&amp;rsquo;t need the computer.&amp;rdquo; — Seymour Cray There &lt;em&gt;is&lt;/em&gt; a resource model in Urbit, but it&amp;rsquo;s all defined and hidden in u3, out of sight and out of control of the Martian programmer (unless we lift the lid on u3, at which point Urbiters leave Martian computing to go back to all too Human computing — and certainly not Houyhnhnm computing). At best, you have to consider evaluation of Nock programs as happening in a big fat ugly &lt;a href="https://wiki.haskell.org/Monad"&gt;Monad&lt;/a&gt; whereby programs compute functions that chain state implicitly managed by u3.&lt;/p&gt;

&lt;p&gt;Of course, you could write a resource-aware language as a slow interpreter on top of Nock, then reimplement it efficiently under u3 as &amp;ldquo;jets&amp;rdquo;. Sure you could. That&amp;rsquo;s exactly what a Houyhnhnm would do if forced to use Urbit. But of course, every time you make a change to your design, you must implement things twice, where you used to do it only once on Human or Houyhnhnm systems: you must implement your logic once as a slow interpreter in Nock; and you must implement it a second time in the Human system in which u3 jets are written. And how do you ensure the equivalence between those two implementations? You can fail to, or lie, and then Urbit is all a sham; or you can spend a lot of time doing it, at which point you wasted a lot of effort, but didn&amp;rsquo;t win anything as compared to implementing the human code without going through Urbit. What did the detour through Nock buy you? Nothing. Maybe the persistence — but only if persistence with the exact modalities offered by u3 are what you want. If you aim at a different tradeoff between latency, coherency, replication, etc., you lose. And even if perchance you aimed at the exact very same tradeoff, you might be better off duplicating the general persistence design of u3 without keeping any of Nock and Urbit above it.&lt;/p&gt;

&lt;p&gt;Oh, if only you had an advanced metaprogramming infrastructure capable of manipulating arbitrary program semantics in a formally correct way! You might then automatically generate both the Nock code in Monadic style and the supporting u3 code for your software, and be confident they are equivalent. And if furthermore your metaprogramming infrastructure could also dynamically replace &lt;em&gt;at runtime&lt;/em&gt; an inefficient implementation by a more efficient one that was shown to be equivalent, and for arbitrary programs defined by the users rather than a fixed list of &amp;ldquo;jets&amp;rdquo; hardwired in the system, then you could short-circuit any inefficiency and directly call the low-level implementation you generated without ever going through any of the Urbit code. But then, you&amp;rsquo;d have been using a Houyhnhnm system all along, and Urbit would have been a terrible impediment that you had to deal with and eventually managed to do away with and make irrelevant, at the cost of a non-trivial effort.&lt;/p&gt;

&lt;h3 id="computing-ownership"&gt;Computing Ownership&lt;/h3&gt;

&lt;p&gt;Martian computing is presented as a technical solution to a social problem, that of allowing individuals to reclaim sovereignty on their computations. That&amp;rsquo;s a lofty goal, and it would certainly be incorrect to retort that technology can&amp;rsquo;t change the structure of society. Gunpowder did. The Internet did. But Urbit is not the solution, because it doesn&amp;rsquo;t address any of the actually difficult issues with ownership and sovereignty; I have discussed some of these issues in a previous speech: &lt;a href="http://fare.tunes.org/computing/reclaim_your_computer.html"&gt;Who Controls Your Computer? (And How to make sure it’s you)&lt;/a&gt; The only valuable contribution of Urbit in this space is its naming scheme with its clever take on Zooko&amp;rsquo;s triangle — which is extremely valuable, but a tiny part of Urbit (happily, that also makes it easy to duplicate in your own designs, if you wish). The rest, in the end, is mostly a waste of time as far as ownership goes (but resurrecting the idea of orthogonal persistence is still independently cool, though its Urbit implementation is ultimately backwards).&lt;/p&gt;

&lt;p&gt;It could be argued that the Nock VM makes it easier to verify computations, and thus to ascertain that nobody is tampering with your computations (though of course these verifications can&amp;rsquo;t protect against leakage of information at lower levels of the system). Certainly, Urbit makes this possible, where random Human systems can&amp;rsquo;t do it. But if Humans wanted to verify computations they could do it much more easily than by using Urbit, using much lighter weight tools. Also, the apparent simplicity of Nock only hides the ridiculous complexity of the layers below (u3) or above (Arvo, Ames). To really verify the computation log, you&amp;rsquo;d also have to check that packets injected by u3 are consistent with your model of what u3 should be doing, which is extremely complex; and to make sense of the packets, you have to handle all the complexity that was moved into the higher layers of the system. Once again, introducing an intermediate virtual machine that doesn&amp;rsquo;t naturally appear when factoring an application creates an impedance mismatch and a semantic overhead, for no overall gain.&lt;/p&gt;

&lt;h3 id="not-invented-here"&gt;Not Invented Here&lt;/h3&gt;

&lt;p&gt;Martian computing comes with its own meta-language for sentient beings to describe computing notions. Since Martians are not Humans, it is completely understandable that the (meta)language they speak is completely different from a Human language, and that there is not exact one-to-one correspondence between Martian and Human concepts. That&amp;rsquo;s a given.&lt;/p&gt;

&lt;p&gt;Still, those who bring Martian technology to Earth fail their public every time they use esoteric terms that make it harder for Humans to understand Martian computing. The excuse given for using esoteric terms is that using terms familiar to Human programmers would come with the &lt;em&gt;wrong&lt;/em&gt; connotations, and would lead Humans to an incorrect conceptual map that doesn&amp;rsquo;t fit the delineations relevant to Martians. But that&amp;rsquo;s a cop out. Beginners will start with an incorrect map anyway, and experts will have a correct map anyway, whichever terms are chosen. Using familiar terms would speed up learning and would crucially make it easier to pin point the similarities as well as dissimilarities in the two approaches, as you reuse a familiar term then explain how the usage differs.&lt;/p&gt;

&lt;p&gt;As someone who tries to translate alien ideas into Human language, I can relate to the difficulty of explaining ideas to people whose &lt;em&gt;paradigm&lt;/em&gt; makes it unexpressible. This difficulty was beautifully evidenced and argued by Richard P. Gabriel in his article &lt;a href="https://www.dreamsongs.com/Files/Incommensurability.pdf"&gt;The Structure of a Programming Language Revolution&lt;/a&gt;. But the Urbit authors are not trying to be understood, trying their best not to be, and that&amp;rsquo;s a shame, because whatever good and bad ideas exist in their paradigm deserve to be debated, which first requires that they should be understood. Instead they lock themselves into their own autistic planet.&lt;/p&gt;

&lt;p&gt;There is a natural tradeoff when designing computing systems, whereby a program can be easy to write, be easy to read, be fast to run, and can even be two of these, but not three. Or at least, there is a &amp;ldquo;triangle&amp;rdquo; of a tradeoff (as with Zooko&amp;rsquo;s triangle), and you can only improve a dimension so much before the other dimensions suffer. But Urbit seems to fail in all these dimensions. Its alien grammar, vocabulary, primitives, paradigm, etc., make it both hard to read and hard to write; and its forced abstraction makes programs slower to run.&lt;/p&gt;

&lt;p&gt;If that abstraction came &amp;ldquo;naturally&amp;rdquo; when factoring some programs, then it could make writing these programs easier; but the Urbit VM looks very little like what Humans use for anything, and offers no &amp;ldquo;killer app&amp;rdquo; that can&amp;rsquo;t be implemented more simply. Its applicative functional machine with no cycles exchanging messages is reminiscent of the Erlang VM; but then it&amp;rsquo;s not obvious what advantages Nock brings for the applications that currently use the Erlang VM, and all too obvious what it costs. It would be much easier to make an Erlang VM persistent or to teach Erlang Ames-style authentication than to teach u3 to do anything useful.&lt;/p&gt;

&lt;p&gt;Yet, by having deliberately cut themselves from the rest of the world in so many ways, Urbit programmers find themselves forced to reinvent the world from scratch without being able to reuse much of other people&amp;rsquo;s code, except at a very high cost both in terms of implementation effort (doing things both in Nock and in u3) and integrity (ensuring the two things are equivalent, or cheating). For instance, it looks like the Urbit authors wrote a markdown processor in Hoon, for instance, and have a &amp;ldquo;jet&amp;rdquo; recognizing it and replacing it by some common Markdown library in C. Except the two pieces of code are not bug compatible, so it&amp;rsquo;s all a lie.&lt;/p&gt;

&lt;h3 id="urbit-as-a-demo"&gt;Urbit as a demo&lt;/h3&gt;

&lt;p&gt;Urbit has none of the support for modular design necessary for programming &lt;a href="https://en.wikipedia.org/wiki/Programming_in_the_large_and_programming_in_the_small"&gt;&amp;ldquo;in the large&amp;rdquo;&lt;/a&gt;. But its superficial simplicity of Nock makes it suitable as a cool demo of orthogonally persistent system.&lt;/p&gt;

&lt;p&gt;Of course, the demo only &amp;ldquo;works&amp;rdquo; by sweeping under the rug the difficult issues, to be solved by u3, the metasystem of Urbit; and unlike Nock, u3, where most of the interesting things happen, remains informal in its all-important side-effects, and not actually bound to behave as a faithful implementation as for the parts specified by the Nock machine. In other words, the pretense of having fully formalized the state of the system and its state function, and of putting the end-user in control of it, is ultimately a &lt;em&gt;sham&lt;/em&gt;, a corruption. The power remains in the opaque and totally unspecified centralized implementation of the metaprogram that implements Nock and issues real-world side-effects.&lt;/p&gt;

&lt;p&gt;There is no one-size fits all way to handle all the issues with connection to real-world devices, and policies that resolve tradeoffs regarding persistence, privacy, latency, efficiency, safety, etc. A centralized implementation for the metaprogram that handles them is not a universal solution. Only a general purpose platform for people to build their own metaprograms can enable them to each solve the issues to their satisfaction. And once you have this platform, you don&amp;rsquo;t need any of the Urbit operating system, because you already have a Houyhnhnm computing system.&lt;/p&gt;

&lt;p&gt;Houyhnhnms have no ill feelings towards either Martians or Humans. They hope that Urbit will be a great success, and demonstrate a lot of cool things and inspire people to adopt orthogonal persistence. However, Houyhnhnms believe that Urbit won&amp;rsquo;t be able to outgrow being a cool demo unless it embraces a more general purpose metaprogramming architecture.&lt;/p&gt;</description></item>
  <item>
   <title>Chapter 7: Platforms not Applications</title>
   <link>http://ngnghm.github.io/blog/2015/12/25/chapter-7-platforms-not-applications/?utm_source=Autistic&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-ngnghm-github-io:-blog-2015-12-25-chapter-7-platforms-not-applications</guid>
   <pubDate>Sat, 26 Dec 2015 03:33:44 UT</pubDate>
   <author>Ngnghm</author>
   <description>
&lt;p&gt;My previous discussion with Ngnghm (or Ann as I call her) left me baffled: I could somehow understand that &lt;a href="/blog/2015/11/28/chapter-6-kernel-is-as-kernel-does/"&gt;Houyhnhnms don&amp;rsquo;t have the concept of an Operating System Kernel&lt;/a&gt; (note that I pronounce &amp;ldquo;Houyhnhnm&amp;rdquo; &amp;ldquo;Hunam&amp;rdquo;); and I could vaguely guess how each of the many aspects of a Human kernel could correspond to a family of software patterns in a Houyhnhnm computing system, at various levels of abstractions. But while I could visualize these patterns individually, it was less clear to me what the big picture was when these smaller compile-time, link-time and runtime abstractions were put together. So I decided to approach their software architecture from the other end: what do end-user applications look like in Houyhnhnm computing systems?&lt;/p&gt;

&lt;p&gt;I was baffled again, but not surprised anymore, to find that Houyhnhnms don&amp;rsquo;t have a notion of application. Granted, there are simple cases where Human applications have direct counterparts in Houyhnhnm computing systems. But in the general case, Houyhnhnms don&amp;rsquo;t think in terms of standalone applications; they think in terms of platforms that they extend with new functionality.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="autistic-applications"&gt;Autistic Applications&lt;/h3&gt;

&lt;p&gt;Ann was starting to get familiar with Human computer systems, and the few end-user applications that he was using daily. She noticed that a certain class of applications was quite reminiscent of software that existed in Houyhnhnm computing systems, at least superficially: self-contained end-user applications, such as games, interactive art, audiovisual performances, showroom displays, news and other writings, etc. These applications had in common that they are made to be explored by the user but not modified in any significant way; they mostly didn&amp;rsquo;t communicate much, if at all, with any other application in any way that the end-user cared to control; they had no significant input and no output beside the user experience. I dubbed the concept &lt;em&gt;autistic applications&lt;/em&gt;. But when Ann tried to translate the Houyhnhnm expression for the concept, it sounded more like &lt;em&gt;interactive documents&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In any case, these things look pretty much the same in Houyhnhnm computing systems and Human computer systems: You somehow get ahold of the software; installing it automatically installs its dependencies, if any; you run it in a sandbox (at least Houyhnhnms do); and you interact with it. It doesn&amp;rsquo;t matter too much what the program does (if anything), precisely because the information flow is essentially one way, from the application to the user.&lt;/p&gt;

&lt;p&gt;Still, there were a few subtle points where even these autistic applications in Human computer systems differ from interactive documents in Houyhnhnm computing systems. For instance, in a Houyhnhnm computing system, you can always copy and paste text and pictures and sounds, search for words in registered dictionaries, or otherwise manipulate the application output; these do not require the application developers having to do anything to enable such features. But a more striking difference is that all Houyhnhnm activities inherit from the system its &lt;a href="/tags/Orthogonal-Persistence.html"&gt;orthogonal persistence&lt;/a&gt;. You can thus always interrupt the application and save and restore its state at any point in time, except where explicitly not desired (e.g. in the middle of a transaction). Then you can go back in time and replay (and in the case of videos or music, go forward in time), according to a protocol that is uniform across applications; and not only is it no additional burden on application programmers, it is something they can&amp;rsquo;t get subtly wrong, and that users can thus casually rely upon. There is never any loss of any session state, disappearing tabs or windows, games where you can&amp;rsquo;t both pause and save your game, etc. There are no messages you have to enter twice because they were cleared between two page loads or browser crash and restart, or that reappear because the clearing failed to be recorded.&lt;/p&gt;

&lt;p&gt;Of course, in a Houyhnhnm computing system, interactive documents (like any other activity), even when they require interaction with a remote service, are always able to save and restore the client-side state from previous user interactions; however that does not entail being able to save and restore any server-side state, at least not without support from the server. And while the system typically makes it easy for the server developers to provide that support if they want, there are many reasons why they might not want to do it, including cost and confidentiality. Conversely, for reasons of privacy, a user might want to replay a previous session without telling the remote server. Also, for regression testing or for debugging their applications, developers may want to replay parts of the server side interactions without affecting the users. All these behaviors are expressible in Houyhnhnm computing systems: you can specify the scope and context in which you replay some computation, within the resources that you control.&lt;/p&gt;

&lt;h3 id="typical-applications"&gt;Typical Applications&lt;/h3&gt;

&lt;p&gt;Now, most applications are not autistic; they do involve exchanging data with other applications: using data produced by other applications, and producing data that will be used by other applications. In other words, the information processes they partake in may directly involve other automated programs; they do not require a Sentient being&amp;rsquo;s brain (Human or Houyhnhnm) as an exclusive intermediate between their processing and further automated processing; the sentient being doesn&amp;rsquo;t have to recreate the entirety of the next program&amp;rsquo;s input based on what it sees or remembers of the previous program&amp;rsquo;s output. And there we see that even &amp;ldquo;autistic applications&amp;rdquo; are not &amp;ldquo;autistic processes&amp;rdquo;: An autistic application does not communicate with other automated programs but does interact with sentient users; its implementation might also interact with other programs below the abstraction provided to the user, though that&amp;rsquo;s mostly invisible to the user. An &amp;ldquo;autistic process&amp;rdquo; that communicates with no other process whatsoever, not even those in a sentient being&amp;rsquo;s brain, can and will be wholly optimized away.&lt;/p&gt;

&lt;p&gt;Ann then explained that the situation differs sharply between Human and Houyhnhnm systems regarding all these typical, non-autistic, applications — to the point that Houyhnhnms don&amp;rsquo;t really have a notion of application. For technical reasons with historical roots in marketing, Human computer systems tend to organize software into neatly separated, standalone, black-box &amp;ldquo;applications&amp;rdquo;; communication between different applications is very difficult, and must be explicitly handled by each of these applications; every application must include an implementation of all these modes of communication it will partake in. Instead, Houyhnhnm computing systems consider such communication the heart of the system, and make it comparatively easy; they do not usually have self-contained &amp;ldquo;applications&amp;rdquo;; they start from a common platform that handles all the communication; and they extend this platform to handle new kinds of situations, until they include all the interesting situations that the &amp;ldquo;application&amp;rdquo; would have covered.&lt;/p&gt;

&lt;p&gt;A first obstacle to inter-application communication in Human computer systems, is that the only common abstractions are very low-level, in terms of arrays of bytes. Any higher-level objects have to be encoded into sequences of bytes, shipped across costly runtime virtual process boundaries, then decoded back into objects on the other side by a matching algorithm. Applications thus have to agree on complex, expensive, bug-prone yet inexpressive low-level communication protocols that are big security liabilities. Having to deal with such protocols is a huge barrier to entry that explains why few programmers endeavour to try it. A lot of this work can be completely automated using type-directed code-generation; and the better Human systems do it to a point (see Protocol Buffers, Cap&amp;rsquo;n&amp;rsquo;Proto, piqi, etc.); but the integration with types of actual programming languages remains generally lackluster. What types can be used for generally shareable data remain very limited and inexpressive, whereas whatever types they can use for manipulating data within a given program remain generally oblivious of any sharing constraints, ownership rights, access restrictions, etc.&lt;/p&gt;

&lt;p&gt;In Houyhnhnm computing systems, communication of objects is handled by the system at the highest level of abstraction possible: that of whichever types and checks are being used to define and validate these objects. Low-level encoding and decoding can be eschewed altogether for linear objects where both processes trust each other with respect to representation invariants; it can sometimes be reduced to mere checking when the trust is incomplete; and where encoding or checking is actually required, it is automatically extracted based on type information available either at compile-time or at runtime. The programming language types &lt;em&gt;are&lt;/em&gt; the communication types, and if foreign languages need to communicate with each other, it&amp;rsquo;s a regular matter of FFI (Foreign Function Interface) that you need to solve anyway, and might as well solve once and for all, rather than have each application invent its own bad incompatible partial solution.&lt;/p&gt;

&lt;p&gt;A second obstacle to inter-application communication in Human computer systems is that they have very poor algebras and interfaces for users to combine processes. For most users, sharing data between applications requires one of two things: selecting and copying (or cutting) data from one application using a mouse, then pasting it into another application; or having the application save or export a file to a local disk, then opening or importing that file in another application (with &amp;ldquo;interesting&amp;rdquo; consequences when two applications try to modify it at the same time). Developers can do better, but there&amp;rsquo;s a large discontinuity between the skills required to merely use the system, and the skills required to do even the simplest things as you program the system. Modern Human computer systems tend to allow for an intermediate layer between the two, &amp;ldquo;scripting&amp;rdquo;, with Unix shells and their pipes, or the notably more modern PowerShell on Windows. Scripting lowers the barrier to building applications, and when using &amp;ldquo;client&amp;rdquo; utilities and libraries, allows programmers to share data beyond copy-pasting and files; but it still remains quite complex to use, and often brittle and limited in expressiveness, because it does not directly partake in either of the programs&amp;rsquo; invariant enforcement and atomic transactions (though a few applications offer a suitable transactional interface).&lt;/p&gt;

&lt;h3 id="houyhnhnm-platforms"&gt;Houyhnhnm Platforms&lt;/h3&gt;

&lt;p&gt;Houyhnhnm computing systems are based on the premise of small modular entities that each do one thing well; and these entities can be combined inside a common platform that does its best to reduce the discontinuity between using and programming. To Houyhnhnms, there is no difference between using and programming; if anything, &lt;em&gt;the difference between a programmer and a user, is that the programmer knows there is no difference between using and programming&lt;/em&gt;. Certainly, there is a continuum of proficiency and knowledge amongst users; but there is generally no large barrier to overcome in order for users to generalize and automate as a script whatever computations they know how to achieve interactively; and there isn&amp;rsquo;t a large amount of boilerplate required to write the least program, as there is in all Human programming languages except &lt;a href="https://github.com/fare/asdf3-2013/blob/master/scripting-slides.rkt"&gt;&amp;ldquo;scripting languages&amp;rdquo;&lt;/a&gt;. Houyhnhnm platforms are built around a high-level programming language accessible to the user; therefore communication happens directly using objects in the system language so no serialization or deserialization into low-level bit sequences is required (or if it is, for the sake of network communication, it can be automated); and the system language is available to name entities, combine and apply programs.&lt;/p&gt;

&lt;p&gt;A few Human computer systems have historically followed this model: Smalltalk workstations (from Xerox), Lisp Machines (from Xerox, MIT, Symbolics, LMI or TI), Hypercard (on old Apple Macintosh&amp;rsquo;es); to a point, HP calculators or Mathematica. But perhaps the most successful such platform to date is &lt;a href="https://www.gnu.org/software/emacs/"&gt;GNU Emacs&lt;/a&gt;: It is largely written as a set of modules in a &amp;ldquo;scripting language&amp;rdquo;, Emacs Lisp. Entities defined in a module can be freely used in another one, and data is directly exchanged without going through any communication or translation layer. Emacs Lisp is antiquated, more so than Smalltalk or Lisp Machine Lisp ever were, and its data structures are heavily biased towards text editing; and yet it remains widely used and actively developed, because in many ways it&amp;rsquo;s still far ahead of any competition despite its limitations.&lt;/p&gt;

&lt;p&gt;In a Houyhnhnm computing system, programmers do not write standalone applications in non-autistic cases; instead, they write new modules that extend the capabilities of the platform. Often, a new module will extend the system to handle new entities. As long as these entities implement common interfaces, they can be used along all previously known entities by all existing modules that use these interfaces. For instance, a new picture compression format is automatically usable by each and every function that uses pictures throughout the system; a common extensible picture editor can be used on all pictures anywhere on the system; a common extensible text editor can handle any kind of writable text in the system; etc. At all times, each of these modules, including all common editors, will include all the user&amp;rsquo;s customizations; this makes writing customizations much more worthwhile than if separate customizations had to be written for each application, each in its own language with its own learning curve, as is the case in Human computer systems.&lt;/p&gt;

&lt;p&gt;A new module may also define new interfaces, and how they apply to existing kinds of entities. There is of course a problem when two modules that don&amp;rsquo;t know each other extend the system by one adding new kinds of entities and the other defining new kinds of interfaces, the combination leading to new cases that are not handled. Houyhnhnm systems are not magic and can&amp;rsquo;t generate handlers for those cases out of thin air: a further module may define how to handle these new combinations; or a suitable generic fallback may have been provided with the new interface; or lacking any of the above, the system will fail and drop to its metasystem, that will handle the error. In the end, it&amp;rsquo;s still the job of some programmer to ensure that the overall system works suitably in the legitimate cases that it will actually encounter. These issues exist in Human and Houyhnhnm systems alike — the only difference is that Human computer systems are so difficult to extend that programmers seldom reach the point when they can confront these problems, whereas Houyhnhnm computing system eliminate enough of the artificial problems in extending the system that users are more often confronted with these extension-related issues.&lt;/p&gt;

&lt;h3 id="different-shapes"&gt;Different Shapes&lt;/h3&gt;

&lt;p&gt;Because of the high barrier to communication between applications in Human computer systems, these applications tend to grow into big hulking pieces of software that try to do everything — yet can&amp;rsquo;t. Indeed, even a picture editor will need to edit text to overlay on pictures, to email the pictures, to browse the filesystem looking for pictures to edit, to search pictures by date or by location, etc. It needs to be extensible to accept new file formats, new color schemes, new filters, new extraction tools, new analyses, new generation techniques, new scanning sources, new social networks on which to publish pictures, etc. Soon, it becomes a platform of its own, its own extension API, its own scripting language, its own plugin ecosystem, its own configuration system, its own sandboxing infrastructure. Every successful application grows this way, until it does many of the same things as all the other applications, all of them badly, except those within its own application core.&lt;/p&gt;

&lt;p&gt;In a Houyhnhnm computing system, a picture editor will handle picture editing, and picture editing only — and do it well. It will delegate sending email, browsing the filesystem, searching for pictures, etc., to suitable other modules of the common platform. Instead of extensions being available for a single application, they will be available to all software. Thus, whereas Human computer systems feature one unwieldy file selector for each application, Houyhnhnm computing systems instead will have a single file selection service for the entire platform. All the improvements ever made to file selection will be available to all activities instead of only a single application: preview of contents, browsing history, restriction and search by type or by many criteria beside filename hierarchy, relevance to context, selection or tagging of multiple files instead of one at once, automatic refresh of search results, generation of content on demand, etc. Security will notably be improved by each component only having access to the capabilities it needs, containing any security breach by construction.&lt;/p&gt;

&lt;h3 id="extension-languages"&gt;Extension Languages&lt;/h3&gt;

&lt;p&gt;Many Human application developers eventually realize that the growing set of predefined usage patterns they develop over time can never cover all the cases required by all potential users. So they eventually invent their own configuration and extension language, so that users can define their own usage patterns. But most application developers are no programming language specialists; even when they are, being pressured by the application development deadlines, they just don&amp;rsquo;t possess the resources to implement more than the strict minimum necessary for a programming language; and they never planned in advance for adding such a language, so it doesn&amp;rsquo;t fit well in their large existing code base. Therefore they usually end up with a very badly designed language, very inefficiently implemented, and no tooling to support using it besides print-debugging at the end of a long edit-compile-test cycle. That resulting language can be very good at the few initial predefined operations, and passable when using some limited usage patterns, but is consistently bad at everything else. Yet it costs a lot to develop, and even more to do without.&lt;/p&gt;

&lt;p&gt;In contrast, Houyhnhnms will use their common platform to configure and extend all software. The platform comes with a variety of programming languages each designed by the best programming language designers; it provides an efficient implementation framework, great tooling, all the programming paradigms users may desire. There are also many ways for developers to control the expressiveness of configuration languages: domain-specific languages, type systems, contracts, etc. Not only do such expressiveness restrictions make it easier for domain experts to precisely express what an application requires, in the terms that best make sense for the domain (here using the informal meaning of &amp;ldquo;application&amp;rdquo;, not the Human computer system notion); they also enable domain-specific meta-programming: since the configurations follow a given pattern or can be otherwise normalized to objects of a given type, various kinds or guarantees, queries and optimizations may apply.&lt;/p&gt;

&lt;h3 id="programming-incentives"&gt;Programming Incentives&lt;/h3&gt;

&lt;p&gt;More generally, considering the larger computing system that includes the sentient programmers, Human computer systems display a singular failure of &lt;em&gt;division of labour and specialization of tasks&lt;/em&gt;. Developer talent is stretched thin, as the same tasks are done over and over, once per application, almost never well, by developers who are seldom specialists in those tasks. Meanwhile, those few developers who are good at a task cannot focus on doing it in a way that will benefit everyone, but must instead improve a single application at a time. And because hiring is application-based rather than specialty-based, even specialists are seldom funded to do what they are good at, instead being paid to badly writing yet another implementation of a feature at which they are mediocre, for whichever application they were hired about. Cultivating a platform is an afterthought to application growth; which platforms happen to succeed depends not at all on its good design, but on a package deal with other aspects of which application will grow biggest. As a result, most successful application extension platforms start as some amateur&amp;rsquo;s quick and dirty design under pressure, with a requirement of matching the application&amp;rsquo;s existing API; platforms then have to forever deal with backward compatibility with a bad and skewed design. Since there is no common platform, developers must relearn the badly designed ad hoc partial platform of each application before they can be productive; this increases the barrier to entry to coding, entrenches the market fragmentation, and adds up to a huge deadweight loss for society as a whole.&lt;/p&gt;

&lt;p&gt;In Houyhnhnm computing systems, experts of any given domain can focus on their domain of expertise, contracting their services to those who require improvement for their applications. Experts don&amp;rsquo;t need to restart their work from scratch for every application, but need only do it once per platform. And there are only a few worthwhile platforms, and they each are designed by experts at platform design rather than by some random application developer. Where a number of expertises must be integrated together toward an &amp;ldquo;application&amp;rdquo;, choosing and cultivating a well-designed platform for software growth is not an afterthought but a prerequisite for the project manager. Without artificial barriers to development, the total amount of effort expanded on each feature is much lower in Houyhnhnm computing systems than in Human computer systems, while the average domain expertise of those who implement each feature is much higher. Houyhnhnms thus achieve better software quality at the cost of a lower quantity of development efforts than Humans. They don&amp;rsquo;t have an &amp;ldquo;app economy&amp;rdquo;, but they do have active markets where producers sell interactive documents and platform extensions, either as services or products.&lt;/p&gt;

&lt;p&gt;The economic structure of software development as well as its technical architecture is thus crucially affected by this simple change of point of view, from &lt;em&gt;computer&lt;/em&gt; systems to &lt;em&gt;computing&lt;/em&gt; systems.&lt;/p&gt;</description></item></channel></rss>