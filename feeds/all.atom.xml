<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
 <title type="text">Houyhnhnm Computing: Houyhnhnm Computing</title>
 <link rel="self" href="http://ngnghm.github.io/feeds/all.atom.xml" />
 <link href="http://ngnghm.github.io/index.html" />
 <id>urn:http-ngnghm-github-io:-index-html</id>
 <updated>2015-11-29T04:34:45Z</updated>
 <entry>
  <title type="text">Chapter 6: Kernel Is As Kernel Does</title>
  <link rel="alternate" href="http://ngnghm.github.io/blog/2015/11/28/chapter-6-kernel-is-as-kernel-does/?utm_source=all&amp;utm_medium=Atom" />
  <id>urn:http-ngnghm-github-io:-blog-2015-11-28-chapter-6-kernel-is-as-kernel-does</id>
  <published>2015-11-29T04:34:45Z</published>
  <updated>2015-11-29T04:34:45Z</updated>
  <author>
   <name>Ngnghm</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;I admitted to Ngnghm that I was baffled by Houyhnhnm computing systems; to better understand them, I wanted to know what their kernels, libraries and applications looked like. There again, he surprised me by having no notion of what I called kernel or application: the way Houyhnhnm systems are architected leads to widely different concepts; and for the most part there isn&amp;rsquo;t a direct one-to-one correspondance between our notions and theirs. And so I endeavored to discover what in Houyhnhnm computing systems replaces what in Human computer systems is embodied by the operating system kernel.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="kernels"&gt;Kernels&lt;/h3&gt;

&lt;p&gt;&amp;ldquo;What does an Operating System Kernel look like in a Houyhnhnm computing system?&amp;rdquo; I asked Ngnghm. He wasn&amp;rsquo;t sure what I was calling either Operating System or Kernel.&lt;/p&gt;

&lt;p&gt;I explained that in a Human computer system, the kernel is a piece of software that handled the hardware resources, and provided some uniform abstractions that isolated users from the hardware details that varied from machine to machine and across time. All the rest of the system is expressed in terms of those abstractions; modern computer hardware ensures through runtime checks that all programs beside the kernel run in a &amp;ldquo;user mode&amp;rdquo; that only sees these abstractions; the kernel alone runs in a &amp;ldquo;kernel mode&amp;rdquo; that gives it direct access to the hardware. The kernel can also use this hardware support to provide low-level isolation between &amp;ldquo;processes&amp;rdquo;: it allows multiple user programs to run at the same time while ensuring that none may interfere with other programs except through said abstractions.&lt;/p&gt;

&lt;p&gt;Ngnghm however had trouble distinguishing the kernel from any other program based on my description. The notion of kernel, like most concepts of Human computer systems, was too artifact-oriented and didn&amp;rsquo;t fit the grid of interaction-oriented Houyhnhnm computing systems. &amp;ldquo;What is it that a kernel &lt;em&gt;does&lt;/em&gt;?&amp;rdquo; Ngnghm asked me; when he&amp;rsquo;d know that, he could tell me how their systems implement analogous &lt;em&gt;interactions&lt;/em&gt;. And then I was at loss to distinguish exactly what kinds of interaction a kernel does that other pieces of software don&amp;rsquo;t.&lt;/p&gt;

&lt;h3 id="resource-management"&gt;Resource Management&lt;/h3&gt;

&lt;p&gt;The most obvious thing a kernel does is that it manages and &lt;em&gt;multiplexes&lt;/em&gt; resources: it takes some resources, such as CPU time, core memory, disk space, console access, network connections, etc.; and it makes available to multiple programs, either one after the other, or at the same time. It ensures that each program can use all the resources it needs without programs stepping on each other&amp;rsquo;s toes and corrupting each other&amp;rsquo;s state.&lt;/p&gt;

&lt;p&gt;However, resource management cannot &lt;em&gt;define&lt;/em&gt; what a kernel is, since plenty of other components of a computer system also manage resources: All but the simplest programs contain a memory allocator. A database server, or any kind of server, really, manages some kind of records. A sound mixer, a 3D scene renderer, a Window system, or anything worth of being called a system at all, allow multiple entities to coexist, interact with each other, and be perceived, modified, accessed, experienced, by the system&amp;rsquo;s user.&lt;/p&gt;

&lt;p&gt;Houyhnhnms recognize that resource management is an infinitely varied topic; this topic cannot possibly be reduced to a fixed set of resources, but is an inherent aspect of most programs. When they need to explicitly deal with this aspect, Houyhnhnms make it an explicit part of the rich algebra they use to express programs. The simplest idiom for that is to use a proxy, a handle, or some indirect way of naming a resource; programs that use the resource may only go through that handle, while only the program that manages the resource manipulates the underlying details. More advanced idioms include using some variant of what we call &lt;a href="https://en.wikipedia.org/wiki/Linear_logic"&gt;linear logic&lt;/a&gt;; on some systems, linear logic can also be painfully emulated using monads.&lt;/p&gt;

&lt;h3 id="access-control"&gt;Access Control&lt;/h3&gt;

&lt;p&gt;A kernel also provides some kind of &lt;em&gt;access control&lt;/em&gt; to the resources it exposes: for instance, you have to login as a &lt;em&gt;user&lt;/em&gt; to access the system; then you can only access those files owned by said user, or explicitly shared by other users.&lt;/p&gt;

&lt;p&gt;But there again so does any system manage access to resources. Moreover, whichever access control a Human Computer System kernel provides is often so primitive that it&amp;rsquo;s both too slow to be in any code&amp;rsquo;s critical path and yet too coarse and too inexpressive to match any but the most primitive service&amp;rsquo;s intended access policies. Therefore, every program must either reimplement its own access control from scratch or become a big security liability whenever it&amp;rsquo;s exposed to a hostile environment.&lt;/p&gt;

&lt;p&gt;Houyhnhnms recognize that access control too is not a fixed issue that can be solved once and for all for all programs using a pre-defined one-size-fits-all policy. It can even less be solved using a policy that&amp;rsquo;s so simple that it maps directly to a bitmask and some simple hardware operations. Instead, they also prefer to provide explicit primitives in their programming language to let programmers define the access abstractions that fit their purposes; in doing so, they can use common libraries to express all the usual security paradigms and whichever variant or combination the users will actually need; and these primitives fit into the algebra they use to manage resources above.&lt;/p&gt;

&lt;h3 id="abstraction"&gt;Abstraction&lt;/h3&gt;

&lt;p&gt;A Human Computer System kernel (usually) provides &lt;em&gt;abstraction&lt;/em&gt;: all programs in the system, beside the kernel itself, are &lt;em&gt;user&lt;/em&gt; programs; their computations are restricted to &lt;em&gt;only&lt;/em&gt; be combinations of the primitives provided by the &lt;em&gt;system&lt;/em&gt;, as implemented at runtime by the kernel. It is not possible for user programs to subvert the system and directly access the resources on top of which the system itself is built (or if it is, it&amp;rsquo;s a major security issue to be addressed as the highest emergency). The system thus offers an abstraction of the underlying hardware; and this abstraction offers portability of programs to various hardware platforms, as well as security when these programs interact with each other. More generally, abstraction brings the ability to reason about programs independently from the specificities of the hardware on which they will run (&amp;ldquo;abstracting away&amp;rdquo; those specificities). And this in turn enables &lt;em&gt;modularity&lt;/em&gt; in software and hardware development: the division of labor that makes it possible to master the otherwise unsurmountable complexity of a complete computer system.&lt;/p&gt;

&lt;p&gt;Now and again, abstraction is also what any library or service interface provides, and what every programming language enforces: by using the otherwise opaque API of the library or service, programmers do not have to worry about how things are implemented underneath, as long as they follow the documented protocol. And by using a programming language that supports it, they can rely on the compiler-generated code always following the documented protocol, and they don&amp;rsquo;t even have to worry about following it manually: as long as the program compiles and runs, it can&amp;rsquo;t go wrong (with respect to the compiler-enforced protocols). Abstraction in general is thus not a defining activity of an operating system kernel either; and neither is abstraction of any of the specific resources it manages, that are often better abstracted by further libraries or languages.&lt;/p&gt;

&lt;p&gt;Houyhnhnms not only reckon that abstraction is an essential mechanism for expressing programs; Houyhnhnms also acknowledge that abstraction is not reserved to a one single &amp;ldquo;system&amp;rdquo; abstraction to be shared by all programmers in all circumstances. Rather, abstraction is an essential tool for the division of mental labor, and is available to all programmers who want to define the limits between their respective responsibilities. The program algebras used by Houyhnhnms thus have a notion of first-class programming system (which includes programming language as well as programming runtime), that programmers can freely define as well as use, in every case providing abstraction. Since they are first-class, they can also be parametrized and made from smaller blocks.&lt;/p&gt;

&lt;p&gt;Note, however, that when parametrizing programming systems, it is important to be able to express &lt;em&gt;full&lt;/em&gt; abstraction, whereby programs are prevented from examining the data being abstracted over. A fully abstracted value may only be used according to the interface specified by the abstract variable type; thus, unless that abstract type explicitly includes some interface to inspect the actual value&amp;rsquo;s type or to deconstruct its value according to specific match patterns, the client code won&amp;rsquo;t be able to do any of these, even if in the end the actual value provided happens to be of a known type for which such operations are available. A dynamic language may implement it through opaque runtime proxies; a static language may provide this feature through static typing; some languages, just like &lt;a href="http://www.csc.villanova.edu/~japaridz/CL/"&gt;computability logic&lt;/a&gt;, may distinguish between &amp;ldquo;blind&amp;rdquo; quantifiers and regular &amp;ldquo;parallel&amp;rdquo; or &amp;ldquo;choice&amp;rdquo; quantifiers. In any case, the fragment of code in which a full abstraction holds is prevented from peering inside the abstraction, even if the language otherwise provides reflection mechanisms that can see through regular abstractions. Of course, when turtling down the tower of implementations, what is a completely opaque full abstraction at a higher level may be a fully transparent partial abstraction at a lower level; that&amp;rsquo;s perfectly fine — the lower-level, which requires proper permissions to access and modify, is indeed responsible for properly implementing the invariants of the higher-level.&lt;/p&gt;

&lt;h3 id="enforced-and-unenforced-abstractions"&gt;Enforced and Unenforced Abstractions&lt;/h3&gt;

&lt;p&gt;There is one thing, though, that kernels do in Human computer systems that other pieces software mostly don&amp;rsquo;t do — because they mostly can&amp;rsquo;t do it, lacking system support: and that&amp;rsquo;s &lt;em&gt;enforcing&lt;/em&gt; full abstraction. Indeed, in a Human computer system, typically, only the operating system&amp;rsquo;s invariants are enforced. They are enforced by the kernel, and no other piece of software is allowed to enforce anything. If a process runs as a given user, say &lt;code&gt;joe&lt;/code&gt;, then any other process running as &lt;code&gt;joe&lt;/code&gt; can do pretty much what it wants to it, mucking around with its files, maybe even its memory, by attaching with a debugger interface, etc. If a user is allowed to debug things he runs at all (and he probably should be allowed), then all processes running as that user are allowed, too. Users in Unix or Windows can&amp;rsquo;t create sub-users that they control, in which they could enforce their user-defined invariants. Any failed invariant potentially puts the entire system at risk, and any security breach means everything the user does is affected (which on single-user computers, means everything worthwhile on the computer). That subsystems shall not break their own or each other&amp;rsquo;s invariants thus remains a pure matter of convention: the kernel will not enforce these invariants at all; they are enforced solely by myriads of informal naming conventions, manual management by system administrators, and social pressure for software developers to play well with software developed by others. Any bug in any application exposed to the internet puts the entire system at risk.&lt;/p&gt;

&lt;p&gt;There does exist a tool whereby user-defined invariants can be enforced, of sorts: machine emulation, machine virtualization, hypervisors, containers, user-level filesystems, etc., allow to run an entire human machine with its own kernel. However, except for the most costly and least powerful strategy, emulation, that is always available, these tools are not available for casual users or normal programs; they are heavy-weight tools that require system administrator privileges, and a lot of setup indeed. Still, they exist; and with these tools, companies with a large expensive engineering crew can enforce their company-wide invariants; they can thus enjoy the relative simplicity that comes when you can reason about the entire system, knowing that parasitic behaviors have been eliminated, because they are just not expressible in the &lt;a href="https://en.wikipedia.org/wiki/Unikernel"&gt;unikernels&lt;/a&gt; that are run inside the managed subsystems.&lt;/p&gt;

&lt;p&gt;Houyhnhnms recognize that the invariants that ultimately matter in a computing system are never those that underlie any &amp;ldquo;base&amp;rdquo; system; instead, they are always those of the overall system, the &amp;ldquo;final&amp;rdquo; applications, as experienced by users. To them, the idea that there should be a one privileged &amp;ldquo;base&amp;rdquo; system, with a kernel that possesses a monopoly on invariant enforcement, is absurd on its face; the invariants of a system low-level enough to implement all the programs that users may care about are necessarily way too low-level to matter to any user. In Houyhnhnm computing systems, virtualization is a basic ubiquitous service that is universally relied upon; each activity is properly isolated and its invariants cannot be violated by any other activity, except those that explicitly run at its meta-level.&lt;/p&gt;

&lt;p&gt;What&amp;rsquo;s more, Houyhnhnms understand that when building software components and applications, programmers almost never want to start from that &amp;ldquo;base&amp;rdquo; system of a low-level machine, however virtualized, but want to start from as high-level a system as they can afford. Therefore, Houyhnhnms have first-class notions for computing systems and for implementing a computing system based on another computing system (again, in terms closer to notions of a human computer systems, a computing system includes both a programming language and a runtime virtual machine). Which functionality is virtualized, and which is just inherited unmodified from the lower-level system, can thus be decided at the highest level that makes sense, keeping the overall system simpler, easier to reason about, and easier to implement efficiently — which is all one and the same.&lt;/p&gt;

&lt;p&gt;Certainly, some technical enforcement cannot wholly replace social enforcement: some invariants are too expensive to enforce through technical means, or would require artificial intelligence to do so, which Houyhnhnms don&amp;rsquo;t possess more than Humans. But at least, Houyhnhnms can minimize the &amp;ldquo;surface of attack&amp;rdquo; for technical defects that can possibly violate desired invariants, by making such attacks impossible without a meta-level intervention; and those meta-level interventions that are necessary can be logged and reviewed, making for more effective social enforcement as well as for general reproducibility and debuggability.&lt;/p&gt;

&lt;h3 id="runtime-and-compile-time-enforcement"&gt;Runtime and Compile-time Enforcement&lt;/h3&gt;

&lt;p&gt;In a Human computer system, The Kernel possesses a monopoly on invariant enforcement, and only enforces a static set of invariants; it does so by being an expensive middleman at runtime between any two components that communicate with each other or use any hardware device. In terms of invariant enforcement, this is simultaneously extremely unexpressive and quite expensive.&lt;/p&gt;

&lt;p&gt;Houyhnhnm computing systems, on the other hand, have a decentralized model of invariant enforcement. Every user specifies his invariants by picking a high-level language as the base for his semantics, and using this language to define further computing elements and their invariants. Most invariants can be enforced statically by the high-level language&amp;rsquo;s compiler, and necessitate no runtime enforcement whatsoever, eschewing the cost of a kernel. When multiple components need to communicate with each other, the linker can similarly check and enforce most invariants, and eschew any runtime enforcement cost.&lt;/p&gt;

&lt;p&gt;Well-behaved programming language implementations can therefore manipulate low-level buffers directly without any copying, when producing video or sound; the result is real-time performance without expensive runtime tricks — or rather, performance precisely by the absence of expensive runtime tricks. When the user requests causes a change in the circuit diagram, the code may have to be unlinked and relinked: thus, relinking will happen when users add or remove a filter between the sound producers and the actual audio output, or similarly introduce some window between graphic drawers and the actual video output. But this relinking can happen without any interruption in the music, with an atomic code switch at a time the buffers are in a stable state.&lt;/p&gt;

&lt;p&gt;Certainly, any available hardware support to optimize or secure virtualization can and will be used, wherever it makes sense. But it isn&amp;rsquo;t the exclusive domain of a One Kernel enforcing one static set of invariants. Rather, it is part of the panoply of code generation strategies available to compilers targetting the given hardware platform. These techniques will be used by compilers when they are advantageous; they will also be used to segregate computing systems that do not mutually trust each other. But what matters most, they are not foundational system abstractions; the computing interactions desired by the users are the foundational system abstractions, and all the rest is implementation details.&lt;/p&gt;

&lt;h3 id="bootstrapping"&gt;Bootstrapping&lt;/h3&gt;

&lt;p&gt;The last thing (or, depending on how you look at it, first thing) that a Kernel does in a Human Computer System is to &lt;em&gt;bootstrap&lt;/em&gt; the computer: The Kernel will initialize the computer, detect the hardware resources available, activate the correct drivers, and somehow publish the abstracted resources. The Kernel will take the system from whatever state the hardware has it when it powers up to some state usable by the user programs at a suitable level of abstraction.&lt;/p&gt;

&lt;p&gt;As always, between the firmware, the boot loader, The Kernel, the initialization service manager, the applications that matter, plus various layers of virtualization, etc., the task of initializing the system is already much less centralized even in Human computer systems than the Human &amp;ldquo;ideal&amp;rdquo; would have it. Houyhnhnms just do away with this not-so-ideal ideal. They consider that what matters is the state in which the system is ready to engage in whichever actual interactions the user is interested in; anything else is either an intermediate step, or is noise and a waste of resources — either way, nothing worth &amp;ldquo;blessing&amp;rdquo; as &amp;ldquo;the&amp;rdquo; &amp;ldquo;base&amp;rdquo; system. Instead, automatic snapshotting means that the time to restart a Houyhnhnm system is never more than the time to page in the state of the working memory from disk; only the first run after an installation or update can take more time than that.&lt;/p&gt;

&lt;p&gt;As for the initial hardware resources, just like any resources visible in a system, they are modeled using linear logic, ensuring they have at all times a well-defined owner; and the owner is usually some virtual device broker and multiplexer that will dynamically and safely link, unlink and relink the device to its current users; Conversely, the users will be linked to a new device if there is a change, e.g. because hardware was plugged in or out, or because the system image was frozen on one hardware platform and thawed on a different one. With the ability to unlink and relink, Houyhnhnm computing systems can thus restart or reconfigure any subsystem while the rest of the system is running, all the while &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;persisting any state worth persisting&lt;/a&gt;. This is quite unlike Human computer systems, that require you to reboot the entire system any time a component is stuck, at which point you lose the state of all running programs.&lt;/p&gt;

&lt;h3 id="polycentric-order"&gt;Polycentric Order&lt;/h3&gt;

&lt;p&gt;In the end, it appeared that once again, the difference of approach between Humans and Houyhnhnms led to very different architectures, organized around mutually incommensurable notions. Humans think in terms of fixed artifacts; Houyhnhnms think in terms of evolving computing processes. My questions about some hypothetical fixed piece of software in their computing architecture were answered with questions about some hypothetical well-defined patterns of interactions in our computer architecture.&lt;/p&gt;

&lt;p&gt;Houyhnhnm computing systems do not possess a one single Kernel; instead they possess as many &amp;ldquo;kernels&amp;rdquo; as there are computing subsystems and subsubsystems, each written in as high-level a language as makes sense for its purpose; and the set of those &amp;ldquo;kernels&amp;rdquo; continually changes as new processes are started, modified or stopped. Resource management is decentralized using linear logic and meta-level brokers, linking, unlinking and relinking. Invariant enforcement, though it may involve runtime checks, including hardware-assisted ones, is driven primarily by compile-time and link-time processes. Overriding invariants, while possible, requires special privileges and will be logged; unlike with Human computer systems, processes can&amp;rsquo;t casually interfere with each other &amp;ldquo;just&amp;rdquo; because they run with the same coarse &amp;ldquo;user&amp;rdquo; privilege.&lt;/p&gt;

&lt;p&gt;Humans try to identify an artifact to buy or sell; Houyhnhnms look for processes to partake in. Humans have static understanding of relations between artifacts; Houyhnhnms have a dynamic understanding of interactions between processes. Humans use metaphors of centralized control; Houyhnhnms use metaphors of decentralized ownership. Humans think of enforcement as done by a superior third-party; Houyhnhnms think of enforcement as achieved through mutually agreeable contracts between equally free parties. Humans see all resources as ultimately owned by the Central entity and delegated to users; Houyhnhnms see resources as being used, shared or exchanged by independent processes. I could see a lot of ways that the paradigm of Human computer systems fit in a wider trend of patterns in which to conceive of social and political interactions. Yet, I resisted the temptation of asking Ngnghm about the social and political context in which Houyhnhnm computing systems were being designed; at least for now, I was too deeply interested in figuring out the ins and outs of Houyhnhnm computing to be bothered by a digression into these far ranging matters. However, I did take stock that there was a lot of context that led towards the architecture of Human computer systems; and I saw that this context and its metaphors didn&amp;rsquo;t apply to Houyhnhnm computing, and that I needed to escape from them if I wanted to better understand it.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Chapter 5: Non-Stop Change</title>
  <link rel="alternate" href="http://ngnghm.github.io/blog/2015/09/08/chapter-5-non-stop-change/?utm_source=all&amp;utm_medium=Atom" />
  <id>urn:http-ngnghm-github-io:-blog-2015-09-08-chapter-5-non-stop-change</id>
  <published>2015-09-09T03:54:23Z</published>
  <updated>2015-09-09T03:54:23Z</updated>
  <author>
   <name>Ngnghm</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;a href="/blog/2015/08/02/chapter-1-the-way-houyhnhnms-compute/"&gt;Ngnghm&lt;/a&gt; decided that while stranded among us Humans, he would conduct an ethnographical study of Human computer systems, and took to heart to examining my programming habits. In return, I was more and more curious of how Houyhnhnm systems worked, or failed to work. That&amp;rsquo;s when, trying to imagine what the Houyhnhnm computing systems might keel over, with their making everything persistent, I had this a-ha moment: surely, they must have extreme trouble with live upgrade of their data schema, and their programmers must spend their time in hell trying to reconcile modifications in what amounts to an unrestricted distributed database, that anyone can modify at any time. Ngnghm wasn&amp;rsquo;t sure what I was talking about that could be a major issue, and so interrogated me as to the Human practices with respect to handling change in persistent data — and found that many of the issues stemmed from limitations with how Humans approached the issue, rather than being intrinsic with the problem of ensuring persistence of data, and all but disappeared if you considered code change transactions as first-class objects.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="the-best-laid-schemas-of-houyhnhnms-and-men"&gt;The Best Laid Schemas of Houyhnhnms and Men&lt;/h3&gt;

&lt;p&gt;I challenged Ngngnhm to explain to me how Houyhnhnm systems dealt with code and data upgrade, because they were some of the hardest problems I had faced while working with Human computer systems. Ngnghm wasn&amp;rsquo;t sure what I meant, or why whatever he suspected I meant would be particularly difficult. That surprised me, so I started by stating that any software that survives long enough has to change its code as the software became obsolete, in a phenomenon humorously known as &lt;a href="http://www.jargon.net/jargonfile/b/bitrot.html"&gt;&lt;em&gt;bitrot&lt;/em&gt;&lt;/a&gt;: as time goes by, the software becomes no good anymore, as if it were being degraded by some kind of rot; of course, and that&amp;rsquo;s the joke, no external force can possibly cause the bits that compose the software to change and degrade — on the contrary, the problem is precisely that the software fails to change when the world around does and requires the software to adapt so as to remain relevant. Ngnghm told me that the analogy makes sense to Houyhnhnms but isn&amp;rsquo;t as humorous: it isn&amp;rsquo;t the software, but the &lt;em&gt;relationship of the software with the world&lt;/em&gt;, that rots; the degradation thus applies not to the &lt;em&gt;computer&lt;/em&gt; system, but to the &lt;em&gt;computing&lt;/em&gt; system, as constituted not just by the computer, but also by the sentient programmers and users, and by the larger context in which the computation happens. To a Human focused on the software artifact, it may be a funny paradox; but to a Houyhnhnm focused on the softwaring process, it&amp;rsquo;s a painful truism.&lt;/p&gt;

&lt;p&gt;I was intrigued, but I went on. As a program evolves, the way that it organizes data, its &lt;em&gt;schema&lt;/em&gt;, will eventually be found wanting, as well designed as it might once have been for its original purpose. New &lt;em&gt;data types&lt;/em&gt;, new &lt;em&gt;file formats&lt;/em&gt;, new &lt;em&gt;database schemas&lt;/em&gt;, will be needed, as the old one becomes insufficient and inadapted to newly required kinds of computations. Now, the correspondances between old and new data is often non-trivial, and upgrading data from the old format to the new format is hard; what more, upgrading &lt;em&gt;all&lt;/em&gt; the data, consistently, without any race condition between old and new data processors, was even harder. And if the upgrade has to happen without any down time from the system as it keeps processing data, then achieving it without a hitch becomes a feat of virtuosity. Finally, bugs in such an upgrade process are both all too easy to introduce and all too hard to recover from. Therefore, Human computer systems shun these &lt;em&gt;schema upgrades&lt;/em&gt; as much as possible, and some companies pay people full time just to manage the writing and/or smooth running of upgrade scripts that keep their data up to date, and to maintain the expensive database servers capable of doing it all dependably. However, humans are lucky enough to be able to avoid these difficult data upgrades altogether in most cases, precisely because Humans were not persisting data but throwing it away, so it didn&amp;rsquo;t need to be upgraded. When they really have to deal with the hard case, the best tools available to regular Humans, like &lt;a href="https://github.com/google/protobuf"&gt;protocol buffers&lt;/a&gt; or its successor &lt;a href="https://capnproto.org/"&gt;Cap&amp;rsquo;n&amp;rsquo;Proto&lt;/a&gt;, are not very expressive in terms of types and leave a whole lot to the programmers in terms of managing upgrades; but at least they provide a framework for data to survive and be upgraded beyond the short life of a program that by necessity can only handle one version of the schema. And yet even these tools are only so good for their cost, and most people just do upgrades the hard way: either with completely disjoint old and new schemas, duplicated code, and big ad hoc functions to translate between them; or with ad hoc sharing of code and data between the schemas, a complexity that increases with each new version, and plenty of corner cases that are never considered, much less tested.&lt;/p&gt;

&lt;p&gt;Surely, by persisting everything all the time, Houyhnhnm computing systems were forced to deal with this hard issue all the time, which of necessity must have made all programming difficult and tedious. Yet Ngnghm claimed that he didn&amp;rsquo;t know data upgrade to be particularly tedious on Houyhnhnm systems.&lt;/p&gt;

&lt;h3 id="afterthought-or-forethought"&gt;Afterthought or Forethought&lt;/h3&gt;

&lt;p&gt;First, he said, the case that is easy in Human computer systems, namely eschewing any upgrade and dropping the old data is just as easy in Houyhnhnm computing systems: just don&amp;rsquo;t upgrade the data, and drop it, as you modify the code. If you don&amp;rsquo;t care about data, then by definition you don&amp;rsquo;t care whatever automation the system provides to upgrade it for you, and don&amp;rsquo;t care to fix it; if you know that for sure in advance, you can even tell the system, so it won&amp;rsquo;t bother saving the data, and you might get a nice speed up for it. However, in the cases the data actually matters and you do care about it, that&amp;rsquo;s when you&amp;rsquo;ll miss system support for data persistence, and that&amp;rsquo;s also when you&amp;rsquo;ll miss system support for upgrading your data.&lt;/p&gt;

&lt;p&gt;Moreover, even in this easy case, since Houyhnhnms, &lt;a href="/blog/2015/08/09/chapter-3-the-houyhnhnm-version-of-salvation/"&gt;as you may remember&lt;/a&gt;, save code and data together, you can simply fork the old data together with the correct version of the code that knows how to create and manipulate it, and keep using it while you work on a new version. &amp;ldquo;Orphaned data&amp;rdquo;, &amp;ldquo;version mismatch&amp;rdquo;, &amp;ldquo;DLL hell&amp;rdquo;, are issues that might sometimes delay upgrade, and cause a lot of grief to Houyhnhnm programmers indeed, but they never can prevent reusing current or old data — computation itself is immortal, even if its relationship to the world can suffer. And Houyhnhnms just don&amp;rsquo;t know the catastrophic failure modes of Human computer systems, such as a system becoming unusable due to failure in the middle of an upgrade, what more with missing or incomplete backups so you cannot downgrade, cannot upgrade, and must reinstall and reconstitute all local configuration from scratch.&lt;/p&gt;

&lt;p&gt;Second, since the &amp;ldquo;canonical&amp;rdquo; representation of data is not low-level bytes, but high-level data types, a whole lot of the extrinsic complexity that Human computer systems have to deal with can instead be automated away: When the schema change is merely a change in low-level representation with identical high-level data types, then a trivial strategy is always available for upgrade — decompile and recompile. Moreover, the system can automatically track which object currently uses which underlying representation; it can therefore manage the upgrade without sentient oversight. The sentient programmers can then focus on the actual intrinsic issues of schema upgrade: non-trivial data transformations into semantically non-equivalent data types; and even then, the system provides a framework that automates a lot.&lt;/p&gt;

&lt;p&gt;Third, and more importantly, upgrade is inevitable indeed; but the problem with Human computer systems is that since they both focus on software as a finished artifact and define things at a low-level, they drop all the data that matters about upgrade when it&amp;rsquo;s readily available, only to desperately try to reconstitute it the hard way after it&amp;rsquo;s too late. Upgrade automation is thus almost inexistent in Human computer systems, because it comes as an afterthought. By contrast, it comes naturally in Houyhnhnm computing systems, because it is part and parcel of how they conceive software development.&lt;/p&gt;

&lt;h3 id="change-comes-from-the-inside"&gt;Change Comes from the Inside&lt;/h3&gt;

&lt;p&gt;To Humans, change happens &lt;em&gt;outside&lt;/em&gt; the computer system that is changed: A program is an immutable object, especially so once invoked to start a process (limited virtualized computer system). To change a program thus requires shutting down the processes started with the old program and starting new processes with the new program, although doing it naively can cause an interruption of service.&lt;/p&gt;

&lt;p&gt;As for changing a program&amp;rsquo;s data types and having to preserve data, that&amp;rsquo;s an exceptional situation to be dealt with using exceptional tools; such a catastrophic event happens every so many months or years, when releasing a new &amp;ldquo;major&amp;rdquo; version of the program.&lt;/p&gt;

&lt;p&gt;To Houyhnhnms, change happens &lt;em&gt;inside&lt;/em&gt; the computing system that changes, because everything relevant is &lt;em&gt;ipso facto&lt;/em&gt; inside the system, part of its &lt;em&gt;ontology&lt;/em&gt;. What more, to Houyhnhnms, change to programs is what programming verily is: To program is to change programs, and to change programs is to program. That&amp;rsquo;s a tautological identity. Inasmuch as types are part and parcel of a program, then changing a program&amp;rsquo;s data types is part and parcel of programming and is supported by the system as a matter of course, including upgrading any existing data to use the modified types.&lt;/p&gt;

&lt;p&gt;To a Houyhnhnm, the idea that change could happen &lt;em&gt;outside&lt;/em&gt; the system is absurd on its face, and the notion of a programming language that would only be concerned with describing programs that never change is obviously lacking. Of course, change processes may or may not be automated — but as per the &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;&lt;em&gt;Sacred Motto&lt;/em&gt; of the Guild of Houyhnhnm Programmers&lt;/a&gt;, whatever parts of them &lt;em&gt;can&lt;/em&gt; be automated, &lt;em&gt;should&lt;/em&gt; be automated, &lt;em&gt;must&lt;/em&gt; be automated, &lt;em&gt;will&lt;/em&gt; be automated, until eventually they &lt;em&gt;are&lt;/em&gt; automated.&lt;/p&gt;

&lt;p&gt;Once again, Humans focus on &lt;em&gt;programs&lt;/em&gt; as finished entities with fixed semantics, whereas Houyhnhnms think in terms of &lt;em&gt;systems&lt;/em&gt; made of ongoing interactions. And once again, this &lt;a href="http://www.dreamsongs.com/Files/Incommensurability.pdf"&gt;fundamental difference in paradigm&lt;/a&gt; implies fundamental differences in the most basic structures of computing systems, including programming languages.&lt;/p&gt;

&lt;h3 id="human-languages-that-support-live-upgrade"&gt;Human Languages that Support Live Upgrade&lt;/h3&gt;

&lt;p&gt;However, there are two notable exceptions amongst Human programming languages, whereby the language does — to some degree — support &lt;em&gt;live upgrade&lt;/em&gt;, that is, changes to programs and data types in an actively running program, from &lt;em&gt;within&lt;/em&gt; the language: Common Lisp, and Erlang.&lt;/p&gt;

&lt;p&gt;Common Lisp lets you redefine functions, classes, types, etc., in an existing, running &lt;em&gt;image&lt;/em&gt;, and immediately thereafter use the modified program. There are however a lot of limitations and a lot of pain in doing so, especially if the program has to be actively running while the modifications take place. For instance, you may have to declare your functions &lt;code&gt;notinline&lt;/code&gt; to ensure the latest version is always used, or to &lt;code&gt;shadow&lt;/code&gt; their symbol to ensure the version at time of definition is always used. Before you redefine a class, you can define methods on &lt;a href="http://malisper.me/2015/07/22/debugging-lisp-part-3-redefining-classes/"&gt;&lt;code&gt;update-instance-for-redefined-class&lt;/code&gt;&lt;/a&gt; to ensure that all data will be properly upgraded after the class redefinition; this is somewhat low-level, there is no automated enforcement of consistency between such methods and the types, and you better keep supporting older versions of the class, because you can&amp;rsquo;t be sure that some older instances still haven&amp;rsquo;t been upgraded; but it&amp;rsquo;s there and it works. There are many limitations however to this hot upgrade support; notably, and depending on which Common Lisp implementation you use, there are many kinds of code modifications that will not be safe to issue if multiple threads are active at the same time (particularly changes to the class hierarchy); so you will have to somehow synchronize those threads before you upgrade your code. The big problem with Common Lisp is that its model of side-effects to a single global world doesn&amp;rsquo;t play well with a modern concurrent and distributed world, where a complete system is made of many processes running on many machines, or where you want to have multiple versions of the software running at the same time. Lisp hasn&amp;rsquo;t been actively developed as a self-contained &lt;em&gt;system&lt;/em&gt; for three decades, and what remnant support it has for thinking in terms of system is a partial subset of whatever antiquated solutions existed when it was standardized. Yet that it supports at all the upgrade of code and data without resorting to magic from outside the language makes it miles ahead of all known Human languages. Except Erlang.&lt;/p&gt;

&lt;p&gt;Erlang&amp;rsquo;s heritage and ambitions are very different from those of Lisp, and in many ways it is much more primitive — but as an upside its primitives are better defined. Through a reinvention decades later, Erlang embodies the Actor model of the early 1970s, which is what the &amp;ldquo;Object Oriented&amp;rdquo; utterly failed to be despite its own hype: a programming model where many well-encapsulated entities, called &amp;ldquo;processes&amp;rdquo; in Erlang rather than &amp;ldquo;objects&amp;rdquo; because they are &lt;em&gt;active&lt;/em&gt;, communicate with each other by passing &lt;em&gt;messages&lt;/em&gt;. Processes can be distributed, and reliability is achieved by assuming that individual processes will fail eventually, letting them fail, and restarting them, whereas important data will have been stored and replicated in several other processes that won&amp;rsquo;t all fail at the same time. Now, one thing where Erlang shines, far better than Lisp and far far better than anything else, is in its support of live upgrade of code and data — a topic that it alone seems to take seriously. When you upgrade code, it is always clearly defined which function calls will go to the old version of the code, and which function calls will go to the new version. And the strict evaluation model in terms of processes and messages makes it possible to reason about what state each process is in, how it will update its state, and and how it will handle old or new messages. All in all, Erlang feels like a low-level language, but not a Human low-level language, that tries to track the underlying computer hardware for efficiency, and more like a Houyhnhnm low-level language, that tries to track the underlying computing model for correct and reliable behavior; what makes it less than a high-level language is that it isn&amp;rsquo;t easy enough to build higher abstractions on top of the provided abstraction level — though it may still be better at it than many alleged &amp;ldquo;high-level&amp;rdquo; Human programming languages.&lt;/p&gt;

&lt;p&gt;Therefore, there are some precedents in Human computing systems that allude to what Houyhnhnm computing systems would be. But they are quite undeveloped compared to what would be required of a full-fledged Houyhnhnm computing system.&lt;/p&gt;

&lt;h3 id="automating-live-upgrade"&gt;Automating Live Upgrade&lt;/h3&gt;

&lt;p&gt;So what do Houyhnhnm programming languages look like when it comes to supporting live upgrade of code and data in a running program?&lt;/p&gt;

&lt;p&gt;First, Houyhnhnm programming languages have a notion of transactions for code and data upgrade, that can be &lt;em&gt;composed&lt;/em&gt; and applied &lt;em&gt;atomically&lt;/em&gt;, so that you can &lt;em&gt;coherently&lt;/em&gt; upgrade a system from one version to another, without having to wonder what happens if some code is running right in between when two mutually dependent entities are being redefined. Of course, while developing, Houyhnhnm programmers will want to experiment and explore with upgrades of some part of the code or data only, that lack overall system coherence; such exploratory modifications are doomed to fail, but that&amp;rsquo;s alright, because until they are properly vetted, all changes are always run each in its own virtual fork of the system, where failure is quite acceptable.&lt;/p&gt;

&lt;p&gt;Houyhnhnm type systems track how incoherent the system is, and maintain for the programmer a to-do list for those parts of the system that haven&amp;rsquo;t been updated to follow the new types yet. At runtime, incoherent calls are intercepted before they may break the system, and the debugger offers the programmer a chance to give a new function definition before to enter (or re-enter) the function that fails — among Human programming languages, good Lisp implementations provide the latter (though they can&amp;rsquo;t undo the side-effects of bad functions), whereas good statically typed languages provide a bit of the former (though they don&amp;rsquo;t maintain to-do lists and thus gloss over functions that fail to break the type system e.g. because they have an &lt;code&gt;otherwise&lt;/code&gt; clause that implicitly covers a newly added case when a correct code update ought to explicitly handle the new case, of at least requires the programmer to vet that the &lt;code&gt;otherwise&lt;/code&gt; clause indeed applies to the new case).&lt;/p&gt;

&lt;p&gt;Houyhnhnm systems, since they remember the history of type modifications, require every type modification to be accompanied by a well-typed upgrade function, taking an object in the old type and returning an object in the new type. Simple upgrade functions can be trivially expressed, such as using default values for new fields, or erroring out. The system also uses &lt;a href="https://en.wikipedia.org/wiki/Linear_logic"&gt;linear logic&lt;/a&gt; to ensure that when writing an upgrade operator, you must &lt;em&gt;explicitly&lt;/em&gt; drop any data that you don&amp;rsquo;t care about anymore, so you can&amp;rsquo;t lose information by mistake or omission (old versions of the data will persist by default, as with all data in a Houyhnhnm system, but upgrade mistakes will still make further fixes harder). Because the same function doesn&amp;rsquo;t necessarily apply to all data of a given type, upgrade functions can be overridden at the granularity of individual variables, and they may take some contextual information as extra parameters.&lt;/p&gt;

&lt;p&gt;The system also tracks the difference between some variable having been renamed and some variable having been deleted and another added, because they imply very different upgrade behavior. Branching and merging are similarly supported. These are very easy to track in an interactive development environment, but hard to reconstitute, indeed impossible to reliably reconstitute, just by looking at the code before and after the modifications, when the language doesn&amp;rsquo;t allow to explicitly express the difference.&lt;/p&gt;

&lt;p&gt;Houyhnhnm systems allow you to edit the &lt;em&gt;effective history&lt;/em&gt; that applies to software releases; this history is made of coherent changes that describe how to upgrade from previous released versions to the latest one while preserving all system invariants. It can be contrasted with the actual history through which the result was attained, which is often made of incoherent changes, of backing from blind alleys and of downright mistakes. By upgrading data along the effective history rather than actual history, data corruption can be avoided, and bugs introduced in the upgrade process can themselves be fixed. Now, since Houyhnhnm computing systems accommodate for situations with many distributed agents each with their own version of the code and data and their history, the modifying, branching and merging of objects also apply to these histories of code and data changes. Histories themselves constitute a monotonic algebra where it is always possible to merge histories; unhappily, merge conflicts may cause the result to be incoherent according to the criteria that the users care about; however the system can also check the coherence of the results and reject incoherent transactions, and/or require manual intervention by sentient programmers to fix whatever incoherence it tracked. Of course, this can lead to arbitrarily complex situations when merging histories in a distributed system; but in practice, there is a finite amount of data that needs to be upgraded that people actually care about, so there is a cap on how much complexity Houyhnhnm programmers need to deal with; moreover, since Houyhnhnm computing systems, unlike Human computer systems, do track where all that data is, Houyhnhnm programmers can be fairly confident that they did (or didn&amp;rsquo;t) complete the upgrade of all relevant data, at which point no further upgrade effort is required.&lt;/p&gt;

&lt;p&gt;Now, none of this negates the interest of having a subset of the programming language that allows to express immutable programs and reason about them. Immutability is indeed a very important property that Houyhnhnm programmers can appreciate as well as Human functional programmers. But when programs do mutate, and they do, by the very virtue of programming itself, Houyhnhnm programmers much prefer being able to reason about these changes from within the system, with all the automation that the system makes possible, including automated refactoring, detection and tracking of incoherence, formal reasoning, etc., where instead Humans have to revert to completely informal reasoning on pen and paper, and must indulge in expensive manual drudgery which robs them from being able to focus on the difficult parts that really require their attention, drowning the important details in an ocean of boring repetitive tasks. Houyhnhnm programming languages thus allow to express immutable programs written in suitably restricted programming languages that make it easy to reason about them; and simultaneously they can express meta-level programs without these expressive restrictions (but with different restrictions) that can describe changes in the former programs, from outside their universe but still within the automated part of the overall computing system, also allowing for reasoning.&lt;/p&gt;

&lt;p&gt;In conclusion, as compared to Human computer systems, Houyhnhnm computing systems handle all the easy cases of live upgrade, and automatically ensure coherence for the hard cases, where Humans instead rely on large bureaucracies of &lt;em&gt;database administrators&lt;/em&gt; and upgrade experts to manually manage live upgrade of code and data through excruciating slow and still error-prone unautomated processes. And this, it bears repeating, is but a necessary consequence of the simple difference of &lt;em&gt;point of view&lt;/em&gt;, of &amp;ldquo;philosophy&amp;rdquo;, between Human &lt;em&gt;computer&lt;/em&gt; systems and Houyhnhnm &lt;em&gt;computing&lt;/em&gt; systems.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Chapter 4: Turtling down the Tower of Babel</title>
  <link rel="alternate" href="http://ngnghm.github.io/blog/2015/08/24/chapter-4-turtling-down-the-tower-of-babel/?utm_source=all&amp;utm_medium=Atom" />
  <id>urn:http-ngnghm-github-io:-blog-2015-08-24-chapter-4-turtling-down-the-tower-of-babel</id>
  <published>2015-08-24T23:51:01Z</published>
  <updated>2015-08-24T23:51:01Z</updated>
  <author>
   <name>Ngnghm</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Ngnghm examined how manual persistence was managed underneath Human computer systems, and contrasted with how Houyhnhnms automated its implementation. This led him to more general remarks about the compared architectures of Human computer systems and Houyhnhnm computing systems: Houyhnhnm computing systems can and do go meta, which is notionally &lt;em&gt;down&lt;/em&gt;, which allows them to enjoy qualities not found in Human computer systems that can&amp;rsquo;t go meta.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="stacked-up-against-quality"&gt;Stacked up against Quality&lt;/h3&gt;

&lt;p&gt;Ngnghm wanted to know how Humans dealt with &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;manual persistence&lt;/a&gt;. He found that we were using an large quantity of mutually incompatible and often fragile &amp;ldquo;libraries&amp;rdquo; in each of many loose categories that each implement some aspect of persistence: &amp;ldquo;I/O&amp;rdquo;, &amp;ldquo;file formats&amp;rdquo;, &amp;ldquo;serialization&amp;rdquo;, &amp;ldquo;marshalling&amp;rdquo;, &amp;ldquo;markup languages&amp;rdquo;, &amp;ldquo;XML schemas&amp;rdquo;, &amp;ldquo;protocols&amp;rdquo;, &amp;ldquo;interchange formats&amp;rdquo;, &amp;ldquo;memory layout&amp;rdquo;, &amp;ldquo;database schema&amp;rdquo;, &amp;ldquo;database servers&amp;rdquo;, &amp;ldquo;query languages&amp;rdquo;, &amp;ldquo;object relational mapping&amp;rdquo;, &amp;ldquo;object request brokers&amp;rdquo;, &amp;ldquo;foreign function interface&amp;rdquo;, and many &amp;ldquo;wrappers&amp;rdquo;, &amp;ldquo;adapters&amp;rdquo; and &amp;ldquo;glue layers&amp;rdquo; to make them work together. Indeed, some old IBM study had estimated that 30% of all application code written was related to the basic functions of saving data and restoring it — and at least my experience suggests that this estimate might still be valid to this day. Houyhnhnms, like Dijkstra, regard this as a huge cost: &lt;a href="https://www.cs.utexas.edu/~EWD/transcriptions/EWD10xx/EWD1036.html"&gt;if we wish to count lines of code, we should not regard them as &amp;ldquo;lines produced&amp;rdquo; but as &amp;ldquo;lines spent&amp;rdquo;: the current conventional wisdom is so foolish as to book that count on the wrong side of the ledger.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Unhappily, that huge cost also comes with limited benefits, because a program can only manipulate an object if it gets the entire large tower of libraries, the &amp;ldquo;software stack&amp;rdquo;, just right, and thus two objects built on top of incompatible &amp;ldquo;software stacks&amp;rdquo; cannot interoperate. Costly adapters can be written to bridge between the two towers, but this not only requires extra copying and management by programmers, this also loses any atomicity properties of transactions between the two object systems — and isn&amp;rsquo;t accessible to casual users, who thus pain to manage their data.&lt;/p&gt;

&lt;p&gt;Moreover, the above estimate did not include the error handling strategies when the above failed; meanwhile, the complexity of these baroque towers incur enormous security risks. Indeed, a lot of &amp;ldquo;layers&amp;rdquo; in these software &amp;ldquo;stacks&amp;rdquo; are written in unsafe low-level languages for reasons of alleged &amp;ldquo;performance&amp;rdquo; or &amp;ldquo;compatibility&amp;rdquo;, whereas another (overlapping) lot of such &amp;ldquo;layers&amp;rdquo; include some complex &lt;a href="https://www.usenix.org/system/files/login/articles/login_aug15_02_bratus.pdf"&gt;manual parsing&lt;/a&gt; of data going through the layer, that are as many points where attackers may inject unwanted behavior; these many layers further interact in ways that make it nearly impossible to assess the overall semantics of the system, much less its security properties. As for performance, a lot of it is wasted just crossing the layers at runtime, rather than e.g. folding them at compile-time.&lt;/p&gt;

&lt;p&gt;This architecture in software towers is thus detrimental not only to persistence, but also to robustness, to security, to performance, to upgradability, to maintainability, etc., — all the qualities that managers of Human computer development projects often demote as being &amp;ldquo;non-functional&amp;rdquo;, because their development processes are so deeply dysfunctional, at least from the Houyhnhnm point of view: by neglecting as an afterthought aspects of software development that are not directly visible through a quick test of a software artifact, these processes ensure that those aspects cannot be addressed properly. By contrast, Houyhnhnm computing systems consider as primary the processes of software development and use, not the artifacts, and thus consider these aspects as primary properties of the overall system, that are important to address as part of the architecture of the softwaring process.&lt;/p&gt;

&lt;h3 id="meta-level-strategies"&gt;Meta-level Strategies&lt;/h3&gt;

&lt;p&gt;Houyhnhnms do not have any library to manage persistence; instead, Houyhnhnms have a number of libraries to manage transience. Indeed, persistence is a system-wide protocol, universally provided using generic strategies, and comes for free to users and programmers alike; they don&amp;rsquo;t have to manually flush main memory buffers to mass storage any more than they have to manually flush memory cache lines to main memory buffers, or to manually spill processor registers to memory cache lines. But if they care about extra performance, they can manage these things indeed, and escape or improve the system-provided strategies. In other words, correctness, safety, etc., come for free, and it takes extra effort for a variable &lt;em&gt;not&lt;/em&gt; to be saved, for infinite undo &lt;em&gt;not&lt;/em&gt; to be available, etc., — and for extra performance to be squeezed out of otherwise working programs. I already mentioned in &lt;a href="/blog/2015/08/09/chapter-3-the-houyhnhnm-version-of-salvation/"&gt;the previous chapter&lt;/a&gt; many things that you might want not to persist altogether, or for which to only keep episodic backups. More interesting are the cases where you may want to extend the system to more efficiently support some data type (say, domain-specific compression), some consensus protocol (say, a variant of the PAXOS algorithm), some reconciliation process (say, a new CRDT), or some resource ownership discipline (say, a variant of linear logic). Then you want to specify a new implementation strategy for common system protocols; and for this you usually specify a modular incremental variant of the openly-accessible existing strategies.&lt;/p&gt;

&lt;p&gt;Unlike what you&amp;rsquo;d use in Human computer systems, these strategies are not merely runtime libraries that you link to, the APIs of which programs must explicitly call — this would require every program to be modified any time you change its persistence strategy (or to use very rigid virtual machine, with either a very slow interpreter or a very expensive compiler). Instead, they are meta-level software modifications that customize the implementation of the usual programming languages. Thus, these strategies can arbitrarily instrument the code generated for existing programs, to automatically add any required call to suitable libraries, but also to efficiently handle any associated bookkeeping, depending on what strategies are in the &lt;em&gt;domain&lt;/em&gt; in which the unmodified programs are run. Updated objects may be marked, either individually, in &amp;ldquo;cards&amp;rdquo; or in &amp;ldquo;pages&amp;rdquo; for the sake garbage collection or persistence; counts or sets of local or remote references may be maintained; drawing pictures may be achieved either by blitting directly to video memory or by issuing requests to some server; some type system may be enforced through some combination of static inference and dynamic checks; etc. Of course, these implementation strategies may reject requests to create or move a process into a domain where some incompatibility exists: the program might not pass some static type checks, or possess appropriate permissions, or sufficient resources, etc. Then the user or programmer may have to modify his program or try a different strategy.&lt;/p&gt;

&lt;p&gt;Importantly, this variety of strategies is made possible because Houyhnhnm computing systems are first-class entities abstracted from any specific implementation strategy. Therefore, a very same process (which includes not only source program, but also running state) may be run with different strategies — and indeed with strategies that vary during its execution. When you write a program, the source language you choose completely specifies allowed behavior, and all strategies are guaranteed to preserve this behavior, no more, no less.&lt;/p&gt;

&lt;p&gt;Of course, either at the time you start the program or later, you may decide to constrain the process to only use a given subset of strategies: this actually means that you really wanted a more specific program in a more specific language than initially declared. Not only is that fine, that&amp;rsquo;s a common and recommended way of writing programs: always specify the program&amp;rsquo;s behavior at as high-level as you can, to make it easier to reason about it; yet make sure the optimization strategies you require have been applied, so the performance profile isn&amp;rsquo;t surprisingly bad. As a trivial example, the Fibonacci function would be specified with its usual equational definition, but would typically be annotated with a compile-time assertion that the linear recursion recognizer has kicked in, at which point the system guarantees that the function will be computed in constant time for small values, and polylog time for big ones — rather than exponential time, with a naive implementation.&lt;/p&gt;

&lt;p&gt;Formally speaking, if you wrote a program in abstract language &lt;em&gt;A&lt;/em&gt;, and specify a given implementation &lt;em&gt;I&lt;/em&gt; of language &lt;em&gt;A&lt;/em&gt; generating code in concrete language &lt;em&gt;C&lt;/em&gt;, then you actually specified a program in language &lt;em&gt;C&lt;/em&gt;. And as long as you don&amp;rsquo;t proceed to make modifications at the lower level of language &lt;em&gt;C&lt;/em&gt; that invalidate the abstraction to language &lt;em&gt;A&lt;/em&gt;, then you can remove the constraint, go back to the level of program &lt;em&gt;A&lt;/em&gt;, and later choose a different implementation &lt;em&gt;I&amp;rsquo;&lt;/em&gt; targetting language &lt;em&gt;C&amp;rsquo;&lt;/em&gt;. That&amp;rsquo;s how you migrate a process from one domain to another. (This also requires having formalized the notion of an implementation such that you can interrupt and decompile a process, including running state, and not just source code, from its concrete implementation back to the level of abstraction at which the user has chosen to interact with it — but that&amp;rsquo;s a topic for a future chapter.)&lt;/p&gt;

&lt;h3 id="anything-you-can-do-i-can-do-meta"&gt;Anything You Can Do I Can Do Meta&lt;/h3&gt;

&lt;p&gt;In Houyhnhnm computing systems, programs are thus persistent by default (as well as type-safe, and safe in many other ways); yet they can be made faster and smaller by locally dropping to lower levels of abstraction in structured ways that preserve higher level of semantics. This generalizes the remark made by Paul Graham that, on Lisp, as compared to other languages, &amp;ldquo;You can get fast programs, but you have to work for them. In this respect, using Lisp is like living in a rich country instead of a poor one: it may seem unfortunate that one has to work so as to stay thin, but surely this is better than working to stay alive, and being thin as a matter of course.&amp;rdquo; This doesn&amp;rsquo;t mean that the default mode of operation is especially slow or wasteful of memory: given a fixed amount of development resources, accumulating reusable automated strategies as in Houyhnhnm computing systems can achieve more performance than manually implementing strategies in every program like in Human computer systems.&lt;/p&gt;

&lt;p&gt;Indeed, manual implementation of software strategies, known in the Human computer industry as &amp;ldquo;design patterns&amp;rdquo;, is the primary source of bad quality in software: humans are just so much worse than machines (not to mention slower and more expensive) at applying algorithmic strategies — which notably goes against the &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;Sacred Motto of the Guild of Houyhnhnm Programmers&lt;/a&gt;. (Of course, quality is &lt;em&gt;even worse&lt;/em&gt; when the underlying design patterns have not even been recognized and their properties haven&amp;rsquo;t even been semi-formalized between sentients.) Now, errors can be made when writing the meta-program that automates the strategy — but it&amp;rsquo;s much easier to debug one simple general meta-program once than thousands of context-specific manual instances of the pattern that each had to precisely match the pattern in excruciating details; what more, without automation, it&amp;rsquo;s much harder to keep these myriads of instances right as the pattern or its parameters change, and maintenance requires all of them to be modified accordingly. As Rich Hickey quipped, &lt;em&gt;(Design) patterns mean &amp;ldquo;I have run out of language.&amp;rdquo;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Because software strategies ultimately preserve the safe semantics of high-level languages, they involve less code written in unsafe low-level languages, and what low-level code is generated can be automatically and verifiably made to preserve high-level invariants that matter for safety. Entire classes of bugs that commonly plague Human computer systems thus never appear in Houyhnhnm computing systems. Of course, Houyhnhnms make many mistakes while developing their computing systems, and the inconsistent strategies they write can cause inconsistent behavior, with catastrophic consequences. But virtualization ensures that these catastrophes do not escape the narrow scope of the sandbox in which the developer is trying them; and catastrophic effects are actually easier to detect, so that most such bugs are faster to fix. Subtle meta-level bugs causing delayed catastrophes, though they exist, are quite rare. To eliminate them, the usual combination of testing and formal methods can help; there again, generic code is usually harder to test or formalize than a single specific instance of the code, but much easier to test or formalize than thousands or millions of instances, as necessarily happens when strategies are applied manually rather than automatically.&lt;/p&gt;

&lt;p&gt;Finally, because Houyhnhnm computing systems work at the level of abstract data types, most messaging happens with robust system-provided pairs of printers and parsers, rather than an ever renewed collection of &lt;em&gt;ad hoc&lt;/em&gt; manual printers and parsers for manually designed interchange languages, each introducing a renewed layer of bugs. Indeed, in Human computer systems, the humans who &amp;ldquo;design&amp;rdquo; these interchange languages are often unaware that they are designing languages indeed, or in deep denial when confronted to that fact; they thus prefer to remain ignorant of the very basics of language design, and ever repeat all the beginners&amp;rsquo; mistakes. In Houyhnhnm computing systems, it is understood that whatever interactions happen between sentient beings and/or automated processes by definition constitute a language; and while you want the overall design interaction between sentient being and machine to happen at the highest possible level using as expressive a language as possible, the interactions between automated processes should happen using the highest level but least expressive language possible, so they remain easier to analyze.&lt;/p&gt;

&lt;p&gt;Therefore, when contrasted to Human computer systems, it appears that Houyhnhnm computing system thus achieve &lt;em&gt;better&lt;/em&gt; quality through &lt;em&gt;meta&lt;/em&gt; programming.&lt;/p&gt;

&lt;h3 id="building-up-vs-building-down"&gt;Building up vs building down&lt;/h3&gt;

&lt;p&gt;Humans can only build software but &lt;em&gt;up&lt;/em&gt;. Houyhnhnms can build both up &lt;em&gt;and&lt;/em&gt; down.&lt;/p&gt;

&lt;p&gt;All computer software has to start from a given &lt;em&gt;base&lt;/em&gt;: whatever abstractions the operating system provides, or, in absence of operating system, the &amp;ldquo;bare metal&amp;rdquo; — which for Human computer systems is often not quite so bare these days, with plenty of firmware and coprocessors involved. Now, Human computer systems are built by piling layers upon layers on top of this base; and the operating system itself can be already considered such a tower of layers, on top to build higher towers. One limitation of Human computer systems, though, is that to cooperate on the same data structures, programs typically have to reuse the very exact same tower of layers. Because each layer adds a lot of informal underspecified details, and it is impossible to reproduce computations or assume that programs have similar enough semantics unless they are identical from the ground up. With this tower architecture, as with the legendary Tower of Babel, people are divided by a confusing diversity of languages that prevent them from communicating.&lt;/p&gt;

&lt;p&gt;Now, it is actually important to share data between different programs. Human software developers thus onerously build &lt;em&gt;abstractions&lt;/em&gt;, without system support, so that they may save files in one format, which will hopefully be implemented in a compatible enough way by the other program or next version of the program. The operating system itself is such an abstraction, trying to present a uniform view of the computer to programs that run on top of it, despite a wild variety of underlying computers; so are to a point various virtual machines, or programming language specifications. So is, more trivially, the informal promise in successive versions of the &amp;ldquo;same&amp;rdquo; program to keep working with data saved by previous versions. Yet, any given abstraction usually has at most one sensible implementation on any given Human computer system.&lt;/p&gt;

&lt;p&gt;Slightly more advanced Human computer systems, using macros, can at compile time lift the system up and add a number of layers below. For an extreme case, some &lt;a href="http://www.cliki.net/screamer"&gt;Common Lisp&lt;/a&gt; &lt;a href="http://quickdocs.org/hu.dwim.delico/api"&gt;libraries&lt;/a&gt; reimplement Common Lisp in Common Lisp to add first-class multiple-entry or even serializable continuations, so as to enable logic programming or direct-style web programming. Some interactive development systems also instrument the virtual machine so as to lift execution into something that allows for debugging, with Omniscient Debugging as an extreme example. But even then, once the program is built, once the runtime has been chosen, once the program has started running, the system remains forever grounded on top of the chosen basis.&lt;/p&gt;

&lt;p&gt;Houyhnhnm computer systems, by contrast, can dynamically add new layers below a running program: not only can you add a layer on top of any existing tower before you start using it, you can add or replace layers below the tower, or anywhere in the middle of it, while you are using it. This ability to build &lt;em&gt;down&lt;/em&gt; as well as &lt;em&gt;up&lt;/em&gt; crucially relies on processes being specified in formally well-defined high-level languages, so that it is always clear what is the semantics to be preserved when modifying the underlying implementation. Therefore, Houyhnhnms don&amp;rsquo;t even have a fixed notion of ground or base. Rather than rigid towers of stone being built up, they have living worlds that stand on an indefinite number of other living worlds, just like the turtles of the common joke, whereby there are &lt;a href="https://en.wikipedia.org/wiki/Turtles_all_the_way_down"&gt;&lt;em&gt;turtles all the way down&lt;/em&gt;&lt;/a&gt;; then Houyhnhnms can lift the stack of turtles at any desired point and add or replace some of the turtles beneath, while the system is running. Every turtle is unique, but no turtle is special.&lt;/p&gt;

&lt;p&gt;The superficial differences between Houyhnhnm computing systems and Human computer systems are thus the reflection of radical differences between their underlying software architectures — that once again, derive from the initial divergence in &lt;em&gt;point of view&lt;/em&gt;: considering the entire sentient-machine processes, rather than focusing only on the finished machine artifacts.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Chapter 3: The Houyhnhnm Version of Salvation</title>
  <link rel="alternate" href="http://ngnghm.github.io/blog/2015/08/09/chapter-3-the-houyhnhnm-version-of-salvation/?utm_source=all&amp;utm_medium=Atom" />
  <id>urn:http-ngnghm-github-io:-blog-2015-08-09-chapter-3-the-houyhnhnm-version-of-salvation</id>
  <published>2015-08-09T05:10:00Z</published>
  <updated>2015-08-09T05:10:00Z</updated>
  <author>
   <name>Ngnghm</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Following our &lt;a href="/blog/2015/08/03/chapter-2-save-our-souls/"&gt;discussion on persistence&lt;/a&gt;, Ngnghm had plenty of questions about how Human computer systems held together when they can&amp;rsquo;t seem to get basic persistence right. But in return, I had even more questions about what Houyhnhnm computing systems could even be like, when all data persisted by default: What did the user interface look like? Was there no more save button? What happened when you copied or deleted files? Were there files at all? How did people deal with all the garbage? Were your mistakes forever? If you somehow hosed your machine, would it remain forever hosed? How did you test potentially dangerous changes?&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="when-data-persists"&gt;When Data Persists&lt;/h3&gt;

&lt;p&gt;Thus, on a first approach, the interface to a Houyhnhnm computing system may look very similar to that of a Human computer system — and that&amp;rsquo;s how Ngnghm had been fooled at first into thinking they were designed along the same principles. If you have a Human laptop that you usually put to sleep and don&amp;rsquo;t turn off or restart, your setup might be similar to what it would be on a Houyhnhnm muzzletop. But even if your operating system is very stable, you must always be prepared for a catastrophic failure; and whenever you upgrade your software (which you &lt;em&gt;must&lt;/em&gt; do eventually if only to apply security patches), then you &lt;em&gt;must&lt;/em&gt; restart just to make sure the restart procedure will be working if the failure happens while you&amp;rsquo;re disconnected from online help — even though your system is stable and doesn&amp;rsquo;t really need it. And at that point you lose all your current working state. The eventuality of losing all your current working state is thus a Sword of Damocles hanging above any and all working state you weave, even on the most stable of systems. Note that a Houyhnhnm computing system may have to be restarted semi-periodically for the very same reason; the difference being that you don&amp;rsquo;t lose any working state as you do — or else, your system administrator and/or your or his insurance will cover any damages caused. Impermanence is thus a pervasive assumption in all Human computer systems, around which all habits are built; the loss of working state can be mitigated by using &amp;ldquo;session managers&amp;rdquo; that automatically save your session, or &amp;ldquo;startup scripts&amp;rdquo; where you manually re-create your usual work session, but both approaches only save very partial session information, in addition to being quite fragile and unreliable in practice — and intensive in programmer effort to implement. By contrast, in a Houyhnhnm computing system, you never lose your session; moreover, you can extract a startup script to re-create a similar session on another system, by introspecting the state of the system (the support for which is necessarily present for the sake of persistence) or by selectively replaying the relevant parts of the system persistence log.&lt;/p&gt;

&lt;h3 id="system-wide-versioning"&gt;System-wide Versioning&lt;/h3&gt;

&lt;p&gt;In Human computer systems, editors and other applications have a &amp;ldquo;save&amp;rdquo; button. In Houyhnhnm computing systems, there are no applications and no &amp;ldquo;save&amp;rdquo; buttons. Instead, there are components that each deal with the specific aspects of some kinds of documents or data, and otherwise share common system features for general aspects of data — including notions of versioning and releasing, i.e. publishing a stable version so it&amp;rsquo;s visible to other people, whereas intermediate changes and their inglorious details remain unpublished. Thus, instead of &amp;ldquo;text editors&amp;rdquo; and &amp;ldquo;picture editors&amp;rdquo;, there are &amp;ldquo;text editing components&amp;rdquo; and &amp;ldquo;picture-editing components&amp;rdquo;, that are available anywhere that there are modifiable texts or pictures in the system — and pretty much any text or picture you see can be copied into an editable variant, or traced back to its source, which can be forked and edited, if it&amp;rsquo;s not directly editable already. In Human computer systems, programmers have to bundle a finite number of such components into the package-deal that is an &amp;ldquo;application&amp;rdquo;, where you can&amp;rsquo;t use the component you want without being stuck with those you don&amp;rsquo;t want. In Houyhnhnm computing systems, users can individually configure the components or combinations of components they want to use for each type of data they are interested in. They all delegate their versioning aspect to the user&amp;rsquo;s favorite versioning component, that will handle forking new branches of data, and branches off branches of data, merging data from multiple branches, atomically committing changes, releasing data from a subbranch to make its changes available to a wider branch, etc.&lt;/p&gt;

&lt;p&gt;Another advantage of system-wide versioning is that in Houyhnhnm computing systems, infinite undo comes for free on any kind of data, without any special effort from the developer, and without any limitation for the user; what more it is available atomically for all data in the system or any joint subset thereof. By contrast, in Human computer systems, the ability undo of a few steps for a few kinds of documents is a very costly, unreliable and/or error-prone operation requiring a lot of programming and a lot of maintenance, and working on one document at a time; some applications maintain history, but it is optimized for data mining by spies, and useless for users to recover past sessions. One more feature made possible by system-wide versioning is the ability to easily reproduce and isolate bugs — an activity that consumes a lot of expensive programmer time in Human computer systems, and that is made much easier in Houyhnhnm computing systems, since the log of interaction events that led to the erroneous behavior was recorded and can be replayed until the behavior was narrowed down; then the error case can automatically be reduced to its essence, by shaking the tree of actually used dependencies as detected by re-running an instrumented version of the same code to achieve e.g. Omniscient Debugging; the test case is then ready for inclusion in a regression test suite.&lt;/p&gt;

&lt;h3 id="data-at-the-proper-level-of-abstraction"&gt;Data at the Proper Level of Abstraction&lt;/h3&gt;

&lt;p&gt;Because persistence in Human Computer Systems consists in communicating sequences of bytes to external processes and systems (whether disks or clouds of servers), all data they hold is ultimately defined in terms of sequences of bytes, or files; when persisting these files, they are identified by file paths that themselves are short sequences of bytes interpreted as a series of names separated by slashes &lt;code&gt;/&lt;/code&gt; (or on some systems, backslashes &lt;code&gt;\&lt;/code&gt;, or something else). Because persistence in Houyhnhnm Computing Systems applies to any data in whichever high-level language it was defined in, all Houyhnhnm computing data is defined in terms of &lt;a href="https://en.wikipedia.org/wiki/Algebraic_data_type"&gt;Algebraic Data Types&lt;/a&gt;, independently from underlying encoding (which might automatically and atomically change later system-wide). For the sake of importing data in and out of independently evolving systems, as well as for the sake of keeping the data compressed to make the most of limited resources, some low-level encoding in terms of bytes may be defined for some data types. But on the one hand, this is the exception; on the other other, the data is still part of the regular Algebraic Data Type system, can still be used with type constructors (e.g. as part of sum, product or function types), etc. Whereas Human computer systems would require explicit serialization and deserialization of data, and would require ad hoc containers or protocol generators to allow larger objects to contain smaller ones, Houyhnhnm computing systems abstract those details away and generate any required code from type definitions. Low-level encodings can even be replaced by newer and improved ones, and all objects will be transparently upgraded in due time — while preserving all user-visible identities and relationships across such changes in representation.&lt;/p&gt;

&lt;p&gt;Since objects are not defined in terms of sequences of bytes, the very notion of file doesn&amp;rsquo;t apply to Houyhnhnm computing systems. At the same time, accessing an object inside a data structure is often (though not always) conveniently represented as following an access path from the root of the data structure to the desired element. An &amp;ldquo;access path&amp;rdquo; is thus a natural notion in all computing systems &lt;!--
see Clojure: assoc-in, update-in--&gt; even though in general a path is not a sequence of strings separated by slashes, but a list of accessors, that may be symbols or strings (when accessing a dictionary), integers (when accessing a sequence by index), or arbitrary accessor functions. But few are the cases where the natural way to locate data is via a list of sequences of bytes containing neither ASCII slash nor ASCII NUL; or worse, sequences of Unicode code glyphs up to some subtle case-conversion, represented as UTF&amp;ndash;8 code points in some normal form; or even worse, the greatest common denominator between an underspecified set of several variants of the above, with unspecified separators.&lt;/p&gt;

&lt;p&gt;Thus, files are not the general case for persisting data; text files even less so. Still, a good text editor and good text-based diff tools can provide a handy way to view and modify data and view and act on modifications to data. Indeed, unless and until you have better tools to represent change between arbitrary data structures, it makes sense to translate otherwise unsupported data structures to and from a well supported generic data structure such as text. &lt;a href="http://www.cs.yale.edu/homes/perlis-alan/quotes.html"&gt;&lt;em&gt;It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures.&lt;/em&gt;&lt;/a&gt; Now, it is important to understand the distinction between a representation and the real thing; the text being presented is not &amp;ldquo;canonical&amp;rdquo;, it is not usually &amp;ldquo;source&amp;rdquo;. In Houyhnhnm computing systems, the source is the semantic state of the system, on which change happens, and from which the text is extracted if and when needed; this is in sharp contrast with typical Human computer systems, where the source (that is, the locus of modification by sentients) is text files that are compiled, disconnected from the state of the system.&lt;/p&gt;

&lt;h3 id="dealing-with-bad-memories"&gt;Dealing with Bad Memories&lt;/h3&gt;

&lt;p&gt;But, I inquired, if they log everything and almost never forget anything, don&amp;rsquo;t Houyhnhnm computing system quickly get filled with garbage? No, replied Ngnghm. The amount of information that users enter through a keyboard and mouse (or their Houyhnhnm counterparts) is minute compared to the memory of modern computers, yet, starting from a well-determined state of the system, it fully determines the subsequent state of the system. Hence, the persistence log doesn&amp;rsquo;t need to record anything else but these events with their proper timestamp. This however, requires that all sources of non-determinism are either eliminated or recorded — which Houyhnhnm computing systems do by construction. Of course, to save resources, you can also configure some computations so they are not recorded, or so their records aren&amp;rsquo;t kept beyond some number of days. For instance, you might adopt a &lt;a href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller"&gt;&lt;em&gt;model-view-controller&lt;/em&gt;&lt;/a&gt; approach, and consider the view as transient while only logging changes to the controller, or even only to the model; or you might eschew long-term storage of your game sessions; or you might forget the awkward silences and the street noise from your always-on microphone; or you might drop data acquired by your surveillance camera when it didn&amp;rsquo;t catch any robber; or you might delete uninteresting videos; or you might expunge old software installation backups from long gone computers; or you might preserve a complete log only for a day, then an hourly snapshot for a few days, and a daily snapshot for a few weeks, a weekly snapshot for a few months, etc.; or you might obliterate logs and snapshots as fast as you can while still ensuring that the system will be able to withstand partial or total hardware failure of your personal device; or then again, given enough storage, you might decide to keep &lt;em&gt;everything&lt;/em&gt;. It&amp;rsquo;s your choice — as long as you pay for the storage. The decision doesn&amp;rsquo;t have to be made by the programmer, though he may provide hints: the end-user has the last say. Indeed, the interests of the programmer may not be aligned with those of the user; if he needs to delegate these decisions, the user will thus entrust administrators he pays, whose interests are better aligned because they will be liable in case of malpractice.&lt;/p&gt;

&lt;p&gt;Now, beyond clutter that uselessly fills up memory, what about actively bad things that surely you don&amp;rsquo;t want to memorize? Some mistake might cause your entire system to become unresponsive by resource exhaustion (a fork bomb on Unix, an out-of-memory situation on any system); something might trigger a system bug and cause a hardware crash, a Blue Screen Of Death or a kernel panic; even worse, some subtle combination of factors could generate a memory corruption that jeopardizes the integrity of the persistent data. Houyhnhnms computing systems may be more robust than Human computer systems in this regard, yet even Houyhnhnms are not perfect in avoiding catastrophic mistakes. If you detect such a situation, what do you do? Old Houyhnhnm engineers tell classic stories of catastrophic system modifications that were reverted by shutting down the computer before the modification was written to disk; but of course, as the latency of persistence goes down, the window of opportunity for such a feat goes away. The general answer is that to fix a system that has entered a bad state, you need an &lt;em&gt;external&lt;/em&gt; system that can stop the broken system, inspect it, fix it, and restart it.&lt;/p&gt;

&lt;p&gt;On a Human computer system, when things get that bad, you can often reboot in a special &amp;ldquo;failsafe&amp;rdquo; mode (that can usually handle but the simplest of situations), or you can use a USB key with a known stable version of the system (with which experts can handle complex situations), or at worst if the hardware was damaged, you can disconnect the computer&amp;rsquo;s mass memory unit and connect it into another computer. In a Houyhnhnm computing system, you can do as much, but you can also use a reserved input sequence (the equivalent of 
 &lt;kbd&gt;Ctrl-Alt-Del&lt;/kbd&gt; on Windows) to enter a &lt;em&gt;monitor&lt;/em&gt;. The monitor is a &lt;em&gt;simple&lt;/em&gt; but complete computing system, as per the &lt;a href="/blog/2015/08/02/chapter-1-the-way-houyhnhnms-compute/"&gt;Houyhnhnm criteria of simplicity&lt;/a&gt;; it is universal and can do everything a computing system can do, and is often a bare-bones variant of a regular computing system, as used for secure bootstrapping purposes; it also specifically understands enough of the semantics of the regular system to inspect it, fix it, and restart it, using the full power of a complete computing system (though a simple one). A small amount of memory is reserved for the operation of the monitor; actually, if mass memory units are working (as they should be) and have some reserved space for the monitor (as is the case on a default installation), then the monitor will actually spawn a virtualized monitor; this allows monitor operations to have more memory available, so they can for instance merge in a lot of the system state (up to some point deemed safe by the user); but this also makes it possible to still have a monitor (and possibly more virtualized monitors) in case you make mistakes in the virtualized monitor; as a result, it is safe to use dichotomy to determine which change broke the system.&lt;/p&gt;

&lt;h3 id="virtualization-as-branching"&gt;Virtualization as Branching&lt;/h3&gt;

&lt;p&gt;More generally, a Houyhnhnm can use virtualization and system rollback while conducting any kinds of experiments, so he never has to hesitate about doing anything risky, half-baked, downright stupid, or otherwise dangerous. But virtualization doesn&amp;rsquo;t mean the same thing in a Houyhnhnm computing system as in a Human computer system. In a Human computer system, virtualization is an &lt;em&gt;ad hoc&lt;/em&gt; tool for system administrators, that allows the deployment of specially prepared servers; it is implemented using heroic techniques, by faking an entire physical computer at the level of abstraction of CPU instructions and memory accesses; the awkward result requires extra care and makes the users&amp;rsquo; life overall more miserable rather than less, though it is sometimes an economic necessity. In a Houyhnhnm computing system, virtualization is merely the ability to branch the history of changes in the system, and derives naturally from the fact that the entire system is under version control; it is available at whichever level of abstraction the users and programmers specify their computations, which is most usually at a much higher level of abstraction than that of CPU instructions (though a naive, fallback strategy is indeed always available that consists in going down to that low level).&lt;/p&gt;

&lt;p&gt;Thus, all destructive or catastrophic experiments happen in branches that are never merged into the official reality — the errors remain imaginary. Alternatively, when bad things happen in said official reality, they can be &lt;a href="http://www.jargon.net/jargonfile/r/retcon.html"&gt;retcon&lt;/a&gt;&amp;rsquo;ed into having been but a bad dream, as the bad reality is demoted into being a mere unmerged error branch while a nicer reality becomes the master copy. Of course, it may be too late to undo bad communications with external systems, if they were let happen; some of them may be cancelled or compensated with new transactions; some of them may have to be accepted as losses or errors: you can&amp;rsquo;t unprint a pile of garbled characters, and you won&amp;rsquo;t get your bitcoins back. Even Houyhnhnm computing systems can&amp;rsquo;t protect you from yourself; but they can make it easy to try things in a virtualized environment, and to only merge into reality those transactions that pass all checks.&lt;/p&gt;

&lt;p&gt;It is also possible to branch only part of the system while the rest of the system remains shared; and of course you can merge two branches back together, somehow fusing changes. Thus, there is no need to specially prepare an image for virtualization; any system and any subsystem, any program and any subprogram, can be forked at any time; any ongoing I/O with other systems can be redirected to one fork or the other, or multiplexed to/from both, or filtered, etc., and the user can dynamically re-wire all these connections from a monitor outside the virtualized system, that can itself be virtualized, etc.&lt;/p&gt;

&lt;p&gt;Version control should be familiar to developers of Human computer systems; but these days, they apply it only to source code; and so live, interactive data is not covered by the version control, or at best only in a very indirect way, if the programmers make large, contrived and expensive efforts to check in every change. Houyhnhnms think of code and data as coming together, part of the same interaction with the Sentient user, with data and code being useless without the other, or out of synch with the other; and thus Houyhnhnm computing systems casually apply version control to the entire state of the system.&lt;/p&gt;

&lt;p&gt;In the end, thinking like Houyhnhnms in terms of &lt;em&gt;computing&lt;/em&gt; systems, rather than like Humans in terms of &lt;em&gt;computer&lt;/em&gt; systems has far-ranging consequences in terms of software and hardware architecture. Persistence is but one aspect of this architecture, though ultimately, it cannot be separated from the rest. And on this aspect like on others, from the necessity of dealing with the same basic needs and failure scenarios, the change in &lt;em&gt;point of view&lt;/em&gt; leads to very different approaches to making and keeping the systems working.&lt;/p&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Chapter 2: Save Our Souls</title>
  <link rel="alternate" href="http://ngnghm.github.io/blog/2015/08/03/chapter-2-save-our-souls/?utm_source=all&amp;utm_medium=Atom" />
  <id>urn:http-ngnghm-github-io:-blog-2015-08-03-chapter-2-save-our-souls</id>
  <published>2015-08-03T05:10:00Z</published>
  <updated>2015-08-03T05:10:00Z</updated>
  <author>
   <name>Ngnghm</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;&lt;a href="/blog/2015/08/02/chapter-1-the-way-houyhnhnms-compute/"&gt;Ngnghm&lt;/a&gt; was fascinated by our keyboards: because of physiological differences between our races, similar devices had never been imagined by &lt;a href="http://en.wikipedia.org/wiki/Houyhnhnm"&gt;Houyhnhnm&lt;/a&gt; computing engineers. Now, as he was watching me closely, Ngnghm noticed that I was punctuating most of my typing with recurring combinations of key chords. I told him I had no idea what he meant; and so he had me record and review how, after every sentence or so, or before changing activities, I was composing the sequence 
 &lt;kbd&gt;Ctrl-X Ctrl-S&lt;/kbd&gt;, or 
 &lt;kbd&gt;Command-S&lt;/kbd&gt;, or some other mantra that varied slightly with the application I was using. Interestingly, I wasn&amp;rsquo;t even aware that I was doing that before he told me! What was this mantra doing, he inquired? How could I possibly repeat it without even noticing — and why would I? I told him that depending on the application, each of these mantra &lt;em&gt;saved&lt;/em&gt; the current file, and that typing it had become ingrained in me as a subconscious habit, because I used it so often, out of necessity. What does "&lt;em&gt;saved&lt;/em&gt;" mean wondered Ngnghm, and what made it a necessity?&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="persistence-automated"&gt;Persistence, Automated&lt;/h3&gt;

&lt;p&gt;I explained that Human computer software and hardware are prone to crashing, or to losing battery power, and other unexpected failures — there he sighed with sympathy, for Houyhnhnms were just as frustrated as Humans with how unreliable their computers were. I continued that the solution universally adopted for Human computer systems was therefore that Humans had to explicitly &lt;em&gt;save&lt;/em&gt; each file for its contents to be later recoverable in the event of such a crash. Having been burned too many times by the loss of many hours of hard work, I had grown the habit of saving often, and doing it unconsciously at every pause in my thought process; thus I didn&amp;rsquo;t have to think hard to predict when the computer was at risk and explicitly decide when I ought to save. Ngnghm was properly appalled. Didn&amp;rsquo;t the system just automatically save everything I typed? Why was human thought and habit involved at all in a task that could have been fully automated long ago — and indeed had been automated in all but the earliest and most primitive Houyhnhnm computing systems?&lt;/p&gt;

&lt;p&gt;Although, he remarked, considering the overall computing system containing both Sapient and Computer, the task had been automated indeed. Indeed, if you came to think of it, this task couldn&amp;rsquo;t possibly &lt;em&gt;not&lt;/em&gt; be automated, unless the computing system were only used but to produce worthless data never worth keeping — at which point it would thus be itself worthless. However, the task had been imperfectly automated at great cost by creating a habit in my brain and hands, rather than automated both perfectly and cheaply by having it done by the computer. Certainly, building a physical habit that lightened the burden on the higher parts of my mind was better than no automation at all, but what a waste of precious wetware! At least in this instance and for this concern, the very purpose of computers had been defeated. As went the &lt;em&gt;Sacred Motto&lt;/em&gt; of the Guild of Houyhnhnm Programmers: &lt;a href="http://www.wanderings.net/notebook/Main/BitterAcknowledgmentsOfOlinShivers"&gt;&lt;em&gt;I object to doing things that computers can do&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And then, suddenly, Ngnghm became worried about his journal. He had been consigning his observations on a computer ever since he had learned to use a mouse to draw Houyhnhnm symbols in a paint application. (Ngnghm once remarked that a one-button mouse is an exquisite input device for a Houyhnhnm&amp;rsquo;s hoof, but that it takes a Yahoo to believe it is suited to a Yahoo´s hand.) Now, he admitted that he could never retrieve his old notes; but he just thought that it was due to his not understanding the Houyhnhnm-Computer Interface properly, and to his not knowing how to search back in time what he had previously drawn. He assumed that Human computers were probably not geared to properly index his observations for easy retrieval, but that they would otherwise all be logged in the computer&amp;rsquo;s memory. Was I implying that all his notes were lost, including some of the finest poetry he had ever written, as inspired both by the suffering from his predicament and the marvel at his discoveries? I won&amp;rsquo;t claim any proficiency at judging Houyhnhnm poetry — it all sounds like nickering and whickering to me — but to this day, I fear that one of the greatest pieces of Houyhnhnm literature has been lost to the world, due to the failings of Human computer systems.&lt;/p&gt;

&lt;h3 id="orthogonal-persistence"&gt;Orthogonal Persistence&lt;/h3&gt;

&lt;p&gt;Ngnghm explained to me that Houyhnhnm computing systems make data persistence the default, at every level of abstraction. Whether you open the canvas of a graphical interface and start drawing freely, or you open an interactive evaluation prompt and bind a value to a variable, or you make any kind of modification to any document or program, the change you made will remain in the system forever — that is, until Civilization itself crumbles, or you decide to delete it (a tricky operation, more below). Everything you type remains in your automatic planet-wide backups, providing several layers of availability and of latency — kept private using several layers of cryptography.&lt;/p&gt;

&lt;p&gt;Of course, you can control what does or doesn&amp;rsquo;t get backed up where, by defining domains each with its own privacy policy that may differ from the reasonable defaults. The user interface is aware of these domains, and makes it clear at all times which domain you&amp;rsquo;re currently working with. It also prevents you from inadvertently copying data from a more private domain then pasting it into a more public one; in particular, you only type your primary passwords but in a very recognizable special secure domain that never stores them; and your secondary access keys are stored in a special private domain using stronger cryptography than usual, and also subject to various safety rules to avoid leakage.&lt;/p&gt;

&lt;p&gt;Deletion (as opposed to mere de-indexing), while possible, gets more expensive as the data you want to delete gets older: logs, backups and indexes dating back to the deleted change have to be scrubbed and rewritten; the system must triple-check that everything is still in working order after this sweeping change; it must also make sure that the user is ultimately happy with the results, including with whatever might break for other users he knows who might have depended on details of the old history (assuming he shared any of it). Now, when deleting anything but most recent changes, this expensive operation will leave traces that something was deleted, though the details of what was deleted will indeed have been deleted. Of course, deletion doesn&amp;rsquo;t affect copies other people may have of the data, if you ever shared it; therefore, thou shalt not lightly share thy data, and thou shalt never share any access keys — but that&amp;rsquo;s true anyway. At least Houyhnhnm systems let you manage your sharing and backup policies in a systematic way, and ensure that everyone can depend on sensible, safe, defaults.&lt;/p&gt;

&lt;p&gt;In other words, Houyhnhnm computing systems have &lt;a href="http://tunes.org/wiki/orthogonal_20persistence.html"&gt;&lt;em&gt;orthogonal persistence&lt;/em&gt;&lt;/a&gt; — and have had it for &lt;a href="http://tunes.org/wiki/eumel.html"&gt;&lt;em&gt;decades&lt;/em&gt;&lt;/a&gt;. The adjective &amp;ldquo;orthogonal&amp;rdquo; means that the persistence of data is a property of the domain you&amp;rsquo;re working in, as managed by the system; it is &lt;em&gt;not&lt;/em&gt; an aspect of data that programmers have to deal with in most ordinary programs; unless of course they are programmers specifically working on a new abstraction for persistence, which is after all an ordinary program, just in a narrow niche. Regular programmers just manipulate the data with full confidence that the inputs they consume, the code that manipulates them, and the outputs they produce will each remain available as long as the user wants them, with the consistency guarantees specified by the user, as long as the user affords the associated costs.&lt;/p&gt;

&lt;p&gt;Actually, ordinary programs don&amp;rsquo;t know and can&amp;rsquo;t even possibly know which domain they will be running in, and letting them query those details would be a breach of abstraction, with serious security implications and performance impediments, even assuming for a moment that it wouldn&amp;rsquo;t otherwise affect program correctness. Therefore, only programs with adequate capabilities can manipulate the persistence and privacy levels of computing domains, except of course to deliberately spawn a subdomain with yet strictly fewer capabilities. The system of course can recognize privacy and performance annotations about authorized programs and automatically distribute the many components of these programs each in a suitable domain.&lt;/p&gt;

&lt;p&gt;It is important to maintain full abstraction when keeping the semantics of ordinary programs orthogonal to various concrete aspects of the computing domains: the persistence, privacy, robustness and performance (but also machine word size, endianness, memory layout, physical location of the machine, etc.). This abstraction allows the user to independently specify what domain he wants, and to later change his specification, while the program keeps running. The same abstraction allows the underlying system to independently pick the best suited or cheapest concrete implementation, and to migrate the program to a different underlying machine when the conditions change. And whether migration is prompted by user request, system adaptation, or a change of phase in the execution of the program, the concrete code to run the program can automatically be re-generated to fit the new conditions, so the program may continue running in a new domain implementation, without any interruption in its semantics (though possibly with an observable pause). Thus, the system may optimize away logging and copying in transient computations for which speed matters more than robustness; or it may introduce extra logging and extra copying when debugging existing programs (e.g. enabling &lt;a href="http://www.lambdacs.com/debugger/"&gt;Omniscient Debugging&lt;/a&gt; for a failed computation); it may automatically introduce synchronization steps in computations performed in lock-step by several redundant machines based on different architectures to ensure detection and elimination of low-level failures (or tampering); or then again it may add layers of encryption between CPU and memory where the user feels paranoid; or it may compile the code to FPGA where performance &lt;em&gt;really&lt;/em&gt; matters.&lt;/p&gt;

&lt;p&gt;The possibilities are endless, as long as the system maintains full abstraction of program semantics from the underlying implementation, as Houyhnhnm computing systems do. When on the contrary, as in Human computer systems, the code is pegged to a particular implementation, then not only is it practically impossible to migrate a program from one domain to another at runtime, but programs may have to be completely rewritten from scratch before they may even be executed in a domain with slightly different constraints regarding persistence, privacy, performance, etc.&lt;/p&gt;

&lt;h3 id="fractal-transience"&gt;Fractal Transience&lt;/h3&gt;

&lt;p&gt;Interestingly, on the visible side of the system, successful Human &amp;ldquo;apps&amp;rdquo; these days have evolved into offering to users some semblance of persistence: configuration settings, lists of open tabs, documents you manipulate — most user-visible application state, most of the time, seems to be preserved from one session to the next, without the user having to issue any explicit command to &amp;ldquo;save&amp;rdquo; anything. Desktop apps still tend to display a counter-productive &amp;ldquo;recovery&amp;rdquo; menu at startup, though. And more annoyingly, this apparent persistence still doesn&amp;rsquo;t cover the most frequent case these days of people typing things: input forms and message boxes in web pages. Also, the &amp;ldquo;catastrophic&amp;rdquo; events are covered include so predictable the event as is the eventual death of each and every piece of hardware — and of each and every software project and service-providing business. Yet, content with expectations from this &lt;em&gt;apparent&lt;/em&gt; persistence, users can easily be fooled, like Ngnghm was initially, into believing that Human computer systems are just as good as Houyhnhnm computing systems in this regard; and just like Ngnghm, they can be led to believe that failures are due to incompetence on their part, rather than on the part of the computing system developers.&lt;/p&gt;

&lt;p&gt;Well, at least, that&amp;rsquo;s how the Houyhnhnm see things: whether or not you can assign blame to any person in particular for the situation of Human computer systems, this situation is deeply dysfunctional. Actually, the Houyhnhnm also have something to say if you cannot assign personal blame for it — and it doesn&amp;rsquo;t look like you can: this means that the meta-system for assigning responsibilities itself is also dysfunctional. Why do &amp;ldquo;vendors&amp;rdquo; of Human computer systems by and large hoard all the freedom but none of the responsibility when it comes to modifying and maintaining their software so it doesn&amp;rsquo;t fail catastrophically and betray the customers? This is a clearly dysfunctional process according to Houyhnhnm criteria. Even when these vendors tout themselves as selling &amp;ldquo;software as a service&amp;rdquo;, they often hide behind their &amp;ldquo;Intellectual Property&amp;rdquo; monopolies to actually make it &amp;ldquo;rotware as a racket&amp;rdquo; — they offer &lt;a href="http://www.jargon.net/jargonfile/b/bitrot.html"&gt;bitrotting&lt;/a&gt; bad expensive service, oriented towards the vendor&amp;rsquo;s interests to the detriment of the users&amp;rsquo;, with no enforceable service level agreement, no way to extract your data in a state usable by any competing service, with the promise that the service &lt;em&gt;will&lt;/em&gt; grow even more inadequate and eventually die (being cancelled, bankrupted, or bungled), yet that you &lt;em&gt;will&lt;/em&gt; have to keep paying, and then pay again when you have to leave or be left behind; but you don&amp;rsquo;t have much choice because patents and other monopolies attract capital and provide disincentive to investment in any competition (if legally allowed at all) or in other services that don&amp;rsquo;t similarly exclude competition through legal tactics. By contrast, Houyhnhnms individually have full ultimate control over their own machines, and it is based on this control that they enjoy division of labour in delegating software maintenance of most (if they are programmers) or all (if they aren&amp;rsquo;t) of their systems to competing providers who are held individually liable in case of failure, and aren&amp;rsquo;t granted monopolies by a centralized privilege-doling entity.&lt;/p&gt;

&lt;p&gt;Now, after Ngnghm made this painful first hoof experience of the persistence failure of Human computer systems, he started investigating how Human computer systems implemented persistence, or failed to. And he discovered to his dismay that beneath the &lt;a href="http://www.www.loper-os.org/?p=448"&gt;veneer of persistence&lt;/a&gt;, there was transience at every level he was looking at — not just transience, but &lt;a href="http://rationalwiki.org/wiki/Fractal_wrongness"&gt;fractal transience&lt;/a&gt;: this fundamental design difference between Human and Houyhnhnm computing systems is observable at every level of these systems. The user, the programmer, the library developer, the compiler writer, the operating system implementer, everyone, all the time, has to assume the software and hardware layers below him are fragile, supposed to work only a single computing domain; everyone will in turn provide a similarly fragile and non-transportable device to the users above him. All the manual handling of persistence costs a significant fraction of software development (about 30% of all code written, an IBM study once counted); &lt;!--
https://web.archive.org/web/20060813202835/http://www.st-andrews.ac.uk/services/admissions/postgrad/schleaf5.html
...Ron Morrison...
"Well, perhaps not quite so easy. Research in persistent programming systems started in the late seventies, when it was noticed that storing 'long-term' data in a different logical framework from 'short-term' data leads to all sorts of problems in large and complex applications. An analysis by IBM showed that around 30% of the code of long-lived, large scale applications was devoted to the movement of data in and out of the programming language domain. The fact that this code is notoriously susceptible to system evolution errors, coupled with the statistic that 2% of the USA's GNP is spent on software 'maintenance', leads us to believe that storing long-term data in a file or database system is expensive."

King, F. IBM report on the contents of a sample of programs surveyed. San Jose, CA: IBM, 1978.
Notably cited by Atkinson &amp; Morrison https://dl.acm.org/citation.cfm?id=615226--&gt; and if you ever want to make a significant improvement to any component at any level, you pretty much have to rewrite the entire software &amp;ldquo;stack&amp;rdquo; above whichever level you are hoping to improve — in other words this requires a significant world-changing event.&lt;/p&gt;

&lt;p&gt;And yet, it runs! Ngnghm was in awe that Human computer systems could run at all; they clearly demonstrated some emerging order so powerful that it could survive despite ubiquitous design flaws — or could it possibly be surviving &lt;em&gt;thanks&lt;/em&gt; to what to this Houyhnhnm appeared as flaws? Ngnghm decided to pursue his investigations…&lt;/p&gt;
&lt;!-- http://j.mp/NgnghmPersist--&gt;&lt;/html&gt;</content></entry>
 <entry>
  <title type="text">Chapter 1: The Way Houyhnhnms Compute</title>
  <link rel="alternate" href="http://ngnghm.github.io/blog/2015/08/02/chapter-1-the-way-houyhnhnms-compute/?utm_source=all&amp;utm_medium=Atom" />
  <id>urn:http-ngnghm-github-io:-blog-2015-08-02-chapter-1-the-way-houyhnhnms-compute</id>
  <published>2015-08-02T14:56:46Z</published>
  <updated>2015-08-02T14:56:46Z</updated>
  <author>
   <name>Ngnghm</name></author>
  <content type="html">&lt;html&gt;
&lt;p&gt;Dear fellow programmer,&lt;/p&gt;

&lt;p&gt;&lt;a href="/About.html"&gt;I&lt;/a&gt; used to think humans wrote software the way they did because they knew what they were doing. Then I realized that they didn&amp;rsquo;t really know, but adopted ways that seemed to work better than others. Or maybe rather humans were adopted by the ways that best knew how to survive, whether they actually &amp;ldquo;worked&amp;rdquo; or not. In any case, I trusted &amp;ldquo;evolution&amp;rdquo;, that is, ultimately, &lt;em&gt;other people&lt;/em&gt;, to have figured out the best way that software could and should be written. But everything I knew about computing changed when one day I met a &lt;a href="http://en.wikipedia.org/wiki/Houyhnhnm"&gt;Houyhnhnm&lt;/a&gt;, who told me how things were done in his faraway land. He made me think in terms of computing systems rather than computer systems; and from my newly found understanding, I could see clearly how computing systems could and should be, that today&amp;rsquo;s (mainstream) Human computer systems aren&amp;rsquo;t. But mostly, he taught me how to &lt;em&gt;think&lt;/em&gt;, by myself, about computing. And so let me take you through my story of computing enlightenment.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h3 id="beyond-the-sea-of-potentiality"&gt;Beyond the Sea of Potentiality&lt;/h3&gt;

&lt;p&gt;This adventurous Houyhnhnm, whose name was &lt;a href="https://twitter.com/Ngnghm"&gt;Ngnghm&lt;/a&gt;, had heard of a stranger who long ago visited his home country. So the legend said, the traveler, called &lt;a href="https://www.gutenberg.org/files/17157/17157-h/17157-h.htm"&gt;Gulliver&lt;/a&gt;, was a &amp;ldquo;Human&amp;rdquo;: a paradoxical creature that looked just like a Yahoo, yet who like a Houyhnhnm (to a point) possessed the ability to reason and speak. There were fantastic tales of a planet full of such Humans, as attributed to this Gulliver; and the stories went that in the land of Humans, there were animals known as &amp;ldquo;Horses&amp;rdquo; that looked just like Houyhnhnms, but like Yahoos couldn&amp;rsquo;t speak any language and were likely not fully sentient. Ngnghm, immensely curious, had embarked on a journey of discovery to find and visit this fantasy land of Humans and Horses, if it existed at all. But while sailing the Sea of Potentiality, his transdimensional ship collided with débris caused by Human (or was it Yahoo) pollution — and he was shipwrecked. Now he was stranded onto our plane of existence (actually a sphere). Not being able to communicate in Human language, he was initially mistaken for an old horse; and he had but narrowly escaped being sent to the knacker — or worse, to a government research facility.&lt;/p&gt;

&lt;p&gt;By the time I met him through a friend, though, Ngnghm had already learned to read and write our language, albeit imperfectly. He was desperately looking for parts to build a new ship, so that he may some day sail back home. Since I know nothing of transdimensional travel, I instead showed him how to use the Internet to find all the support that mankind could offer him. He was stupefied by how similar yet how different our Human computer systems were from those of the Houyhnhnms; in some way, ours were so much more advanced, yet in other ways they were so desperately primitive. And as he was telling me of how Computing was done amongst Houyhnhnms, I was suddenly reminded of how I had always felt that there had to be better ways to engage in computing, but couldn&amp;rsquo;t pin point exactly what was wrong. Now I had found a clearer vision of a world I was yearning for — a world I felt like I had lost, though I never had it — and a world that was within reach if only I could build a suitable ship, to sail the Sea of Potentiality and reach the mysterious and enticing land of Houyhnhnm computing.&lt;/p&gt;

&lt;p&gt;Some people have accused me of having imagined all this encounter. But my hope is that, after reading my story, you&amp;rsquo;ll see that it is not only real but necessary, and soon you will start telling other people what you&amp;rsquo;re now imagining; and eventually, you and I will build the ship with which we will sail together to the land of Houyhnhnm computing.&lt;/p&gt;

&lt;h3 id="a-different-point-of-view"&gt;A Different Point of View&lt;/h3&gt;

&lt;p&gt;The fundamental difference between Human computer systems and Houyhnhnm computing systems is one of &lt;em&gt;point of view&lt;/em&gt;. Houyhnhnms do not possess a different kind of logic, nor mathematics, nor physics; though they have discovered how to travel through many dimensions, they do not have quantum computers, logical oracles, or any magic means of computation beyond our own capabilities. But they approach computing in a way that is foreign to us Humans, and that leads to very different results.&lt;/p&gt;

&lt;p&gt;Whereas Humans view computers as tools below them to which they give orders and that do their bidding, Houyhnhnms view computing as an interaction within a system around them that extends their consciousness. Humans articulate their plans primarily in terms of things: the logical and physical devices they build (sometimes including tools to make more tools), in the lower realms of software and hardware. Houyhnhnms weave their conversations foremost in terms of processes: the interactions they partake in that they attempt to automate (including these conversations themselves), which always involves &lt;a href="https://en.wikipedia.org/wiki/Wetware_(brain)"&gt;wetware&lt;/a&gt; first. In short, Humans have &lt;em&gt;computer&lt;/em&gt; systems, Houyhnhnms have &lt;em&gt;computing&lt;/em&gt; systems.&lt;/p&gt;

&lt;p&gt;You may dismiss all this as dreamy philosophy, empty words without any consequences — I certainly did so at first. Yet the difference in point of view that I am now attempting to distill leads to systems that are organized in very different ways, that are optimized for very different metrics, and that engage users in very different processes, with role delineations according to very different criteria, resulting in a very different variety of artifacts of very different sizes, but most importantly, connected in very different ways. It may all be but &lt;em&gt;just so stories&lt;/em&gt;, but &lt;a href="http://fare.tunes.org/computing/bal2009.pdf"&gt;stories have consequences&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id="simplicity"&gt;Simplicity&lt;/h3&gt;

&lt;p&gt;What made me most aware of this difference was when Ngnghm discovered that, like him in his own world, I was trained in writing software, and then asked me to demonstrate the working of some Human computing systems, starting with the simplest I could find. So I showed him simple programs I was writing in C; C is a relatively simple programming language with a somewhat familiar syntax and well defined enough formal semantics, but nevertheless a universal programming language capable of doing everything, and indeed used almost everywhere that Humans have computers. Yet, after he painfully assimilated enough of what I showed him, struggling all the way, his conclusion was that, no, I was obviously not programming in C, and that I couldn&amp;rsquo;t possibly be programming in C, because C was not a universal programming system at all, but could do next to nothing, and only very inefficiently so. Instead, what I was programming in was not just C, but a C compilation toolchain plus an IDE and an Operating System plus plenty of libraries and utilities, that all together constituted a very large computing system with incredibly complex formal semantics; what more, a large part of the interaction between these components depended on a large number of completely informal semi-conventions about how the filesystem was or wasn&amp;rsquo;t used by which process, and how these system and user processes themselves were managed. What to Humans looked simple because our &lt;em&gt;point of view&lt;/em&gt; focuses on some aspects and neglects others, to the Houyhnhnms was an unmanaged and unmanageable mess because they see things from a different angle.&lt;/p&gt;

&lt;p&gt;What Houyhnhnms considered to be a simple system was one that has a short description when you take into account the entire software system, including the compiler, interactive editor, formal verification tools, libraries, operating system, drivers, hardware blueprints, etc., and including the informal conventions used by isolated or cooperating users, or the chaotic lack thereof. C, because its underlying development environment necessitated huge and largely informal support structures, constituted a very complex computing system, even though it looked small and simple once the support system was assumed. Functional programming languages like ML or Haskell yield much simpler systems if you take into account the verification tools and the development process; yet they still neglected entire swaths of what makes a complete computing system, such as IDE, Operating System, persistent storage usage conventions, schema upgrade, etc., and so they ended up being overall still pretty complex.&lt;/p&gt;

&lt;p&gt;By Houyhnhnm standards, the simplest Human computing systems, though far from ideal, would be more something like Smalltalk or the other systems built by Alan Kay&amp;rsquo;s &lt;a href="http://vpri.org/"&gt;ViewPoints Research Institute&lt;/a&gt;, where the description for the entire system, including compiler, IDE, libraries, operating system, drivers, interactive graphical environment, font rendering, etc., all fit in a few tens of thousands of lines of code. Note that FORTH has been used to build complete systems of even smaller overall software size; but being low-level, FORTH relies more on informal design patterns and manually enforced limitations, which according to Houyhnhnm criteria make the resulting system overall more complex, especially so if multiple people are supposed to work on the same system; still such simplistic systems make sense for the isolated resource-starved programmers.&lt;/p&gt;

&lt;p&gt;Houyhnhnms certainly don&amp;rsquo;t restrict themselves to using systems that are simple (according to their metric). But these simple systems do play an essential role in the Houyhnhnm computing system ecology: first, they are an essential part of computing curricula, so programmers can get a grasp of all the parts that make a complete system; second, the ways to factor and evolve such systems is also studied by designers and managers so they may think in terms of overall system architecture (including the Houyhnhnm factor, of course); last but not least, they are also instrumental in the bootstrapping process by which more complex systems are built in a way that is &lt;a href="http://fare.tunes.org/computing/reclaim_your_computer.html"&gt;auditably &lt;em&gt;secure&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In any case, a change in point of view led to a completely different metric to assess the simplicity of computing systems. It would also change how to judge other qualities of computing systems in general — and thus change the approach to how computing is done and what artifacts it yields. And next on the block was something as basic as Persistence&amp;hellip;&lt;/p&gt;

&lt;h3 id="post-scriptum-pronounciation"&gt;Post-Scriptum: Pronounciation&lt;/h3&gt;

&lt;p&gt;How do you pronounce &amp;ldquo;Ngnghm&amp;rdquo; or &amp;ldquo;Houyhnhnm&amp;rdquo;, my friends ask? Mostly, I don&amp;rsquo;t. You must realize that these are attempted transcriptions of sounds that Houyhnhnms make with their equine mouths. So if you&amp;rsquo;ve never met a Houyhnhnm, just imagine one of them whinnying in a way that you&amp;rsquo;d transcribe like that if you had to. Or don&amp;rsquo;t. Personally, I have stopped trying to mimic the way Ngnghm neighs, and if I have to pronounce one of these two names when talking to a friend, I just say the initial &amp;ldquo;N.&amp;rdquo; or &amp;ldquo;H.&amp;rdquo;. Importantly, though, I am careful to avoid the H-word when talking about Ngnghm and his kin: he deeply dislikes and vehemently objects to being assimilated to these stupid creatures, Horses — just like you probably wouldn&amp;rsquo;t want to be taken for (and treated as) a Yahoo.&lt;/p&gt;&lt;/html&gt;</content></entry></feed>