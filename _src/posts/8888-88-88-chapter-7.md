    Title: Chapter 7: Applications or Platforms
    Date: 2015-12-13T14:15:16
    Tags: DRAFT

My previous discussion with Ngnghm left me baffled:
I could somehow understand that
Houyhnhnm computing doesn't have the concept of an Operating System Kernel;
and I could vaguely guess how each of several aspects of a Human kernel
could have corresponding software patterns in a Houyhnhnm system,
at each of various levels of abstractions.
But while I could visualize each of these individual patterns,
it was less clear to me what the big picture was
when these smaller compile-time, link-time and runtime abstractions
were put together.
So I decided to approach the software architecture from the other end:
what do end-user applications look like in Houyhnhnm computing systems?


<!-- more -->

### Autistic Applications ###

By that time, Ngnghm was starting to get familiar with Human computer systems,
and the few end-user applications that he was using daily.
He explained that at least for self-contained end-user applications,
the situation was very similar, at least superficially:
games, interactive art, audiovisual performances,
showroom displays, news and other writings, etc.,
things that are made to be explored by the user
but not modified in any significant way.
The Houyhnhnms had a name for them
that I translated as _autistic applications_:
applications that mostly didn't communicate much if at all with any other application
in any way that the end-user cared to control.
These things looked pretty much the same in Houyhnhnm computing systems and Human computer systems:
You somehow get ahold of a program and its dependencies if any,
run it in a sandbox, and interact with it;
and it doesn't matter too much what the program does,
precisely because the information flow is essentially one way,
from the application to the user.

Still, there were a few subtle points
where even these autistic applications differ on Houyhnhnm computing systems
from what they look like on Human computer systems.
For instance, you could always copy and paste text and pictures and sounds,
search for words in registered dictionaries, or otherwise manipulate the application output;
these did not require the application developers having to do anything to enable such features.
But a more striking difference was that
all Houyhnhnm applications inherit from the system its
[orthogonal persistence](/tags/Orthogonal-Persistence.html).
You can thus always interrupt the application and save and restore its state at any point in time,
except where explicitly not desired (e.g. in the middle of a transaction).
Then you could go back in time and replay
(and in the case of videos or music, go forward in time),
according to a protocol that is uniform across applications;
and not only was it no additional burden on application programmers,
it was something they couldn't get subtly wrong, and that users could thus casually rely upon.
There was never any loss of any session state, disappearing tabs or windows,
games where you couldn't save, etc.
There were no messages you had to enter twice because they were cleared between two loads,
or that reappeared because the clearing failed to be recorded.

Of course, applications that require interaction with a remote service
are always able to save and restore the client-side state from previous user interactions;
however that does not entail being able to save and restore any server-side state,
at least not without support from the server.
And while the system typically makes it easy for the server developers
to provide that support if they want,
there are many reasons why they might not want to do it, including cost and confidentiality.
Conversely, for reasons of privacy, a user might want to replay a previous session
without telling the remote server.
And for regression testing or debugging their applications, developers might want to replay
parts of the server side interactions without affecting the users.
All these are expressible in Houyhnhnm computing systems:
you can specify the scope and context in which you replay some computation,
within the resources that you control.


### Typical Applications ###

Now, most applications are not autistic:
they do involve exchanging data with other applications.
In other words, the information processes they partake in
may directly involve other automated programs;
they do not require a sentient being's brain (Human or Houyhnhnm)
as an intermediate between their processing and further automated processing.
And there we see that even "autistic applications" are not "autistic processes":
An autistic application does not communicate with other automated programs
but does interact with sentient users
(its implementation might also interact with other programs
below the abstraction provided to the user, but that's mostly invisible to the user).
An "autistic process" that communicates with no other process whatsoever,
not even those in a sentient being's brain, can and will be wholly optimized away.

Ngnghm then explained that the situation differs sharply between Human and Houyhnhnm systems
regarding all these typical, non-autistic, applications.
Human computer systems tend to make communication between applications very difficult,
by requiring any such communication to be handled at the base level;
instead, Houyhnhnm computing systems consider such communication the heart of the system,
and make it easy by handling it at the meta level.
Human computer systems do not even have a proper notion of meta-level!

A first obstacle to inter-application communication in Human computer systems,
is that the only common abstractions are very low-level,
in terms of array of bytes.
Any higher-level objects have to be encoded into sequences of bytes,
shipped across costly runtime virtual process boundaries,
then decoded back into objects on the other side.
Applications thus have to agree on
complex, expensive, bug-prone yet inexpressive low-level communication protocols
that are big security liabilities.
Having to deal with such protocols is a huge barrier to entry
that explains why few programmers endeavour to try it.
A lot of this work can be completely automated
using type-directed code-generation;
and the better Human systems do it to a point
(see Protocol Buffers, Cap'n'Proto, piqi, etc.);
but the integration with types of actual programming languages
remains generally lackluster.
What types can be used for generally shareable data
remain very limited and inexpressive,
whereas whatever types they can use for manipulating data within a given program
remain generally oblivious of any
sharing constraints, ownership rights, access restrictions, etc.
In Houyhnhnm computing systems, communication of objects is handled
by the system at the highest level of abstraction possible:
that of whichever types and checks are being used to define and validate these objects.
Low-level encoding and decoding can be eschewed altogether for linear objects
where both processes trust each other with respect to representation invariants;
it can sometimes be reduced to mere checking when the trust is incomplete;
and where encoding or checking is actually required, it is automatically extracted
based on type information available either at compile-time or at runtime.
The programming language types _are_ the communication types,
and if foreign languages need to communicate with each other,
it's a regular matter of FFI (Foreign Function Interface)
that you need to solve anyway, and might as well solve once and for all,
rather than have each application invent its own bad incompatible partial solution.

A second obstacle to inter-application communication in Human computer systems
is that they have very poor algebras and interfaces
for users to combine processes.
For most users, sharing data between applications requires one of two things:
selecting and copying (or cutting) data from one application using a mouse,
then pasting it into another application;
or having the application save or export a file to a local disk,
then opening or importing that file in another application
(with "interesting" consequences when two applications try to modify it at the same time).
Developers can do better, but there's a large discontinuity between
the skills required to merely use the system,
and the skills required to do even the simplest things as you program the system.
Modern Human computer systems tend to allow for an intermediate layer between the two,
"scripting", with Unix shells and their pipes, or the notably more modern PowerShell on Windows.
Scripting lowers the barrier to building applications,
and when using "client" utilities and libraries, allows to share data beyond copy-pasting and files;
but it still remains quite complex to use, and often brittle and limited in expressiveness,
because it does not directly partake in either of the programs'
invariant enforcement and atomic transactions
(though a few applications offer suitable a transactional interface).


### Houyhnhnm Platforms ###

Houyhnhnm computing systems are based on the premise of small modular entities
that each do one thing well;
and these entities can be combined inside a common platform
that does its best to reduce the discontinuity between using and programming.
To Houyhnhnms, there is no difference between using and programming;
if anything,
_the difference between a programmer and a user, is that
the programmer knows there is no difference between using and programming_.
Certainly, there is a continuum of proficiency and knowledge amongst users;
but there is generally no large barrier for users
to generalize and automate as a script
whatever computations they know how to achieve interactively;
and there isn't a large amount of boilerplate required to write the least program,
as in all Human programming languages except
["scripting languages"](https://github.com/fare/asdf3-2013/blob/master/scripting-slides.rkt).
All these platforms are built around a high-level programming language accessible to the user;
therefore communication happens directly using objects in the system language
so no serialization or deserialization into low-level bit sequences is required
(or if it is, for the sake of network communication, it can be automated);
and the system language is available to name entities, combine and apply programs.

A few Human computer systems have historically followed this model:
Smalltalk workstations (from Xerox),
Lisp Machines (from Xerox, MIT, Symbolics, LMI or TI),
Hypercard (on old Apple Macintosh'es);
to a point, HP calculators or Mathematica.
But perhaps the most successful such platform to date is GNU Emacs:
it is largely written as a set of modules in a "scripting language" (Emacs Lisp);
entities defined in a module can be freely used in another one,
and data is freely exchanged.
Its programming language is antiquated
(more so than Smalltalk or Lisp Machine Lisp ever were),
and its data structures are heavily biased towards text editing;
and yet it remains widely used and actively developed,
because in many ways it's still far ahead of any competition.

In a Houyhnhnm computing system, programmers tend
to not write standalone applications in non-autistic cases;
instead, they write new modules that extend
the capabilities of the platform.
Often, a new module will extend the system to handle new entitites.
As long as these entities implement common interfaces,
they can be used along all previously known entities
by all existing modules that use these interfaces.
For instance, a new picture compression format is automatically usable
by each and every function that uses pictures throughout the system;
a common extensible picture editor can be used on all pictures anywhere on the system;
a common extensible text editor can handle any kind of writable text in the system; etc.
At all times, each of these modules, including all common editors,
will include all the user's customizations;
this makes writing customizations much more worthwhile than if separate customizations
had to be written for each application, each in its own language with its own learning curve,
as is the case in Human computer systems.

A new module may also define new interfaces, and how they apply to existing kinds of entities.
There is of course a problem when two modules that don't know each other
extend the system by one adding new kinds of entities and the other defining new kinds of interfaces,
the combination leading to new cases that are not handled.
Houyhnhnm systems are not magic and can't generate handlers for those cases out of thin air.
A further module may define how to handle these new combinations;
or a suitable generic fallback may have been provided with the new interface;
or lacking any the above, the system will fail and drop to its metasystem, that will handle the error.
In the end, it's still the job of some programmer to ensure the overall system works suitably
in the cases that it will actually encounter.
These issues exist in Human and Houyhnhnm systems alike —
the only difference is that Human computer systems are so difficult to extend
that programmers seldom reach the point when they can confront these problems,
whereas Houyhnhnm computing system eliminate enough
of the artificial problems in extending the system
that users are more often confronted with these extension-related issues.


### Implicit Communication ###

Communication with other processes can be explicit:
a process will specifically name another process or start a new process,
open a communication channel with it, and exchange data.
Explicit communication will do exactly what the programmers want
(or at least say: even Houyhnhnms have no [DWIM](http://www.jargon.net/jargonfile/d/DWIM.html));
it is no more complex or costly than programmers need;
but it requires tight coupling between the programs (and thus programmers)
on all sides of the communication, and is difficult to extend or adapt
to suit the dynamic needs of the end-user.

On the other hand, communication with other processes can be implicit:
something outside some process grabs data from it, and makes it available to some other process.
This is the case with copying/pasting, or with
piping the standard output of one process into the standard input of another.
Implicit communication is controlled by the users of a program
rather than by the programmers who write it, and therefore adapted to _their_ needs.
It may require complex support from the programs that partake in it
(or, we'll argue, their meta-programs);
but programmers don't have to worry about programs on the other side,
as long as they abide by some general protocol (and keep up with its update).

Note that implicit vs explicit is a continuum rather than a clear cut distinction:
every communication is partly explicit, because it necessarily involves grabbing data
that was somehow published by the first process, the publishing of which wasn't optimized away;
and every communication is partly implicit, because it always relies
on something in the context to effect that communication, at the meta-level.
Another name for this dimension of software design is declarative vs procedural programming:
In the declarative approach, programmers describe what is being computed,
without specifying how it is going to be computed or how it will further processed,
which will be determined by strategies at the meta-level.
In the procedural approach, programmers describe the steps of the computation
without specifying what is going to be computed,
and all the operational semantics remains at the base level.

Houyhnhnms recognize the importance of both aspects of communication, implicit and explicit;
meanwhile Humans tend to be blind about the implicit aspect,
because they are habitually reluctant to seriously consider anything at the meta-level.
When Humans tackle implicit communication (and they cannot not do it),
they advance backwards into the topic, blind about what it is about;
and thus they end up with implicit communication systems
simultaneously quite complex and costly for programmers to implement,
yet extremely limited in expressive power for the end-user.


### The Case of Copy-Paste ###

The kind of implicit communication most visible to end-users in Human computer systems
is copy-paste: applications interact with a graphical interface,
and may allow the user to either copy or cut part of a document being displayed;
the clipping is then stored in a global clipboard (with space for a single clip).
Another application interacting with the graphical interface may then
allow the user to paste the clipping currently in the clipboard into its own document.
The two programs may know nothing of each other;
as long as they properly partake in the protocol,
they will have communicated with each other as per the desires of the end-user.
By providing user-controllable implicit communication between most applications,
it is an essential feature in Human computer systems.

Now, on Human computer systems, copy-paste requires every participating application
to specially implement large chunks of graphical interface support.
Every application then becomes somewhat bloated, having to include large graphical libraries
(which in modern systems can happily be shared between applications);
but also having to properly initialize them, follow their protocols,
abide by the strictures of their event loop, etc.
They have to be able to negotiate with the clipboard server
the kind of entities they can copy and paste,
and/or convert between what the server supports and what they directly support.
This means applications are much more complex and more brittle, therefore
harder to reason about, less robust and less secure.
The overall protocol copy-paste will in turn inherit the unreliability
of the applications that partake in it.
And despite all this complexity, often, some application will fail to support that protocol
for some of the information it displays (e.g. an error message),
that the feature is sorely missed as the user needs to copy said information by hand.

An interesting exception to the rule of the above paragraph
is the case of "console" applications:
these applications display simple text to
a "terminal emulator" straight out of the 1970s,
at which point all the output can be copied for further pasting.
The terminal emulator thus serves as the meta-program responsible
for presentation of the application output, and handling copy-paste.
This comes with many limitations:
only plain text is supported, not "rich text", not images;
lines longer than the terminal size may or may not be clipped,
or have an end-of-line marker or escape character inserted;
selecting more than a screenful may be an issue,
that you can sometimes work around by resizing the terminal or switching to tiny fonts;
normal output and error output may be mixed (or worse, output from background programs);
layout artefacts may appear (such as spaces to end-of-line,
or graphic characters that draw boxes in which text is displayed); etc.
Still, the principle of a meta-program to handle display already exists
in some Human computer systems; just with a baroque and antiquated limited protocol.


### Protocols a Meta-level Business ###

Houyhnhnm computing systems resolutely adopt the notion that presenting computation results
is generally the task of a series of (meta)programs separate
from the one that is computing the results.
Factoring out the interface at the meta-level means that
each level can be kept conceptually simple.
The system remains ["reasonable"](http://thetrendythings.com/read/20412),
that is susceptible to be reasoned about.
Security properties can be assessed once,
abstracting away the specifics of programs using the features.
Robustness and timeliness of the system don't have to depend
on every application partaking in the protocol being well-behaved,
on every programmer working on any such application
being steadfast at all times and never making any mistake.
There can be no bugs in all the lines of code that the programmers don't have to write anymore.
And updating or extending the protocol is much easier,
since it only involves updating or extending the according meta-programs,
without having to touch the base-level applications
(unless they want to explicitly take advantage of new features).

Moving features from base-level applications to meta-level layers can be justified
with all the same arguments why "preemptive multitasking" beats "cooperative multitasking"
as an interface offered to programmers:
human programmers are intrinsically unreliable,
any kind of "cooperation" that relies manually written code to abide by non-trivial invariants
in all cases will result in massive system instability.
At the same time, "cooperation" is (and actually must be) used under the hood
to preserve any non-trivial system invariant — but can it be used automatically, correctly,
through a meta-program's code-generator.
That is, unlike what's often the case with Human computer systems,
meta-programs need not be strictly runtime entities,
but must actually be also compile-time and/or link-time entities,
that will ensure all runtime code strictly follows all system protocols, by construction.
