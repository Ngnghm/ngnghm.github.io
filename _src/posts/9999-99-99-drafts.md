    Title: Kitchen Sink
    Date: 2015-08-03T00:00:00
    Tags: DRAFT

<!--

# Random ideas for future posts

    Title: Chapter 1: The Way Houyhnhnms Compute
    Date: 2015-08-02T10:56:46
    Tags: Introduction, Point of View, Simplicity

    Title: Chapter 2: Save Our Souls
    Date: 2015-08-03T01:10:00
    Tags: Persistence, Automation, Orthogonal Persistence, Domains

    Title: Chapter 3: The Houyhnhnm Version of Salvation
    Date: 2015-08-09T01:10:00
    Tags: Persistence, Orthogonal Persistence, Files, Versioning, Monitor

    Title: Chapter 4: Turtling down the Tower of Babel
    Date: 2015-08-24T19:51:01
    Tags: Persistence, Transience, Quality, Meta, Strategies, Turtles

    Title: Chapter 5: Non-Stop Change
    Date: 2015-09-08T23:54:23
    Tags: Live Upgrade, Upgrade, Types, Schema

    Title: Chapter 6: Kernel Is As Kernel Does
    Date: 2015-11-28T23:34:45
    Tags: Kernel, Resource Management, Abstraction, Enforcement, Security, Meta

    Title: Chapter 7: blah
    Date: 2015-12-13T14:15:16
    Tags: DRAFT


## Intros


Conversely, I grew more and more curious of how things were done in Houyhnhnm computing systems
that seemed hard to me while programming Human computer systems;
and as the conversation went on,
I took it as a challenge and as a matter of the Honor of the Human Race to discover things
where Human computer systems would shine in comparison to Houyhnhnm computing systems,
or at least where Houyhnhnm computing systems would suck even more than Human computer systems.

## Ownership

In a swarm of nanobots and wearable devices,
how do you respect the landowner while serving the owner?


### Casual Browsing ###

Persistence: History, forever.

Diff: notification of changes. Suggestions of changes.


### Applications ###

Delivering software as components, not applications (Human closest: browser plugins)

AOP: Modularity in implementation strategies


### Sentient-Computer interface

Failure of UX Design of the programmer experience.

Aspects: search.


### Conclusion

We discussed many hours, and eventually found that we could explain
the difference in approaches by a Houyhnhnm _computing_ system being a _live system_
based on the premise that the system stays always on
and evolves in internal interaction between machines and programmers,
whereas Human computer systems are [_cult of dead_](http://wiki.squeak.org/squeak/2950) systems
in programs never change then die taking all their data with them,
change happening as external commands from the programmer above.

By starting from the interactions and looking for general abstractions,
Houyhnhnms are able to provide a general solution
where Humans, starting from their devices and building only up
must implement a large variety of ad hoc tools.

That is because the basic premise of Human computer systems is that
change is external to programs, that it comes from Humans above,
in a one way command-and-obey interaction;
in Human computer systems,
programs are fixed entities that never change,
and any change requires that processes running the old programs must die,
taking all their data with them, to be replaced with new programs.
In other words, Human computer systems are
[_cult of dead_](http://wiki.squeak.org/squeak/2950) systems.
By contrast, Houyhnhnm computing systems are _live systems_,
where the code is not separate from the data,
but the two evolve together in a two-way interaction
with a Sentient being who isn't above but beside them.




### Bla blah

ASLR (Address Space Layout Randomization): symptom of deep problem.
<beach> You take a stupid OS and a stupid programming language vulnerable to attacks.
Instead of fixing them, you patch the thing by adding ASLR.  *boggle*!


Any opaque code is to be run in isolated virtual machines; even then,
security requires low-level code to be accompanied with proof that
suitable invariants are preserved, whereas those invariants are
preserved by construction if the code is delivered as written at a
higher level of abstraction. Thus even opaque code may be delivered
at a level that is source code if not for the author, at least for
whoever checks security.


Humans have many devices that they connect into networks, where bits are copied.
Houyhnhnms have a single system that they subdivide into domains,
between which data is distributed.


(Pure) Functional Programming Claims IRL
http://logicaltypes.blogspot.com/2015/08/pure-functional-programming-claims-irl.html


Not only does that make Houyhnhnm systems much simpler,
it also guarantees forever interoperability of every single piece data with any future system,
at whichever level of abstraction that data was defined.
If you want your data to remain relevant to your future self, or to be usable by other people, etc.,
you still need to wisely choose suitable algebraic data types,
to organize software into components with clean interfaces,
to pick appropriate policies that lead to suitably performant implementations,
to rely on suitable libraries.


-------------------------------------------------------------------------------

### Version Incompatibilities

When integrating software as well as in other software development
endeavors, it is good to be able to detect errors early and close to
the change that caused them, rather than late and far from that
change.  Therefore, when some developer knows that some versions of
some modules are incompatible with some versions of other modules, it
is good for that information to be declared so that the
incompatibility be diagnosed and addressed early on, rather than to
wait for a catastrophic failure to happen much later during the build,
or worse in production. (Note that when some combination of versions
is known to work, there is already a way to declare it, by promoting
these versions together in an integration branch.) But there are
rules on how these declarations may be done properly, and they follow
from the Houyhnhnm law of configuration design: _Thou shalt allow each
one to contribute what one knows when one knows it, and thou shalt not
either require or allow anyone to contribute what one doesn't actually
know._

Every new version of any module essentially claims: _thou shalt not
use an earlier version (in this branch)_. Indeed, it fixes known bugs,
introduces needed features, or otherwise improves the software (or
else no one would have bothered to write that version, or to merge it
into their branch of interest); otherwise it wouldn't have been
committed; and it passes all the tests that qualify it to be in that
branch, so is no worse than previous versions, as far as those tests
go. Of course, that version might not make it to more widely tested
branches; but indeed, it won't make it there, so won't be there to
make that claim. In any case, the incompatibility with other versions
of the same module is obvious, and so is the policy as to which
version to prefer in case of doubt or conflict. What is more
interesting is the case of incompatibilities between versions of
different modules.

When the _author_ of a module A initially starts _using_ another
module B, then releases a version of A, he may declare the version of
B he used as a prerequisite for using A. Indeed, if he used a stable
version of B, there should be no reason to ever use an older version
of B, that if B is well-maintained will cause old bugs to resurface
and possibly required features to disappear. And if he used an
unstable version of B, he should only have done that because his
module A needed a recent improvement to B that wasn't released in a
more stable branch. In both cases, he is reasonably justified in
publishing this version requirement. Of course, he may revise this
requirement later, if he somehow has echoes of A passing all tests and
working correctly with an earlier stable version of B, or if he tries
with a stable release of B that includes the features he needs, and
decides to advertise _that_ instead of the unstable version he used
initially.

It's also a good thing to declare incompatibility with old versions
you know not to work. You've seen those versions, you know why trying
to use them will cause headaches to whoever tries to use them (and to
you when they come whining about it). So you forbid them. Users must
use a newer version with fixes to known issues. Fewer headaches.

Now, what's bad is when you're an author and you declare
incompatibility with future versions you can't possibly have seen. In
particular, it's bad if you specify an exact version for a dependency,
rather than a minimum version. Indeed, not only you can't know that
future versions will be incompatible, but it is almost guaranteed that
sooner or later, in that dependency there _will_ be a found a bug, an
essential feature that is lacking, or worse, a major security issue;
and, that will cause its version to be bumped past what you
arbitrarily declared to be the maximum compatible value. People will
_have_ to upgrade, and your declaration will only make it harder, for
now they will have to patch out your declaration. If everyone did
that, hundreds of libraries might have to be recursively patched
everytime the version changes in some dependency at the bottom of the
dependency graph.

Build files, written by a module's _author_, must almost never contain
maximum version information for dependencies. Very rare exceptions
include analyses, benchmarks, patches, exploits, etc., based on an
exact version of some software artifact (e.g. known firmware version).
On the other hand, when build files are distributed separately from the
source they are supposed to build, it makes sense to include version
identification for said source (as opposed to its dependencies) in the
build file itself.

It is the _integrator_'s role to build, test and release many modules
together; he is the one who will have to select the exact versions of
every module involved in his system. When authors try to do the job of
the integrator even though they cannot possibly have the contextual
information required, they are bound to fail.


-------------------------------------------------------------------------------

Build files are tools for _authors_ and _users_, not integrators. If
some weird integrator wants to use a build file to track versions,
rather than git, he's wrong, because for most libraries asd version
strings do not completely identify the exact code being used. Even
assuming asdf version strings were enough, and/or were a useful
checksum, this still shouldn't be a feature of a regular ASDF system.
If this software integrator wants to specify exact and/or future
versions in a defsystem, he first will have to define a subclass of
SYSTEM that will allow those specifications.

If a controversial major incompatibility is
introduced that causes a lot of software not to migrate to the new
API, the right thing to do is to fork the damn library. Either the old
API or new API will have to go by a new name.

You can keep calling your software informally Python 2 and Python 3,
but the system-name as far as ASDF is concerned will be "cl-python2"
and "cl-python3". If the old one was called "cl-python" and you want
to keep the name after the major incompatible API changes, you have to
tell those who refuse to upgrade that they will have to fork your
library and they will from now on have to use "cl-python2" as their
dependency instead of "cl-python".

ASDF has restrictions on the version strings it accepts. It's OK to
have restrictions on the naming conventions users may have. No, you
can't have two divergent majorly incompatible libraries have the same
name, be distinguished by version only, and expect the ASDF version
system to help you. Just nope.

Emergency patches are sometimes necessary, but they are not meant to
be permanent solutions. It's sometimes necessary to do gross hacks due
to imminent deadlines but that's not a reason to bless them as the
right way to do things.

If your system depends on xmls 1.2 but won't work with xmls 1.3, there
are three real permanent solutions:
a) fix your system and/or the latest xmls so they will work together,
and declare a dependency on the latest (maybe fixed) xmls.
b) fork xmls 1.2 to preserve its API and/or implementation forever,
since it is somehow superior for your purposes.
c) introduce a new system xmls-1.2-compatibility that implements the
missing pieces of the xmls 1.2 API on top of the xmls 1.3 API, if
that's possible and sufficient for your purposes but the maintainer
somehow refuses to do so.

But pretending that there is still a single entity "xmls" when in
actuality there has been a fork in the API with permanent divergent
user communities, is always the wrong thing to do. It's as if ffmpeg
and libav both kept the same name despite being complete forks.

No, I use the "moral" vocabulary with no stronger feelings than you
have about providing a service that fits a case that occurs. My
"should" and your "fit" are actually the same concept under different
names. — My apologies for any misunderstanding due to this mismatch in
vocabulary being used.

I believe the central difference between our stances is that I have
recently come to clearly distinguish the two roles of USER and of
INTEGRATOR, that most people seem to confuse at times, and maybe you
included at this time.

As a USER of library XMLS, you have no right to exclude future
versions. That's just not one of your prerogatives. If you're
permanently unhappy with the new versions, you can fork project XMLS,
but you can't declare the future out of existence.

As an INTEGRATOR of a project that uses library XMLS, you're very much
dealing with the present, and indeed, may and sometimes MUST include
unreleased patches to it, and/or withhold upgrade to a new version
with unresolved issues. That's not just your prerogative, that's your
duty and the whole point of your job.

As a WRITER of library XMLS, you get to specify the right way to use
it, to deprecate old usage patterns, etc. If you frequently break
things under the feet of your users without offering a simple way to
upgrade and without sending patches to your known users, you'll piss
them off and maybe they'll fork the project under your feet eventually
or attempt a hostile takeover. Then there will be two projects with
distinct names and/or even more confusion. So be nice, and try to
offer them easy upgrade paths, etc. But ultimately, you're the master
of your ship, and if you decide an old API was buggy, a concurrency
and/or security issue, an unsalvageable mess that cannot be saved —
it's your call to tell your users to man up and adopt the new better
API that solves those essential issues. Or maybe you should be forking
your own project and changing its name if fixing it requires a
completely new API and there's no plan to support existing users.

Often, the same developer wears multiple caps part-time: co-WRITER of
a library A, USER of it in some system B you also co-write, and
INTEGRATOR of some application C that includes it. That's OK. But keep
things separate.

As a WRITER of system B and USER of library A, you can read the git
repo of A, but cannot assume write access to it. And you just cannot
assume that every future INTEGRATOR of every future application
C1...CN will be using any particular version of A; indeed an
"emergency due to an imminent deadline" may very well force each of
said INTEGRATOR at completely different times to each include an
urgent security patch to A, or a forced upgrade, etc. Unless system B
is never ever used by any other application than a single application
C, you just cannot assume control over C when you write system B. And
if you can, then B is actually C and specifying a prohibition on
future version adds little or nothing to the exact version of A
recorded in the source control for C: as an INTEGRATOR, unless a mere
USER, you *do* keep each and every dependency under source control.

Note that in the case of ASDF, the WRITER of ASDF is both the writer
of a library, ASDF itself, and or an application, ASDF-TOOLS that
tests ASDF. As the former, you specifically want to NOT specify any
dependency, as ASDF the library must be capable of running with any
and every past present and future version of every non-broken system.
As the latter, you're an integrator and want a completely reproducible
set of libraries based on which to run your tests. Our current use of
git submodules addresses both usage cases, though awkwardly so.
Another solution might be to split ASDF-TOOLS into a separate
repository indeed.

The .asd file for system B is authored by the WRITER of B who is as
such a USER of A, and has no control or relationship to the INTEGRATOR
of C.

Once again, it's OK to use a horrible kluge when under pressure. But
while it's a solution for the INTEGRATOR who releases an application
that depends on an old variant of the library it is no permanent
solution for the USER whose system uses an obsolete API. And if you
are to go forward as the WRITER of the library that uses an obsolete
API, then some day you'll have to pay, one way or the other. In other
words, you've just accrued TECHNICAL DEBT. To pay it, you may:

1- Fix your project to use the latest upstream library (or switch to
another, better one).
2- Introduce a backward compatibility library that implements the old
API on top of the old one (or of different better-managed library).
3- Fork the upstream library because it sucks and/or has stopped
supporting your use case, and rename everything with a few regexps.
4- Take over the upstream library, declare the new API a heresy, and
the old API the One True API. That works great if the library dies or
falls into being unmaintained and unused, and you are its only user
and/or few users if any have adopted the new API because it actually
sucks.
5- Fork the entire world, declare that the new API never happened.
It's very much like option 4, except that the rest of the world
doesn't believe you.
6- Your lucky project manages to die and/or you manage to leave it
before having to pay its debts. Yay! "Not my problem anymore."

Declaring an upper limit on version compatibility is a semi-formal way
of going into solution 5 or 6.

Note that ASDF version strings, that you here call "versioned software",
are really an API compatibility version, and thus maybe Dan Barlow was right
to model them after Linux .so numbers, even though the
"major number as compatibility breaker" didn't pan out in the end,
due to the model of Lisp software distribution as source differing
from the model of C software distribution as binaries.
So let's speak of software versions and version control
vs api versions and api constraints.

As a WRITER or one system B and USER of another system C,
you have control both software version and api version for B,
but you don't control software version for C and
can only loosely specify api constraints for C.

As a WRITER of C, you control both software version and api version for C,
but neither software version for B, nor api version for B.
[[[Although — maybe we could add a :breaks statement in the defsystem for C,
just like Debian does. This would allow you to warn users against using
known-broken combinations. Or this could be specified in a separate file
that comes with C. In any case, you should only be allowed to specify
known constraints about past releases, and that means putting the
incompatibility
constraint in C and not in B.]]]

As an INTEGRATOR of application A, you control all software versions
for all transitive dependencies of A on your system, including B and C.
Since this trumps control over api versions, you have no interest whatsoever
in adding constraints on api versions, because they would be redundant
underspecifications. You check the existing ones, but you don't need
gratuitously add your own. You otherwise try to be a USER of all dependencies
and if possible a WRITER of none, though you sometimes have to make
local patches
to B and/or C.

If as a USER or INTEGRATOR you have local patches to some dependencies,
and want to register the incompatibility because it is not going to be
resolved any time soon, then send a patch to C that declares the incompatibility
with old versions of B. But patching B to declare incompatibility with future
versions of C is always the wrong thing. See previous email about the right
things you may do.

> Even when using VCS it can be helpful.  I work with people who
> concurrently work on multiple different lisp-based projects.  Those
> projects use different mixes of libraries, and at times even different
> versions of the same library in different projects.  Sometimes,
> inadvertently, ASDF settings can bleed through from one project to
> another.  In such cases -- event when VCSes are pervasively used -- it
> can be quite helpful to have version metadata.
>
It's OK to have different mixes of libraries.
Check the diverging bits in separate file hierarchies and
export a different CL_SOURCE_REGISTRY.
To prevent accident, maintain hygiene and have different windows of
different colors.
If no top-level project is visible from the CL_SOURCE_REGISTRY of the
other projects,
you have a simple way to prevent building something in the wrong environment.
You'll want separate Emacs processes each with its own SLIME, anyway.



------>8------>8------>8------>8------>8------>8------>8------>8------>8------


Things we’d like to do but can’t


Easy process migration, for everything.
Everything can be debugged
What others have done since

Low-Level Language
LLVM

Safe programming
ATS http://www.ats-lang.org/ has dependent and linear types
Idris, Agda: dependent types
Adam Chlipala

Safe Operating System
Coq: Adam Chlipala
Sing# (Spec#): Singularity
SPIN / Promela http://spinroot.com/spin/whatispin.html
Lisp: Symbolics Genera, TI Explorer
SML: Fox, ML/OS
OCaml: OpenMirage
Haskell: hOp, House, H, Kinetic, HLVM http://stackoverflow.com/questions/6638080/is-there-os-written-in-haskell
seL4


High-level language for system programming
ATS http://www.ats-lang.org/ has dependent and linear types
Rust http://www.rust-lang.org/ have linear types

Discussion groups for programming language design
LtU http://lambda-the-ultimate.org/
fonc@VPRI http://vpri.org/mailman/listinfo/fonc

Multi-language platforms
Racket
JVM
.NET

Orthogonal persistence
Eumel.
Most applications now know to save preferences without explicitly clicking save, and keep a recoverable backup of the file being edited even without explicit saving. But it’s expensively done by the programmer just for a few specific settings and data items.
“NoSQL” databases. And SQL back on top of them.

Relational algebra is bad enough. SQL is worse.

And if you shun Turing-completeness, datalog is so much better. (see datomic)
And SQL claims to be declarative but isn't at all for any OLTP use, particularly not so with Oracle
The inventor of relational algebra claimed that SQL was a poor caricature of it.
https://en.wikipedia.org/wiki/Edgar_F._Codd
https://en.wikipedia.org/wiki/Christopher_J._Date
Fabian Pascal database debunkings

That said, I despise the relational data model, though it's better than its SQL caricature. In its common retarded form, it can't even handle either parametric or ad hoc polymorphism. It was already retarded in the 1970s, and is even more so now.
Any old book on Type Theory improves on it.
A "database schema" is just a retarded collection of named types.
And if I had to propose an improvement, it would be to include historical schema transitions as part of the schema.
Only idiots and database experts believe that schema evolution is something magic and not a regular part of working with data.
Atomic transactions have NOTHING WHATSOEVER to do with any particular data model, relational or not.
You can have or not have atomic transactions in a relational database, in a non-relational database, in a filesystem, in a persistent object store, or in your raw mainframe virtual memory. With STM, you can even have it in non-persistent shared memory. It takes a lot of ignorance or stupidity to equate atomic transactions with SQL databases.
And yes, atomic transactions are an extremely important concept. So is the orthogonal notion of Data Model^W^W Type System.
ACID / BASE == Bullshit confused semi-terminology.
And a lot of the interesting systems have NoSQL, though you can always layer some SQL on top. Or conversely implement your objects on top of a SQL database (been there done that). Atomicity and data model are completely orthogonal issues; of course when you go shopping for off-the-shelf solutions you get various package deals. The most robust and/or featureful centralized solutions tend to use SQL, for historical marketing reasons.



Reversible computing
Lenses
Benjamin Pierce’s Bidirectional Programming
Omniscient Debugging (ODB, elm-lang)

Metaprogramming
Most serious languages now provide some means to metaprogram
C++ has templates
Even Haskell has quasiquotations http://scrambledeggsontoast.github.io/2014/09/28/needle-announce/
Wadler: Everything Old is New Again

Whole-system management
some progress in this direction: virtualization, systemd, NixOS, DisNix


Differential Programming
Huet’s zippers http://en.wikipedia.org/wiki/Zipper_%28data_structure%29
http://www.informatik.uni-marburg.de/~pgiarrusso/ILC/
http://www.umut-acar.org/self-adjusting-computation


Crucial things that no one is working on

First-class implementation.
Schema upgrade in an interactive system.
Types for code+data change.
A programming language that can handle both high-level and low-level aspects. Yet, see

Appendices / Draft

Fools ignore complexity; pragmatists suffer it; experts avoid it; geniuses remove it. — Alan Perlis

Bibliography

Better Stories, Better Languages http://fare.tunes.org/computing/bal2009.pdf
http://www.willamette.edu/~fruehr/haskell/evolution.html
http://homes.cs.washington.edu/~emina/
http://okmij.org/ftp/
http://www.ugcs.caltech.edu/~weel/lispm/genera-concepts.pdf


Continuum of Using and Programming.

http://snapl.org/2015/cfp.html


Security.
That there is a way to distinguish "normal" and "virtualized" machines from the inside is itself a huge security #FAIL
All software must always run in "virtualized" mode, and any query attempt is a violation that must cause termination.
& a security violation should kill not just one process, but the entire process group, up to the next a secure(r) shell.
Also automatically log an incident, flag the vendor, and trigger a police investigation.



Management.

https://www.kickstarter.com/projects/637581271/the-3l-project?ref=email


Do better than going back to bits and bytes at the border of every program.
JVM and .NET promise that. Protobufs. CORBA failed. etc.

http://www.desktopneo.com/#tags

-->
