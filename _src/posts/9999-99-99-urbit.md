    Title: Houyhnhnms vs Martians
    Date: 2015-08-03T00:00:00
    Tags: DRAFT

What did Ngnghm think of [Urbit](http://urbit.org/)?
Some elements in Ngnghm's descriptions of Houyhnhnm computing
were remindful of the the famous martian system software stack Urbit:
both computing worlds were alien to Human Computing;
both had Orthogonal Persistence;
and both relied heavily on
pure deterministic computations to minimize
the amount of data to log in the persistence journal
(as contrasted for instance with the amount of data to manipulate
to compute and display answers to end-users).
What else did Houyhnhnm computing have in common with Martian software?
How did it crucially differ?
How did they equally or differently resemble Human systems or differ from them?
Ngnghm took a long look at Urbit;
while he concluded that indeed the three approaches were quite distinct,
he also helped me identify the principles
underlying their mutual differences and commonalities.

<!-- more -->

### Urbit: The Martian Model ###

[Martians](http://moronlab.blogspot.com/2010/01/urbit-functional-programming-from.html)
have developed a peculiar operating system, [Urbit](http://media.urbit.org/whitepaper.pdf)
([docs](http://urbit.org/docs/)),
the Terran port of which seems to be semi-usable since
[2015](https://medium.com/@urbit/design-of-a-digital-republic-f2b6b3109902).
At the formal base of it is a pure functional applicative virtual machine, called _Nock_.
On top of it, a pure functional applicative programming language, called _Hoon_,
with an unusual terse syntax and a very barebones static type inferencer.
On top of that, an Operating System, call _Arvo_, that on each server of the network
runs by applying the current state of the system to the next event received.
The networking layer _Ames_ implements a secure P2P protocol,
while the underlying C runtime system, _u3_, makes it all run on top of a regular Linux machine.

The data model of _Nock_ is that everything is a _noun_,
which can be either a non-negative integer or a pair of nouns.
Since the language is pure and applicative
(and otherwise without cycle-creating primitives),
there can be no cycle in this binary tree of integers.
Since the only equality test is extensional, identical subtrees can be merged
and the notional tree can be implemented as a DAG (Directed Acyclic Graph).

On top of those, the execution model of Nock is to interpret some of these trees
as programs in a variant of combinatory logic, with additional primitives
for literals, peano integers, structural equality,
and a primitive for tree access indexed by integers.
The inefficiency of a naive implementation would be hopeless.
However, just like the tree can be optimized into a DAG,
the evaluation can be optimized into recognizing that some programs implement known function,
then using a special fast implementation of an equivalent program
(which Martians call a _jet_, by contrast with _JIT_)
rather than interpreting said programs by following the definitional rules.
Recognizing such programs in general could be hard, but in practice Urbit only needs
recognize specific instances of such programs — those generated by Hoon
and/or present in the standard library.

Therefore, it is the C runtime system _u3_
that specifies the operational semantics of programs,
whereas Nock only specifies their denotational semantics as arbitrary recursive functions.
By recognizing and efficiently implementing specific Nock programs and subprograms,
u3, like any efficient implementation of the JVM or of any other standardized virtual machine,
can decompile VM programs (in this case Nock programs) into an AST
and recompile them into machine code using the usual compilation techniques.
At that point, the VM is just a standardized but extremely awkward
representation of programming language semantics
(usually all the more awkward since such VM standards are often decided early on,
at the point when the least is known about what makes a good representation).
Where Urbit distinguishes itself from other VM-based systems, however, is that
the semantics of its virtual machine Nock is
forever fixed, totally defined, deterministic, and therefore _future-proof_.

Hoon is a pure functional applicative programming language. Its syntax is terse,
where the core syntax is specified using non-alphanumeric characters and digraphs thereof
(or equivalent for letter keywords);
The syntax allows to write expressions as one liners using parentheses, but
it is colloquial to break functions onto many lines where indentation is meaningful;
as contrasted with other indentation-sensitive languages, however, the indentation rules
are cleverly designed to prevent extraneous indentation to the right as you nest expressions,
by deindenting the last, tail position in a function call.
Whereas Nock is trivially typed (some would say untyped or dynamically typed),
Hoon as a static type system, although quite a primitive one,
and a type inferencer that
requires more type hints than language with e.g. Hindley-Milner type inference (such as ML),
yet less than one without type inference (such as Java).

_Arvo_ is the operating system of Urbit.
The Urbit model is that the state of the system (a noun) encodes a function
that will be applied to the next communication event received by the system.
If the processing of the event terminates, then the event is transactionally appended
to the event journal making it persistent.
The value returned specifies the next state of the system
and any messages to be sent to the world.
Arvo is just the initial state of the system, a universal function that
depending on the next event, may do anything, but in particular
provides a standard library including anything from basic arithmetics
to virtualization of the entire system.
The core of Arvo is typically preserved when processing a message,
even as the state of the system changes to reflect the computations controlled by the user;
as long as this core keeps running (as it should),
Arvo remains the operating system of Urbit;
but users who insist may upgrade and replace Arvo with a new version,
or with another system of their own creation, if they dare.

The events fed into Urbit are generated by the C runtime system u3,
to represent console input, incoming network messages, etc.
Conversely the messages generated by Urbit are translated by the implementation
into console output, outgoing network messages, etc.
If processing an event results in an error,
if it is interrupted by the impatient user,
or if it times out after a minute (for network messages),
then u3 just drops the event and doesn't include it in the event journal.
(Of course, if an adversarial network message can time out an Urbit machine
for a minute or even a second, that's probably already a denial of service vulnerability;
on the other hand, if the owner, being remote, can't get his long-running computations going,
that's probably another problem.)
A stack trace is generated by u3 when an error occurs,
and injected as an event into Arvo in place of the triggering event, that is not persisted.
Users can at runtime toggle a flag in the interactive shell _Dojo_
so that it will or won't display these stack traces.

The networking layer _Ames_ is conceptually a global broadcast network,
where network messages are conceptually visible by all other nodes.
However, a message is typically addressed to a specific node,
using a public key for which only this node has the private key;
and other nodes will drop messages they cannot decrypt.
Therefore, the C runtime will optimize the sending of a message
to route it directly to its destined recipient, as registered on the network.
A node in the network is identified by its address, or _plot_,
that can be 8-bit ("galaxy"), 16-bit ("star"), 32-bit ("planet"), 64-bit ("moon")
or 128-bit ("comet").
A comet has for 128-bit address the cryptographic digest of its public key,
making it self-authenticating.
A moon has its public key signed by the corresponding planet;
a planet has its public key signed by the corresponding star,
a star has its public key signed by the corresponding galaxy,
a galaxy has its public key included in Arvo itself,
in a hierarchical system rooted in whoever manages the base Operating System.
All communications are thus authenticated by construction.
Galaxies, stars, planets and moons are scarce entities,
thus constituting "digital real estate" (hence the name _plot_),
that the Urbit curators intend to sell to fund technological development.

One of Urbit's innovations is to invent mappings from octet to pronounceable three-letter syllables,
so that you can pronounce 8-, 16-, 32-, 64- or 128-bit addresses,
making them memorable, though not meaningful.
To decorrelate names with same address prefix and prevent confusion,
a simple bijective mangling function is applied to an address before to extract its pronounciation.
This constitutes an interesting take on
[Zooko's Triangle](https://en.wikipedia.org/wiki/Zooko%27s%5Ftriangle).
Actually, care was taken so that the syllables would _not_ be too meaningful
(and especially not offensive) in any human language that the author knew of.
Non-alphanumerical characters are also given three-letter syllable names,
though this time the names were chosen so that there were simple mnemonic rules
to remember them (for instance, "wut" for the question mark "?");
this makes it easier to read and learn digraphs
(though you might also name them after the corresponding keywords).


### Houyhnhnms vs Martians ###

Most importantly, the Martian's Urbit is actually available for humans to experiment with
(as of May 2016, its authors describe its status as post-alpha and pre-beta).
By contrast, no implementation of Houyhnhnm Computing system
is available to humans (at the same date),
though the ideas may be older.
This alone make Urbit superior in a non-negligible way;
yet it is all the other ways that we will examine it.

Superficially, both Martian and Houyhnhnm Computing provide Orthogonal Persistence.
But the way they do it is very different.
Martians provide a single mechanism for persistence at a very low-level in their system,
separately on each virtual machine in their network.
But Houyhnhnms recognize that there is no one size fits all in matter of Persistence:
for performance reasons, the highest level of abstraction is desired for the persistence journal;
at the same time, transient or loosely-persisted caches are useful for extra indices;
and for robustness, a number of replicas are required,
with a continuum of potential synchronization policies.
Therefore, Houyhnhnms provide a general framework for first-class computations,
based on which users may select what persists with what modalities.

One could imagine ways that Urbit could be modified so its persistence policies be configurable.
For instance, the underlying C runtime u3 could be sensitive to special side-effects,
such as messages sent to a magic comet, and modify its evaluation and persistence strategies
based on specified configuration.
That would mean, however, that most of the interesting work would actually happen inside u3,
and not over Nock.
What would Nock's purpose then be?
It could remain as an awkward but standardized and future-proof way to represent code and data.
However, unless great care is taken, using formal proofs and/or extensive testing,
so that the semantics of the Nock code generated indeed implements the actual computations,
while indeed being implemented by the underlying system,
then at the least bug introduced or "shortcut" taken,
the entire Nock VM becomes a _sham_.

Now, assuming Nock isn't a complete sham,
it remains an obligatory intermediate representation
between the computations desired by users and the machine implementations provided by the system.
Because Nock is never _exactly_ what the user wants or what the machine provides,
this intermediate representation always introduces an impedance mismatch,
that is all the more costly as the desired computing interactions are remote from the Nock model.

In an extreme case, one could imagine that u3 would be configured
using a Houyhnhnm first-class computation framework.
Users would develop their computations at the level of abstraction they desired;
and they would dynamically configure u3 to use the desired tower of first-class implementations.
At this point, any encoding in terms of Nock could be altogether short-circuited at runtime;
and any impedance mismatch introduced by Nock is thus worked around.
But then, Nock is purely a hurdle and not at all an asset:
all the semantics that users care about is expressed in the Houyhnhnm Computing system;
any Nock code generated is just for show,
obfuscating the real high-level or low-level computations without bringing anything;
and Nock is either a sham, or an expensive tax on the computation framework.


### Future-proofing the wrong thing ###

Both Martians and Houyhnhnms rely heavily on
pure deterministic computations to minimize
the amount of data to log in the persistence journal to describe issues
(as contrasted for instance with the amount of data to manipulate
to compute and display answers to end-users).
But Martians rely on Nock, and to a lesser extent, Hoon, Arvo, Ames, etc.,
having a constant deterministic semantics, cast in stone for all users at all time;
Houyhnhnms frown at the notion: they consider that constraint as unnecessary as it is onerous.
Martians justify the constraint as making it possible to have robust, future-proof persistence.
Houyhnhnms contend that this constant semantics doesn't actually make for robust persistence,
and that on the contrary, it prevents future improvements and fixes while encouraging bad practice.
Also, Houyhnhms claim that requiring the function to be the same for everyone
introduces an extraordinary coordination problem where none existed,
without helping any of the real coordination problems that users actually have.

A global consensus on deterministic computation semantics only matters
if you want to replay and verify other random people's computations,
i.e. for crypto-currencies with "smart contracts" like [Ethereum](https://www.ethereum.org/);
but that's not at all what Urbit is about, and such computation replay in a hostile environment
indeed has issues of its own (such as misincentives or for resource abuse)
that Urbit doesn't even try to address.
If you only want to replay your own computations (or those of friends),
you don't need a global consensus on a deterministic function;
you only need to know what you're talking about.

Houyhnhnms always consider first the interactions
that are supposed to be supported by computing activities.
In the case of Persistence, Houyhnhnms are each interested in persisting their own code and data.
There is no global entity interested in simultaneously looking at persistence logs;
there is no "collective" will, no magically coordinated knowledge.
Each individual Houyhnhnm wants to ensure the persistence of their own data and that data only,
or of that entrusted to them personally; and
even if they want more, that's both the only thing they must do and the only thing they can do.
Now, they each want the most adequate technology for their purpose,
taking costs and benefits into account.
If they somehow had to coordinate together to find a common solution,
the coordination would be extraordinarily costly and would take a lot of time;
they would have to settle on some old technology devised when people knew least,
and could never agree on improvements.
And if the technology were frozen in time at the beginning, as in Urbit,
nothing short of retroactive agreement using a time machine could improve it.
If on the contrary each individual is allowed to choose his own persistence solution,
then those who can devise improved solutions can use them without having to convince anyone;
they can also compete to have their improvements adopted,
whereas users compete to not be left behind,
until they all adopt the improvements that make sense.
In the end, in matters of persistence
[as of build systems](http://common-lisp.net/project/asdf/ilc2010draft.pdf),
_allowing for divergence creates an incentive towards convergence_,
reaching better solutions, through competition.

Urbit incorrectly formulates the problem as being a social problem
requiring a central solution, when it is actually a technical problem for which
a decentralized social arrangement is much better.
Persistence doesn't require anyone to agree with other people on a low-level protocol;
it only requires each person to maintain compatibility with their own previous data.
To decode the data they persisted, users don't need a one deterministic function forever,
much less one they agree on with everyone else:
what they need is to remember the old code and data,
and to be able to express the new code (generator) in terms of the old one (to upgrade the code)
and able to interpret the old data schema in terms of the new data schema (to upgrade the data).
Indeed, even the [Urbit whitepaper](http://media.urbit.org/whitepaper.pdf)
acknowledges that as far as data above the provided abstraction matters,
such schema changes happen (see section 2.0.3 Arvo).

Where Martians get it just as wrong as Humans is in believing that
solving one issue (e.g. persistence) at the system level is enough.
But local "Persistence" of low-level data achieved onerously yet that does not guarantee
distributed persistence of high-level data at the level of service required
(with enough replicas yet low-enough latency) is counter-productive.
The entire point of computing is to support user programs,
and solving an issue for some underlying system at a lower-level of abstraction
without solving it at the higher-level that the user cares about
is actually no solution at all.
It can sometimes be _part_ of a solution,
but only if (1) the desired property can also be expressed in a composable way
so that higher layers of software may benefit from it, and
(2) the lower layers don't impose specific policy
choices that will be detrimental to the higher layers of software.
And this is what Houyhnhnm systems uniquely enable
that Human and Martian systems can't express
because it goes against their paradigm.


### Neglect for the Meta-level ###

The mistake shared by Martians and Humans is to
share the approach of neglecting the importance of metaprogramming.

For Humans, this is often out of ignorance and of fear of the unknown:
Humans are not usually trained in metaprogramming
they don't understand the importance of it, or its proper usage;
they don't know how to define and use Domain Specific Languages (DSLs).
Though their job consists in building machines,
they "enjoy" the job security that comes from
breaking machines that would replace _their_ current jobs:
Mechanized modernity for me, protectionist luddyism for thee.

For Martians, unhappily, there is a conscious decision to eschew metaprogramming.
One recent Urbit presentation explicitly declares that DSLs are considered harmful;
the rationale given is that the base programming language
should have low cognitive overload on entry-level programmers.
To Martians, making the system deliberately simpler and less sophisticated
makes it easier for people to understand and adopt it.
Martians with Hoon commit the same error as the Humans systematically committed with COBOL,
or to a lesser degree with Java:
they designed languages that superficially allow any random
layman (for COBOL) or professional (for Java) or enthusiast (for Hoon)
to understand each of the steps of the program,
by making those steps very simple, minute and detailed.

But the price for this clarity at the micro-level is
to make programs harder to follow at the macro-level.
The abstractions that are denied expression
are precisely those that would allow to concisely and precisely express
the ideas for the actual high-level problem at hand.
Every issue therefore become mired with a mass of needless concerns,
extraneous details, and administrative overhead,
that simultaneously slow down programmers with make-work.
The concepts that underlie the difficult high-level issues that the user needs be solved
cannot be expressed explicitly, yet programmers need to confront them
and possess the knowledge of them implicitly
to grasp, develop and debug the high-level program.
The rejection of metaprogramming prevents unimpeded clear thinking
where it is the most sorely needed;
it makes the easy harder and the hard nearly impossible,
all for the benefit of giving random neophytes a false sense of comfort.

The same mistake goes for all languages that wholly reject syntactic abstraction,
or provide a version thereof that is very awkward
(like C++ templates or Java compile-time annotations)
and/or very limited (such as C macros).
It also applies to all programmers and coding styles that frown upon syntactic abstraction
(maybe after being bitten by the bad implementations thereof such as above).
If you don't build DSLs, your general purpose language has
all the downsides of Turing-equivalence with none of the upsides.


### Runtime Rigidity ###

In their common reject of metaprogramming,
both Human and Martian computing approaches also lack
first-class notions of meta-levels at runtime.
Therefore, all their software is built and distributed
as a fixed semantic tower on top of a provided common virtual machine.
It's just that the virtual machine is very different between the Humans and Martians:
the Martian VM is oriented towards persistence and determinism,
the Human VM is just a low-level portability layer for families of cheap human hardware.


For any performance, have to implement things twice:
in the real jetted implementation, and in the official nock implementation.
And how do you ensure the equivalence?
In the end, you need a complete stack that can prove correctness,
handle both nock and real hardware/software. You don't win.


Trying to simplify the problem to a max and avoid the metalanguage issues.

The [Urbit Whitepaper](http://media.urbit.org/whitepaper.pdf) claims (1.2) that formally,
Urbit is an “operating function”: a general-purpose OS whose entire lifecycle
is defined as a pure, frozen, small function on its input stream.

Houyhnhnm strategies can be contrasted with Urbit _jets_.
Both are meta-level implementation optimizations;
but Urbit jets are a fixed number of strategies built into the system for a fixed language,
whereas new Houyhnhnm strategies can be written by users for any language they want to use.
Urbit is still a "build upwards only" system.


Wilful Resource Blindness
=========================

Beware: purely technical solutions to social problems.
That said, technology certainly shapes society. E.g. gun powder.

Lack of linearity : blind spot for the system, nor able to formalize am essential part of its behavior.

Low-level: it's a bootstrap loader rather than an OS. It emphasizes the wrong level of abstraction... And yes, using "jets" it can be all optimized away... But then what was gained? Nothing. Precision, maybe. But then any sufficiently formal system will do. No abstraction: like set theory vs category theory, is a bad foundation because it's too low level. Simple in the small (for small self contained metatheorems) vs simple in the large (for large inhabitable structures).


Not Invented Here
=================

Vocabulary Issue




Urbit as a demo
===============

Urbit makes for a cool demo of an orthogonally persistent system;
but only by sweeping under the rug the difficult issues,
to be solved by the metasystem of Urbit;
and unlike Nock, this metasystem, where most of the interesting things happen,
remains informal in its all-important side-effects,
and not actually bound to behave as a faithful implementation
as for the parts specified by the Nock machine.
In other words, the pretense of having fully formalized
the state of the system and its state function,
and of putting the end-user in control of it,
is ultimately a _sham_, a corruption.
The power remains in the opaque and totally unspecified
centralized implementation of the metaprogram that implements Nock
and issues real-world side-effects.

There is no one-size fits all way to handle all the issues with
connection to real-world devices, and policies that resolve tradeoffs
regarding persistence, privacy, latency, efficiency, safety, etc.
A one true centralized implementation for the metaprogram that handles them
is not a solution.
Only a general purpose platform for people to build their own metaprograms
can enable them to each solve the issues to their satisfaction.
And once you have this platform, you don't need any of the Urbit operating system,
because you already have a Houyhnhnm computing system.

Houyhnhnms have no ill feelings towards either Martians or Humans.
They hope that Urbit will be a great success, and demonstrate a lot of cool things
and inspire people to adopt orthogonal persistence.
However, Houyhnhnms believe that Urbit won't be able to outgrow being a cool demo
unless it embraces a more general purpose metaprogramming architecture.
