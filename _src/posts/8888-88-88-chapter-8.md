    Title: Chapter 8: Inter-process Communication
    Date: 2015-12-13T14:15:16
    Tags: DRAFT, Meta, Application, Autistic, Implicit, Communication, Copy-Paste, Platform, Free Software

In our discussion about
the difference between Human applications and their Houyhnhnm counterparts,
I was intrigued by claims Ngnghm made that communication
was much easier between activities of a Houyhnhnm computing system
than between applications of a Human computer system.
I decided to elaborate on this topic.

What 


### Implicit Communication ###

Communication between other processes can be explicit:
a process will specifically name another process or start a new process,
open a communication channel with it, and exchange data.
Explicit communication will do exactly what the programmers want
(or at least say: even Houyhnhnms have no [DWIM](http://www.jargon.net/jargonfile/d/DWIM.html));
it is no more complex or costly than programmers need;
but it requires tight coupling between the programs (and thus programmers)
on all sides of the communication, and is difficult to extend or adapt
to suit the dynamic needs of the end-user.

On the other hand, communication with other processes can be implicit:
something outside some process grabs data from it, and makes it available to some other process.
This is the case with copy-pasting, or with
piping the standard output of one process into the standard input of another.
Implicit communication is controlled by the users of a program
rather than by the programmers who write it, and therefore adapted to _their_ needs.
It may require complex support from the programs that partake in it
(or, we'll argue, their meta-programs);
but programmers don't have to worry about programs on the other side,
as long as they abide by some general protocol (and keep up with its update).

Note that implicit vs explicit is a continuum rather than a clear cut distinction:
every communication is partly explicit, because it necessarily involves grabbing data
that was somehow published by the first process, the publishing of which wasn't optimized away;
and every communication is partly implicit, because it always relies
on something in the context to effect that communication, at the meta level.
Another name for this dimension of software design is declarative vs procedural programming:
In the declarative approach, programmers describe what is being computed,
without specifying how it is going to be computed or how it will further processed,
which will be determined by strategies at the meta level.
In the procedural approach, programmers describe the steps of the computation
without specifying what is going to be computed,
and all the operational semantics remains at the base level.

Houyhnhnms recognize the importance of both aspects of communication, implicit and explicit;
meanwhile Humans tend to be blind about the implicit aspect,
because they are habitually reluctant to seriously consider anything at the meta level.
When Humans tackle implicit communication (and they cannot not do it),
they advance backwards into the topic, blind about what it is about;
and thus they end up with implicit communication systems
simultaneously quite complex and costly for programmers to implement,
yet extremely limited in expressive power for the end-user.


### The Case of Copy-Paste ###

The kind of implicit communication most visible to end-users in Human computer systems
is copy-paste: applications interact with a graphical interface,
and may allow the user to either copy or cut part of a document being displayed;
the clipping is then stored in a global clipboard (with space for a single clip).
Another application interacting with the graphical interface may then
allow the user to paste the clipping currently in the clipboard into its own document.
The two programs may know nothing of each other;
as long as they properly partake in the protocol,
they will have communicated with each other as per the desires of the end-user.
By providing user-controllable implicit communication between most applications,
it is an essential feature in Human computer systems.

Now, on Human computer systems, copy-paste requires every participating application
to specially implement large chunks of graphical interface support.
Every application then becomes somewhat bloated, having to include large graphical libraries
(which in modern systems can happily be shared between applications);
but also having to properly initialize them, follow their protocols,
abide by the strictures of their event loop, etc.
They have to be able to negotiate with the clipboard server
the kind of entities they can copy and paste,
and/or convert between what the server supports and what they can directly handle.
This architecture where all features are implemented at the same level of abstraction
contributes significantly to the complexity of applications;
applications are therefore hard to reason about, brittle and insecure.
The overall graphical environment will in turn inherit the unreliability
of the applications that partake in it.
And despite all this complexity, often some application will fail to support
copying for some of the information it displays (e.g. an error message);
the feature is then sorely missed as the user needs to copy said information by hand.

An interesting exception to the rule of the above paragraph
is the case of "console" applications:
these applications display simple text to
a "terminal emulator" straight out of the 1970s,
at which point all the output can be copied for further pasting.
The terminal emulator thus serves as the meta-program responsible
for presentation of the application output, and handling copy-paste.
This comes with many limitations:
only plain text is supported, not "rich text", not images;
lines longer than the terminal size may or may not be clipped,
or have an end-of-line marker or escape character inserted;
selecting more than a screenful may be an issue,
though you can sometimes work around it by resizing the terminal or by switching to tiny fonts;
standard output and error output may be mixed (or worse, output from background programs);
layout artefacts may appear (such as spaces to end-of-line,
or graphic characters that draw boxes in which text is displayed); etc.
Still, the principle of a meta-program to handle display already exists
in some Human computer systems;
its protocol is just limited, baroque and antiquated.

Houyhnhnm computing systems generalize the idea that presenting data to the end-user
is the job of a meta-program separate from the activity that displays the data;
that meta-program is part of an common extensible platform,
rather than of the "application" that underlies each activity.
The display manager will thus manage a shared clipboard;
this clipboard may contain more than just one clip;
it may contain an arbitrarily long list of clips (like the Emacs kill-ring).
Also, clips can also include source domain information, so that
the user can't unintentionally paste sensitive data into untrusted activities,
or data of an unexpected kind.
The platform manages interactive confirmations, rejection notifications, content filters,
that are activated when users copy or paste data;
in these aspects as in all others,
the platform can be extended by modules and customized by end-users.
Other meta-programs beside the display manager can reuse the same infrastructure:
they can use their own criteria to select data from a program's output;
they can use the selected data for arbitrary computations,
and store the results into arbitrary variables or data structures, not just a common clipboard;
they may consult the history of the selected data, or watch the data continuously as it changes,
instead of merely extracting its current value.
And the base program doesn't have to do anything about it,
except properly organize its data
so that the external meta-programs may reliably search that data.


### Protocols as Meta-level Business ###

Houyhnhnm computing systems resolutely adopt the notion that presenting computation results
is generally the task of a series of (meta)programs separate
from the one that is computing the results.
Factoring out the interface at the meta level means that
each level can be kept conceptually simple.
The system remains ["reasonable"](http://thetrendythings.com/read/20412),
that is susceptible to be reasoned about.
Security properties can be assessed once,
abstracting away the specifics of programs using the features.
The robustness and timeliness of the system don't have to depend
on every application partaking in the protocol being well-behaved,
nor on every programmer working on any such application
being steadfast at all times and never making any mistake.
There can be no bugs in all the lines of code that the programmers don't have to write anymore.
And updating or extending the protocol is much easier,
since it only involves updating or extending the according meta-programs,
without having to touch the base-level applications
(unless they want to explicitly take advantage of new features).

Moving features from base-level applications to meta-level layers can be justified
with all the same arguments why "preemptive multitasking" beats "cooperative multitasking"
as an interface offered to programmers:
human programmers are intrinsically unreliable,
any kind of "cooperation" that relies on manually written code to abide by non-trivial invariants
in all cases will result in massive system instability.
At the same time, "cooperation" is (and actually must be) used under the hood
to preserve any non-trivial system invariant â€” but can it be used automatically, correctly,
through a meta-program's code-generator.

In Human computer systems, there is little in the way of meta-programs;
program helpers are strictly runtime entities, and programmers must manually follow
the "design patterns" required to properly implement the system protocols.
In Houyhnhnm computing systems, there are plenty of meta-programs;
though they may have a runtime component,
they are most importantly compile-time and/or link-time entities;
and they ensure that all runtime code strictly follows all system protocols by construction.
The meta-programs that display, select, extract or watch data
use introspection of the program's state, types and variables;
and for reasons of efficiency, they do not re-do it constantly at runtime.

Changes in these meta-programs may involve recompiling or otherwise reprocessing
every base program that uses them.
This meta-processing is deeply incompatible with the traditional Human notion
of "binary-only" or "closed source" software;
but that notion doesn't exist for Houyhnhnms:
Houyhnhnms understand that
[metaprogramming requires free availability of sources](http://fare.tunes.org/articles/ll99/index.en.html).
For similar reasons, Humans who sell proprietary software see
a platform based on meta-programming as the Antechrist.

A program that comes without source is crippled in terms of functionality;
it is also untrusted, to be run in a specially paranoid sandbox.
Houyhnhnms may tolerate interactive documents that behave that way;
they may accept black box services where they only care about the end-result of one-time interactions.
But they have little patience for integrating black-box programs into their regular platforms;
if they care about what a black-box program does,
they will spend enough time decompiling it to make it usable,
or reverse-engineering it and reimplementing it.

### Pipes ###

As another instance of implicit communication,
one of the great successful inventions of (the Human computer system) Unix
was the ability to combine programs through _pipes_:
regular "console" applications possess a mode of operation where
they take input from an implicit "standard input"
and yield output into an implicit "standard output",
with even a separate "error output" to issue error messages and warnings,
and additional "inherited" handles to system-managed entities.
A process does not know and does not care
where the input comes from and where the output is going to:
this was setup by the process's _parent_ before the program started to run.

The parent here plays a bit of the role of a meta-level,
but this role is very limited and only influences the initial program configuration
(unless using advanced tools like `ptrace`,
but that remains uncommon outside its intended use as a debugging tool,
probably because it very unwieldy, non-portable, and inefficient).
Still, even within this limitation, Unix pipes revolutionized the way software was written,
by allowing independent, isolated programs to be composed,
and the resulting compositions to be orchestrated in a script
written in some high-level programming language.

Houyhnhnm computing systems very much acknowledge the power of composing programs;
but they are not so restricted.
Programs may be of arbitrary types, with arbitrary numbers of inputs and outputs,
all of them properly typed according to some high-level object schema,
rather than always low-level sequences of bytes
(note that low-level sequences of bytes do constitute an acceptable type,
though they are rarely used in practice).
These typed inputs and outputs provide natural communication points
that can be used to compose programs together.

Meta-programs can act not just to control the initial configuration of applications,
but also to display their state,

Most of the time, though, communication is in some measure implicit:
a process will write data to its "standard output" and read from its "standard input",
which may be connected to a communication stream with another process, a terminal, or to a file;
data displayed to a terminal, or to the windows of applications that support it,
can be copied and thereafter pasted into another application;
files and communication streams are read by processes
that are largely independent from the processes that write to them.
In the end, each and every single instance of communication between distinct processes
involves _some_ implicit naming:
indeed, division of labor in programming is precisely
the ability to name a communication channel and
not have to worry too much about what's on the other side
except for its following the proper communication protocol.



But there are many obstacles to explicit communication in Human computer systems.
The first reason is that it's very hard to write a program
that other programs can explicitly communicate with:
such a program must specifically implement some server that listens on a known port,
or that registers on a common "data bus";
to process the connections, it must either possess some asynchronous event loop,
or deal with hard concurrency issues;
Human mainstream programming languages have no linguistic support for decentralized event loops,
and so every platform or library if it has an event loop will likely have
its own incompatible centralized variant.


this is alone is complex, and may require tricky integration between several event loops,
or eschewing integration and dealing with hard concurrency issues;
this in addition to the low-level nature of the protocol discussed above
makes writing such a server an expensive proposition as well as a huge security risk
(and then we need to discuss mangement of access rights)
and so most programmers don't bother doing it.
The difficulty of locating one of a few processes available to communicate with
is of course compounded by the notorious instability and transience
of processes in Human computer systems;
you have to constantly name new processes and create new communication channels,
because these processes as well as your current process keep dying.
You can try to automate this communication,
but there is still a vast array of error cases that you will have to handle.

Houyhnhnm computing systems recognize that communication is something to handle
at the meta-level of the systems that are being made to communicate with each other.

Persistence means you can give a stable name to a stable activity.



In the rare cases that you do know exactly what's on the other side,
it's not inter-process communication, it's a (static) function call.
And indeed, a good enough Houyhnhnm computing system implementation
will compile invocation of known services into function calls wherever appropriate;
there may be good reasons pertaining to division of labor and program maintenance
for the application source code to express some computations
as (potentially remote) service requests to a server controlled by the application programmer;
and there may be good reasons pertaining to local configuration and access rights
why the server's behavior may be known at link time, and its state shareable at runtime;
and thus the system can suitably optimize the implementation
and avoid the cost of runtime communication between physically isolated processes
(and later invalidate this optimization if the configuration changes).
Conversely, calls to well-defined functions can sometimes be usefully implemented
as requests to some server,
for instance one running on or managing a separate device
that is specialized for the task, that it can perform cheaper or faster.
Thus, whether the source code for some part of an application is modular or monolithic
is wholly independent of whether the implementation will be modular or monolithic at runtime.
The former is a matter of division of labor and specialization of tasks
between programmers at coding-time;
the latter is a matter of division of labor and specialization of tasks
between programs at runtime.

Controlling these communication channels happens at the meta-level.
So, external meta-level on simple programs (To a point: Unix shells).
Or, extend the program to have meta-level control language for the base functionality.
Houyhnhnms instead understand the need for system support for the meta-level at runtime.

Implicit channels:
Humans have irreflective languages with static binding of meaning to labels;
bindings are not directly express in the language;
instead programmers must explicitly manage handles
and manually follow a protocol consisting of calls to kernel APIs.
Houyhnhnms have reflective languages that can dynamically bind meaning to labels;
programmers can express the binding and rebinding of implicit communication channels
as regular language idioms whereby the compiler manages

the dynamic binding of implicit communication channels
to actual other programs on the other side.

that can't express the dynamic binding


He explained that at least for self-contained end-user applications,
the situation was very similar, at least superficially:

If self-contained applications that don't involve communicating data



### Activity Sandboxing ###

Indeed, in Houyhnhnm computing systems, proper sandboxing ensures that
activities may only share or access data or other communicate
only according to the rules they have declared and that the system owner agreed to.
In particular, applications purporting to be autistic (as above)
are ensured to actually be autistic.
Proper sandboxing also means that the system owner isn't afraid of getting
viruses, malware or data leaks via an activity.

Unlike Human computer systems, Houyhnhnm computing systems always
run all code in a fully abstract sandbox.
There is no way for code to distinguish between "normal" and "virtualized" machines.
If the system owner refuses to grant an application
access rights to some or all requested resources,
the activity has no direct way to determine that
the access was denied;
instead, whenever it will access the resource,
it will get blank data, or fake data from a randomized honeypot,
or network delay notification, or whatever its meta-level is configured to provide;
the system owner ultimately controls all configuration.
If the application is well-behaved, many unauthorized accesses may be optimized away;
but even if it's not, it has no reliable way of telling whether it's running "for real",
and is connected to some actual resource.

Allowing code to make the difference would be a huge security failure;
and any time a monitor in a production system
recognizes the attempt by a process to probe its environment
or otherwise break the abstraction, a serious security violation is flagged;
upon detection thereof, the process and its all associated processes are terminated,
up to the next suitably secure meta-level;
also the incident is logged, a police investigation is triggered,
and the responsible software vendor is arrested.
â€” Unless of course, the people responsible for the break in attempt
are the system's owners themselves, or penetration testers they have hired
to assess and improve their security, which is a recommended practice
among anyone hosting computations controlling any serious actual resources.

Note that proper sandboxing at heart has
[nothing whatsoever](/blog/2015/11/28/chapter-6-kernel-is-as-kernel-does)
to do with having "kernel" support for "containers"
or hardware-accelerated "virtual machines";
rather it is all about providing _full abstraction_,
i.e. abstractions that don't leak.
For instance, a user-interface should makes it impossible to do break the abstraction
without intentionally going to the meta-level.
you shouldn't be able to accidentally copy and paste
potentially sensitive information from one sandbox to the next;
instead, copy and pasting from one sandbox to another
should require extra confirmation _before_ any information is transfered;
the prompt is managed by a common meta-level between the sandboxes,
and provides the user with context about which are the sandboxes
and what is the considered content;
that the user may thus usefully confirm based on useful information
â€” or he may mark this context or a larger context as authorized
for copying and pasting without further confirmations.


