    Title: Chapter 8: Advanced Application Issues
    Date: 2015-12-13T14:15:16
    Tags: DRAFT



### Pipes ###

As another instance of implicit communication,
one of the great successful inventions of (the Human computer system) Unix
was the ability to combine programs through _pipes_:
regular "console" applications possess a mode of operation where
they take input from an implicit "standard input"
and yield output into an implicit "standard output",
with even a separate "error output" to issue error messages and warnings,
and additional "inherited" handles to system-managed entities.
A process does not know and does not care
where the input comes from and where the output is going to:
this was setup by the process's _parent_ before the program started to run.
The parent here plays a bit of the role of a meta-level,
but this role is very limited and only influences the initial program configuration
(unless using advanced tools like `ptrace`,
but that remains uncommon outside its intended use as a debugging tool,
probably because it very unwieldy, non-portable, and inefficient).
Still, even within this limitation, Unix pipes revolutionized the way software was written,
by allowing independent, isolated programs to be composed,
and the resulting compositions to be orchestrated in a script
written in some high-level programming language.

Houyhnhnm computing systems very much acknowledge the power of composing programs;
but they are not so restricted.
Programs may be of arbitrary types;
these types naturally provide communication points
that can be used to compose programs together.
Meta-programs can act not just to control the initial configuration of applications,
but also to display their state,




document extraction.


Most of the time, though, communication is in some measure implicit:
a process will write data to its "standard output" and read from its "standard input",
which may be connected to a communication stream with another process, a terminal, or to a file;
data displayed to a terminal, or to the windows of applications that support it,
can be copied and thereafter pasted into another application;
files and communication streams are read by processes
that are largely independent from the processes that write to them.
In the end, each and every single instance of communication between distinct processes
involves _some_ implicit naming:
indeed, division of labor in programming is precisely
the ability to name a communication channel and
not have to worry too much about what's on the other side
except for its following the proper communication protocol.



But there are many obstacles to explicit communication in Human computer systems.
The first reason is that it's very hard to write a program
that other programs can explicitly communicate with:
such a program must specifically implement some server that listens on a known port,
or that registers on a common "data bus";
to process the connections, it must either possess some asynchronous event loop,
or deal with hard concurrency issues;
Human mainstream programming languages have no linguistic support for decentralized event loops,
and so every platform or library if it has an event loop will likely have
its own incompatible centralized variant.


this is alone is complex, and may require tricky integration between several event loops,
or eschewing integration and dealing with hard concurrency issues;
this in addition to the low-level nature of the protocol discussed above
makes writing such a server an expensive proposition as well as a huge security risk
(and then we need to discuss mangement of access rights)
and so most programmers don't bother doing it.
The difficulty of locating one of a few processes available to communicate with
is of course compounded by the notorious instability and transience
of processes in Human computer systems;
you have to constantly name new processes and create new communication channels,
because these processes as well as your current process keep dying.
You can try to automate this communication,
but there is still a vast array of error cases that you will have to handle.

Houyhnhnm computing systems recognize that communication is something to handle
at the meta-level of the systems that are being made to communicate with each other.





In the rare cases that you do know exactly what's on the other side,
it's not inter-process communication, it's a (static) function call.
And indeed, a good enough Houyhnhnm computing system implementation
will compile invocation of known services into function calls wherever appropriate;
there may be good reasons pertaining to division of labor and program maintenance
for the application source code to express some computations
as (potentially remote) service requests to a server controlled by the application programmer;
and there may be good reasons pertaining to local configuration and access rights
why the server's behavior may be known at link time, and its state shareable at runtime;
and thus the system can suitably optimize the implementation
and avoid the cost of runtime communication between physically isolated processes
(and later invalidate this optimization if the configuration changes).
Conversely, calls to well-defined functions can sometimes be usefully implemented
as requests to some server,
for instance one running on or managing a separate device
that is specialized for the task, that it can perform cheaper or faster.
Thus, whether the source code for some part of an application is modular or monolithic
is wholly independent of whether the implementation will be modular or monolithic at runtime.
The former is a matter of division of labor and specialization of tasks
between programmers at coding-time;
the latter is a matter of division of labor and specialization of tasks
between programs at runtime.

Controlling these communication channels happens at the meta-level.
So, external meta-level on simple programs (To a point: Unix shells).
Or, extend the program to have meta-level control language for the base functionality.
Houyhnhnms instead understand the need for system support for the meta-level at runtime.

Implicit channels:
Humans have irreflective languages with static binding of meaning to labels;
bindings are not directly express in the language;
instead programmers must explicitly manage handles
and manually follow a protocol consisting of calls to kernel APIs.
Houyhnhnms have reflective languages that can dynamically bind meaning to labels;
programmers can express the binding and rebinding of implicit communication channels
as regular language idioms whereby the compiler manages

the dynamic binding of implicit communication channels
to actual other programs on the other side.

that can't express the dynamic binding


He explained that at least for self-contained end-user applications,
the situation was very similar, at least superficially:

If self-contained applications that don't involve communicating data



### Different Shapes ###

Because of this high barrier in communication between components,
Human applications grow into big hulking pieces of software that each try to do everything,
yet can't.
These applications are each stretched into doing many of the same things, all of them badly.

and some things are just too far out there.

Failure of division of labour.
Failure of specialization of tasks.


### Extensibility through Reflection ###




While there are a lot of autistic applications,
most applications involve some communication with other applications.
And that's the difference between Houyhnhnm and Human applications becomes obvious:
Human applications are monolithic and irreflective,
whereas Houyhnhnm applications are modular and reflective.

In a Human application, any case of communication whatsoever
must be supported by an explicitly written case in a procedure;
which requires the programmer to foresee that case of communication and support it.
This is very work intensive, and any new case requires an update
to each and every application involved.
A copy/paste protocol can often alleviate the combinatorial explosion of cases
in communication between pairs of applications:
users may then select and cut or copy some data from one application, and paste it into another.

However, this only works where each application's programmer explicitly enabled
the proper half of the protocol (copying, or pasting).
And even then, it requires user-intensive interactions
and is not suitable to automation.
Some Human computer systems support some form of Object Linking and Embedding,
whereby some of this copy and pasting effort can be automated;
but it remains an advanced feature that requires yet more programmer support,
is only usable by experts, and remains brittle
when communication happens without human supervision.

Human computer systems do their worst to erase any meta-level at runtime,
and can only deal with a fixed number of hardwired cases;
these cases become ever more numerous and complex as the applications get larger,
yet they are never sufficient.
Eventually, some slightly more "clever" Human reinvents an extensible general case,
using a protocol that badly reimplements a small _ad hoc_ subset of dynamic typing and reflection.
Thereupon, whatever static types are used in these Human computer systems
become a burden that have to be fought constantly to keep the system extensible,
deconstructing and reconstructing them
at the interface between the system and its extensibility layer.

By contrast, Houyhnhnms embrace the fact that communication between applications
is an operation at the meta-level with respect to these applications:
at heart, it is about generating the code for two applications to communicate with each other.
This code may be very simple, but will crucially depend on the two applications.
Houyhnhnm computing systems can access the meta-level at runtime and simply generate that code
based on a declarative approach,
whereby each application suitably registers its interfaces and their types.
The type-driven code generation is simultaneously simple, general, safe and fast;
programmers have almost nothing to write by hand.

Types are thus a boon and helps generate all or most of the code
without the programmers having to write anything by hand.
Not having all the general-purpose code at the base-level
also makes it much easier to reason about safety property of the code base.
Not having to reinvent the wheel badly in every application
means that it types and reflection can be invented well
once (or a handful of times) in the entire system.


### Modularity through Declarativeness ###

This communication often happens from outside the application itself:
some application exposes some data, and
the user latter copies and pastes that data into another application.
Often, the might like the data to be directly linked from one application to the next,
and the entire copy/pasting automated;
but in Human computer systems, architectural limitations often make such automated linking
either altogether possible, or too brittle to happen without human supervision.

Extensibility.
Plug-ins.
XXXX

Delivering software as components, not applications (Human closest: browser plugins)

AOP: Modularity in implementation strategies
Aspects: search.



### Application Sandboxing ###

In Houyhnhnm computing systems, proper sandboxing ensures that
applications may only share or access data
according to the rules they have declared and that the system owner agreed to.
In particular, applications purporting to be autistic (as above)
are ensured to actually be autistic.
Proper sandboxing also means that the system owner isn't afraid of getting
viruses, malware or data leaks via an application.

Unlike Human computer systems, Houyhnhnm computing systems always
run all code in a fully abstract sandbox.
There is no way for code to distinguish between "normal" and "virtualized" machines.
If the system owner refuses to grant an application
access rights to some or all requested resources,
the application has no direct way to determine that
the access was denied;
instead, whenever it will access the resource,
it will get blank data, or fake data from a randomized honeypot,
or network delay notification, or whatever the system owner configured;
if the application is well-behaved, many unauthorized accesses may be optimized away;
but even if it's not, it has no way of telling that whether it's running "for real",
and is connected to some actual resource.

Allowing code to make the difference would be a huge security failure;
and any time a monitor in a production system
recognizes the attempt by a process to probe its environment
or otherwise break the abstraction, a serious security violation is flagged;
upon detection thereof, the process and its all associated processes are terminated,
up to the next suitably secure meta-level;
also the incident is logged, a police investigation is triggered,
and the responsible software vendor is arrested.
— Unless of course, the people responsible for the break in attempt
are the system's owners themselves, or penetration testers they have hired
to assess and improve their security, which is a recommended practice
among anyone hosting computations controlling any serious actual resources.

Note that proper sandboxing at heart has
[nothing whatsoever](/blog/2015/11/28/chapter-6-kernel-is-as-kernel-does)
to do with having "kernel" support for "containers"
or hardware-accelerated "virtual machines";
rather it is all about providing _full abstraction_,
i.e. abstractions that don't leak.
For instance, a user-interface should makes it impossible to do break the abstraction
without intentionally going to the meta-level.
you shouldn't be able to accidentally copy and paste
potentially sensitive information from one sandbox to the next;
instead, copy and pasting from one sandbox to another
should require extra confirmation _before_ any information is transfered;
the prompt is managed by a common meta-level between the sandboxes,
and provides the user with context about which are the sandboxes
and what is the considered content;
that the user may thus usefully confirm based on useful information
— or he may mark this context or a larger context as authorized
for copying and pasting without further confirmations.







### Sentient-Computer interface

Failure of UX Design of the programmer experience.



Not only does that make Houyhnhnm systems much simpler,
it also guarantees forever interoperability of every single piece data with any future system,
at whichever level of abstraction that data was defined.
If you want your data to remain relevant to your future self, or to be usable by other people, etc.,
you still need to wisely choose suitable algebraic data types,
to organize software into components with clean interfaces,
to pick appropriate policies that lead to suitably performant implementations,
to rely on suitable libraries.



Implementation vs expression.
expression can be implemented at the meta-level —
but that requires explicitly distinguishing the meta-level.
Human computer systems tend to either not allow meta-level programming,
or to conflate and confuse meta-levels.
Korzybsky: Insanity.
Confusing the thing and its representation.
Turing: universal languages wrt implementation, that can implement anything.
But unless the meta-level is universal, you cannot express everything.
And unless you actually implement other languages (DSLs),
then _in practice_, the restricted subset of your language that you actually use
is _not Turing-equivalent_.
